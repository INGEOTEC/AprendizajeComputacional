<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="es" xml:lang="es"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.5.40">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Aprendizaje Computacional – 9&nbsp; Discriminantes Lineales</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<link href="../capitulos/10Optimizacion.html" rel="next">
<link href="../capitulos/08Arboles.html" rel="prev">
<script src="../site_libs/quarto-html/quarto.js"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "Sin resultados",
    "search-matching-documents-text": "documentos encontrados",
    "search-copy-link-title": "Copiar el enlace en la búsqueda",
    "search-hide-matches-text": "Ocultar resultados adicionales",
    "search-more-match-text": "resultado adicional en este documento",
    "search-more-matches-text": "resultados adicionales en este documento",
    "search-clear-button-title": "Borrar",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancelar",
    "search-submit-button-title": "Enviar",
    "search-label": "Buscar"
  }
}</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="nav-sidebar floating">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Alternar barra lateral" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../capitulos/09Lineal.html"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Discriminantes Lineales</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Alternar barra lateral" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Buscar" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="../">Aprendizaje Computacional</a> 
        <div class="sidebar-tools-main">
    <a href="https://github.com/INGEOTEC/AprendizajeComputacional" title="Ejecutar el código" class="quarto-navigation-tool px-1" aria-label="Ejecutar el código"><i class="bi bi-github"></i></a>
    <a href="../Aprendizaje-Computacional.pdf" title="Descargar PDF" class="quarto-navigation-tool px-1" aria-label="Descargar PDF"><i class="bi bi-file-pdf"></i></a>
</div>
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Buscar"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Prefacio</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../capitulos/01Introduccion.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Introducción</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../capitulos/02Teoria_Decision.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Teoría de Decisión Bayesiana</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../capitulos/03Parametricos.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Métodos Paramétricos</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../capitulos/04Rendimiento.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Rendimiento</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../capitulos/05ReduccionDim.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Reducción de Dimensión</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../capitulos/06Agrupamiento.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Agrupamiento</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../capitulos/07NoParametricos.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Métodos No Paramétricos</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../capitulos/08Arboles.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Árboles de Decisión</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../capitulos/09Lineal.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Discriminantes Lineales</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../capitulos/10Optimizacion.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Optimización</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../capitulos/11RedesNeuronales.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Redes Neuronales</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../capitulos/12Ensambles.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">Ensambles</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../capitulos/13Comparacion.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">Comparación de Algoritmos</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../capitulos/17Referencias.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Referencias</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true">
 <span class="menu-text">Apéndices</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true" aria-label="Alternar sección">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../capitulos/14Estadistica.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">A</span>&nbsp; <span class="chapter-title">Estadística</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../capitulos/15Codigo.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">B</span>&nbsp; <span class="chapter-title">Código</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../capitulos/16ConjuntosDatos.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">C</span>&nbsp; <span class="chapter-title">Conjunto de Datos</span></span></a>
  </div>
</li>
      </ul>
  </li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Tabla de contenidos</h2>
   
  <ul>
  <li><a href="#paquetes-usados" id="toc-paquetes-usados" class="nav-link active" data-scroll-target="#paquetes-usados"><span class="header-section-number">9.1</span> Paquetes usados</a></li>
  <li><a href="#introducción" id="toc-introducción" class="nav-link" data-scroll-target="#introducción"><span class="header-section-number">9.2</span> Introducción</a></li>
  <li><a href="#sec-discriminante" id="toc-sec-discriminante" class="nav-link" data-scroll-target="#sec-discriminante"><span class="header-section-number">9.3</span> Función Discriminante</a>
  <ul class="collapse">
  <li><a href="#sec-binaria" id="toc-sec-binaria" class="nav-link" data-scroll-target="#sec-binaria"><span class="header-section-number">9.3.1</span> Clasificación Binaria</a></li>
  <li><a href="#sec-geometria-funcion-decision" id="toc-sec-geometria-funcion-decision" class="nav-link" data-scroll-target="#sec-geometria-funcion-decision"><span class="header-section-number">9.3.2</span> Geometría de la Función de Decisión</a></li>
  <li><a href="#sec-multiples-clases" id="toc-sec-multiples-clases" class="nav-link" data-scroll-target="#sec-multiples-clases"><span class="header-section-number">9.3.3</span> Múltiples Clases</a></li>
  </ul></li>
  <li><a href="#sec-svm" id="toc-sec-svm" class="nav-link" data-scroll-target="#sec-svm"><span class="header-section-number">9.4</span> Máquinas de Soporte Vectorial</a>
  <ul class="collapse">
  <li><a href="#optimización" id="toc-optimización" class="nav-link" data-scroll-target="#optimización"><span class="header-section-number">9.4.1</span> Optimización</a></li>
  <li><a href="#kernel" id="toc-kernel" class="nav-link" data-scroll-target="#kernel"><span class="header-section-number">9.4.2</span> Kernel</a></li>
  </ul></li>
  <li><a href="#sec-regresion-logistica" id="toc-sec-regresion-logistica" class="nav-link" data-scroll-target="#sec-regresion-logistica"><span class="header-section-number">9.5</span> Regresión Logística</a>
  <ul class="collapse">
  <li><a href="#optimización-1" id="toc-optimización-1" class="nav-link" data-scroll-target="#optimización-1"><span class="header-section-number">9.5.1</span> Optimización</a></li>
  </ul></li>
  <li><a href="#comparación" id="toc-comparación" class="nav-link" data-scroll-target="#comparación"><span class="header-section-number">9.6</span> Comparación</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<div class="quarto-title-block"><div><h1 class="title"><span id="sec-discriminantes-lineales" class="quarto-section-identifier"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Discriminantes Lineales</span></span></h1><button type="button" class="btn code-tools-button dropdown-toggle" id="quarto-code-tools-menu" data-bs-toggle="dropdown" aria-expanded="false"><i class="bi"></i> Código</button><ul class="dropdown-menu dropdown-menu-end" aria-labelelledby="quarto-code-tools-menu"><li><a id="quarto-show-all-code" class="dropdown-item" href="javascript:void(0)" role="button">Mostrar todo el código</a></li><li><a id="quarto-hide-all-code" class="dropdown-item" href="javascript:void(0)" role="button">Ocultar todo el código</a></li><li><hr class="dropdown-divider"></li><li><a id="quarto-view-source" class="dropdown-item" href="javascript:void(0)" role="button">Ver el código fuente</a></li></ul></div></div>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<p>El <strong>objetivo</strong> de la unidad es conocer y aplicar diferentes métodos lineales de discriminación para atacar problemas de clasificación.</p>
<section id="paquetes-usados" class="level2" data-number="9.1">
<h2 data-number="9.1" class="anchored" data-anchor-id="paquetes-usados"><span class="header-section-number">9.1</span> Paquetes usados</h2>
<div id="bf08e957" class="cell" data-execution_count="1">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.svm <span class="im">import</span> LinearSVC, SVC</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.linear_model <span class="im">import</span> LogisticRegression</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.decomposition <span class="im">import</span> PCA</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> train_test_split</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> f1_score, recall_score, precision_score</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> scipy.stats <span class="im">import</span> multivariate_normal</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.datasets <span class="im">import</span> load_iris</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> matplotlib <span class="im">import</span> pylab <span class="im">as</span> plt</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> seaborn <span class="im">as</span> sns</span></code><button title="Copiar al portapapeles" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<hr>
<div class="quarto-video"><iframe data-external="1" src="https://www.youtube.com/embed/BgLm0tVxW8A" width="560" height="315" title="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div>
<hr>
</section>
<section id="introducción" class="level2" data-number="9.2">
<h2 data-number="9.2" class="anchored" data-anchor-id="introducción"><span class="header-section-number">9.2</span> Introducción</h2>
<p>En unidades anteriores se han visto diferentes técnicas para discriminar entre clases; en particular se ha descrito el uso de la probabilidad <span class="math inline">\(\mathbb P(\mathcal Y \mid \mathcal X)\)</span> para encontrar la clase más probable. Los parámetros de <span class="math inline">\(\mathbb P(\mathcal Y \mid \mathcal X)\)</span> se han estimado utilizando métodos <a href="@sec-metodos-parametricos">paramétricos</a> y <a href="@sec-metodos-no-parametricos">no paramétricos</a>. En está unidad se describe el uso de funciones discriminantes para la clasificación y su similitud con el uso de <span class="math inline">\(\mathbb P(\mathcal Y \mid \mathcal X).\)</span></p>
</section>
<section id="sec-discriminante" class="level2" data-number="9.3">
<h2 data-number="9.3" class="anchored" data-anchor-id="sec-discriminante"><span class="header-section-number">9.3</span> Función Discriminante</h2>
<p>En la unidad de Teoría de Decisión Bayesiana (<a href="02Teoria_Decision.html" class="quarto-xref"><span>Capítulo 2</span></a>) se describió el uso de <span class="math inline">\(\mathbb P(\mathcal Y \mid \mathcal X)\)</span> para clasificar, se mencionó que la clase a la que pertenece <span class="math inline">\(\mathcal X=x\)</span> es la de mayor probabilidad, es decir,</p>
<p><span class="math display">\[
C(x) = \textsf{argmax}_{k=1}^K \mathbb P(\mathcal Y=k \mid \mathcal X=x),
\]</span></p>
<p>donde <span class="math inline">\(K\)</span> es el número de clases y <span class="math inline">\(\mathcal Y=k\)</span> representa la <span class="math inline">\(k\)</span>-ésima clase. Considerando que la <a href="@eq-evidencia">evidencia</a> es un factor que normaliza, entonces, <span class="math inline">\(C(x)\)</span> se puede definir de la siguiente manera.</p>
<p><span class="math display">\[
C(x) = \textsf{argmax}_{k=1}^K \mathbb P(\mathcal X=x \mid \mathcal Y=k)\mathbb P(\mathcal Y=k).
\]</span></p>
<p>Agrupando la probabilidad a priori y verosimilitud en una función <span class="math inline">\(g_k,\)</span> es decir, <span class="math inline">\(g_k(x) = P(\mathcal X=x \mid \mathcal Y=k)\mathbb P(\mathcal Y=k),\)</span> hace que <span class="math inline">\(C(x)\)</span> se sea:</p>
<p><span class="math display">\[
C(x) = \textsf{argmax}_{k=1}^K g_k(x).
\]</span></p>
<p>Observando <span class="math inline">\(C(x)\)</span> y olvidando los pasos utilizados para derivarla, uno se puede imaginar que lo único necesario para generar un clasificador de <span class="math inline">\(K\)</span> clases es definir un conjunto de functions <span class="math inline">\(g_k\)</span> que separen las clases correctamente. En esta unidad se presentan diferentes maneras para definir <span class="math inline">\(g_k\)</span> con la característica de que todas ellas son lineales, e.g., <span class="math inline">\(g_k(\mathbf x) = \mathbf w_k \cdot \mathbf x + w_{k_0}.\)</span></p>
<section id="sec-binaria" class="level3" data-number="9.3.1">
<h3 data-number="9.3.1" class="anchored" data-anchor-id="sec-binaria"><span class="header-section-number">9.3.1</span> Clasificación Binaria</h3>
<p>La descripción de discriminantes lineales empieza con el caso particular de dos clases, i.e., <span class="math inline">\(K=2\)</span>. En este caso <span class="math inline">\(C(\mathbf x)\)</span> es encontrar el máximo de las dos funciones <span class="math inline">\(g_1\)</span> y <span class="math inline">\(g_2\)</span>. Una manear equivalente sería definir a <span class="math inline">\(C(\mathbf x)\)</span> como</p>
<p><span class="math display">\[
C(\mathbf x) = \textsf{sign}(g_1(\mathbf x) - g_2(\mathbf x)),
\]</span></p>
<p>donde <span class="math inline">\(\textsf{sign}\)</span> es la función que regresa el signo, entonces solo queda asociar el signo positivo a la clase 1 y el negativo a la clase 2. Utilizando esta definición se observa lo siguiente</p>
<p><span class="math display">\[
\begin{split}
    g_1(\mathbf x) - g_2(\mathbf x) &amp;= (\mathbf w_1 \cdot \mathbf x + w_{1_0}) - (\mathbf w_2 \cdot \mathbf x + w_{2_0}) \\
         &amp;= (\mathbf w_1 + \mathbf w_2) \cdot \mathbf x + (w_{1_0} - w_{2_0}) \\
         &amp;= \mathbf w \cdot \mathbf x + w_0
\end{split},
\]</span></p>
<p>donde se concluye que para el caso binario es necesario definir solamente una función discriminante y que los parámetros de esta función son <span class="math inline">\(\mathbf w\)</span> y <span class="math inline">\(\mathbf w_0.\)</span> Otra característica que se ilustra es que el parámetro <span class="math inline">\(\mathbf w_0\)</span> está actuando como un umbral, es decir, <span class="math inline">\(\mathbf x\)</span> corresponde a la clase positiva si <span class="math inline">\(\mathbf w \cdot \mathbf x &gt; -w_0.\)</span></p>
<p>En la <a href="#fig-lineal-discriminante" class="quarto-xref">Figura&nbsp;<span>9.1</span></a> se observa el plano (linea) que divide las dos clases, este plano representa los puntos que satisfacen <span class="math inline">\(g(\mathbf x)=0\)</span>.</p>
<div id="cell-fig-lineal-discriminante" class="cell" data-execution_count="3">
<details class="code-fold">
<summary>Código</summary>
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a>X_1 <span class="op">=</span> multivariate_normal(mean<span class="op">=</span>[<span class="dv">15</span>, <span class="dv">20</span>],</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a>                          seed<span class="op">=</span><span class="dv">0</span>,</span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a>                          cov<span class="op">=</span>[[<span class="dv">3</span>, <span class="op">-</span><span class="dv">3</span>], [<span class="op">-</span><span class="dv">3</span>, <span class="dv">8</span>]]).rvs(<span class="dv">1000</span>)</span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a>X_2 <span class="op">=</span> multivariate_normal(mean<span class="op">=</span>[<span class="dv">5</span>, <span class="dv">5</span>],</span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a>                          seed<span class="op">=</span><span class="dv">0</span>,</span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a>                          cov<span class="op">=</span>[[<span class="dv">4</span>, <span class="dv">0</span>], [<span class="dv">0</span>, <span class="dv">2</span>]]).rvs(<span class="dv">1000</span>)</span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a>T <span class="op">=</span> np.concatenate((X_1, X_2))</span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a>y_t <span class="op">=</span> np.array([<span class="st">'P'</span>] <span class="op">*</span> X_1.shape[<span class="dv">0</span>] <span class="op">+</span> [<span class="st">'N'</span>] <span class="op">*</span> X_2.shape[<span class="dv">0</span>])</span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a>linear <span class="op">=</span> LinearSVC(dual<span class="op">=</span><span class="va">False</span>).fit(T, y_t)</span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a>w_1, w_2 <span class="op">=</span> linear.coef_[<span class="dv">0</span>]</span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true" tabindex="-1"></a>w_0 <span class="op">=</span> linear.intercept_[<span class="dv">0</span>]</span>
<span id="cb2-12"><a href="#cb2-12" aria-hidden="true" tabindex="-1"></a>g_0 <span class="op">=</span> [<span class="bu">dict</span>(x1<span class="op">=</span>x, x2<span class="op">=</span>y, tipo<span class="op">=</span><span class="st">'g(x)=0'</span>)</span>
<span id="cb2-13"><a href="#cb2-13" aria-hidden="true" tabindex="-1"></a>       <span class="cf">for</span> x, y <span class="kw">in</span> <span class="bu">zip</span>(T[:, <span class="dv">0</span>], (<span class="op">-</span>w_0 <span class="op">-</span> w_1 <span class="op">*</span> T[:, <span class="dv">0</span>]) <span class="op">/</span> w_2)]</span>
<span id="cb2-14"><a href="#cb2-14" aria-hidden="true" tabindex="-1"></a>df <span class="op">=</span> pd.DataFrame(g_0 <span class="op">+</span> <span class="op">\</span></span>
<span id="cb2-15"><a href="#cb2-15" aria-hidden="true" tabindex="-1"></a>                  [<span class="bu">dict</span>(x1<span class="op">=</span>x, x2<span class="op">=</span>y, clase<span class="op">=</span><span class="st">'P'</span>) <span class="cf">for</span> x, y <span class="kw">in</span> X_1] <span class="op">+</span> <span class="op">\</span></span>
<span id="cb2-16"><a href="#cb2-16" aria-hidden="true" tabindex="-1"></a>                  [<span class="bu">dict</span>(x1<span class="op">=</span>x, x2<span class="op">=</span>y, clase<span class="op">=</span><span class="st">'N'</span>) <span class="cf">for</span> x, y <span class="kw">in</span> X_2]</span>
<span id="cb2-17"><a href="#cb2-17" aria-hidden="true" tabindex="-1"></a>                 )</span>
<span id="cb2-18"><a href="#cb2-18" aria-hidden="true" tabindex="-1"></a>ax <span class="op">=</span> sns.scatterplot(data<span class="op">=</span>df, x<span class="op">=</span><span class="st">'x1'</span>, y<span class="op">=</span><span class="st">'x2'</span>, hue<span class="op">=</span><span class="st">'clase'</span>, legend<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb2-19"><a href="#cb2-19" aria-hidden="true" tabindex="-1"></a>sns.lineplot(data<span class="op">=</span>df, x<span class="op">=</span><span class="st">'x1'</span>, y<span class="op">=</span><span class="st">'x2'</span>, ax<span class="op">=</span>ax, hue<span class="op">=</span><span class="st">'tipo'</span>, palette<span class="op">=</span>[<span class="st">'k'</span>], legend<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb2-20"><a href="#cb2-20" aria-hidden="true" tabindex="-1"></a>_ <span class="op">=</span> ax.axis(<span class="st">'equal'</span>)</span></code><button title="Copiar al portapapeles" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div id="fig-lineal-discriminante" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-lineal-discriminante-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="09Lineal_files/figure-html/fig-lineal-discriminante-output-1.png" width="582" height="428" class="figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-lineal-discriminante-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figura&nbsp;9.1: Función Discriminante
</figcaption>
</figure>
</div>
</div>
</div>
</section>
<section id="sec-geometria-funcion-decision" class="level3" data-number="9.3.2">
<h3 data-number="9.3.2" class="anchored" data-anchor-id="sec-geometria-funcion-decision"><span class="header-section-number">9.3.2</span> Geometría de la Función de Decisión</h3>
<p>La función discriminante <span class="math inline">\(g(\mathbf x) = \mathbf w \cdot \mathbf x + w_0\)</span> tiene una representación gráfica. Lo primero que se observa es que los parámetros <span class="math inline">\(\mathbf w\)</span> viven en al mismo espacio que los datos, tal y como se puede observar en la <a href="#fig-lineal-repr-df" class="quarto-xref">Figura&nbsp;<span>9.2</span></a>.</p>
<div id="cell-fig-lineal-repr-df" class="cell" data-execution_count="4">
<details class="code-fold">
<summary>Código</summary>
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a>_ <span class="op">=</span> pd.DataFrame([<span class="bu">dict</span>(x1<span class="op">=</span>w_1, x2<span class="op">=</span>w_2, clase<span class="op">=</span><span class="st">'w'</span>)])</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>df <span class="op">=</span> pd.concat((df, _), axis<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a>ax <span class="op">=</span> sns.scatterplot(data<span class="op">=</span>df, x<span class="op">=</span><span class="st">'x1'</span>, y<span class="op">=</span><span class="st">'x2'</span>, hue<span class="op">=</span><span class="st">'clase'</span>, legend<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a>sns.lineplot(data<span class="op">=</span>df, x<span class="op">=</span><span class="st">'x1'</span>, y<span class="op">=</span><span class="st">'x2'</span>, ax<span class="op">=</span>ax, hue<span class="op">=</span><span class="st">'tipo'</span>, palette<span class="op">=</span>[<span class="st">'k'</span>], legend<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a>_ <span class="op">=</span> ax.axis(<span class="st">'equal'</span>)</span></code><button title="Copiar al portapapeles" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div id="fig-lineal-repr-df" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-lineal-repr-df-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="09Lineal_files/figure-html/fig-lineal-repr-df-output-1.png" width="582" height="428" class="figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-lineal-repr-df-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figura&nbsp;9.2: Función discriminante
</figcaption>
</figure>
</div>
</div>
</div>
<p>Siguiendo con la descripción, los parámetros <span class="math inline">\(\mathbf w\)</span> y la función <span class="math inline">\(g(\mathbf x)\)</span> son ortogonales, tal y como se muestra en la <a href="#fig-lineal-ort" class="quarto-xref">Figura&nbsp;<span>9.3</span></a>. Analiticamente la ortogonalidad se define de la siguiente manera. Sea <span class="math inline">\(\mathbf x_a\)</span> y <span class="math inline">\(\mathbf x_b\)</span> dos puntos en <span class="math inline">\(g(\mathbf x)=0\)</span>, es decir,</p>
<p><span class="math display">\[
\begin{split}
g(\mathbf x_a) &amp;= g(\mathbf x_b) \\
\mathbf w \cdot \mathbf x_a + w_0 &amp;= \mathbf w \cdot \mathbf x_b + w_0\\
\mathbf w \cdot (\mathbf x_a -  \mathbf x_b) &amp;= 0,
\end{split}
\]</span></p>
<p>donde el vector <span class="math inline">\(\mathbf x_a -  \mathbf x_b\)</span> es paralelo a <span class="math inline">\(g(\mathbf x)=0\)</span>, ortogonal a <span class="math inline">\(\mathbf w\)</span> y el sub-espacio generado por <span class="math inline">\(\mathbf w \cdot (\mathbf x_a -  \mathbf x_b) = 0\)</span> pasa por el origen.</p>
<div id="cell-fig-lineal-ort" class="cell" data-execution_count="5">
<details class="code-fold">
<summary>Código</summary>
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a>w <span class="op">=</span> np.array([w_1, w_2]) <span class="op">/</span> np.linalg.norm([w_1, w_2])</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a>len_0 <span class="op">=</span> w_0 <span class="op">/</span> np.linalg.norm([w_1, w_2])</span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a>df <span class="op">=</span> pd.DataFrame(g_0 <span class="op">+</span> <span class="op">\</span></span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a>                  [<span class="bu">dict</span>(x1<span class="op">=</span>x, x2<span class="op">=</span>y, clase<span class="op">=</span><span class="st">'P'</span>) <span class="cf">for</span> x, y <span class="kw">in</span> X_1] <span class="op">+</span> <span class="op">\</span></span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a>                  [<span class="bu">dict</span>(x1<span class="op">=</span>x, x2<span class="op">=</span>y, clase<span class="op">=</span><span class="st">'N'</span>) <span class="cf">for</span> x, y <span class="kw">in</span> X_2] <span class="op">+</span> <span class="op">\</span></span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a>                  [<span class="bu">dict</span>(x1<span class="op">=</span><span class="dv">0</span>, x2<span class="op">=</span><span class="dv">0</span>, tipo<span class="op">=</span><span class="st">'lw'</span>),</span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a>                   <span class="bu">dict</span>(x1<span class="op">=-</span>w[<span class="dv">0</span>]<span class="op">*</span>len_0, x2<span class="op">=-</span>w[<span class="dv">1</span>]<span class="op">*</span>len_0, tipo<span class="op">=</span><span class="st">'lw'</span>)]</span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a>                 )</span>
<span id="cb4-9"><a href="#cb4-9" aria-hidden="true" tabindex="-1"></a>ax <span class="op">=</span> sns.scatterplot(data<span class="op">=</span>df, x<span class="op">=</span><span class="st">'x1'</span>, y<span class="op">=</span><span class="st">'x2'</span>, hue<span class="op">=</span><span class="st">'clase'</span>, legend<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb4-10"><a href="#cb4-10" aria-hidden="true" tabindex="-1"></a>sns.lineplot(data<span class="op">=</span>df, x<span class="op">=</span><span class="st">'x1'</span>, y<span class="op">=</span><span class="st">'x2'</span>, ax<span class="op">=</span>ax, hue<span class="op">=</span><span class="st">'tipo'</span>,</span>
<span id="cb4-11"><a href="#cb4-11" aria-hidden="true" tabindex="-1"></a>             palette<span class="op">=</span>[<span class="st">'k'</span>] <span class="op">+</span> sns.color_palette()[<span class="dv">2</span>:<span class="dv">3</span>],</span>
<span id="cb4-12"><a href="#cb4-12" aria-hidden="true" tabindex="-1"></a>             legend<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb4-13"><a href="#cb4-13" aria-hidden="true" tabindex="-1"></a>_ <span class="op">=</span> ax.axis(<span class="st">'equal'</span>)</span></code><button title="Copiar al portapapeles" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div id="fig-lineal-ort" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-lineal-ort-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="09Lineal_files/figure-html/fig-lineal-ort-output-1.png" width="582" height="428" class="figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-lineal-ort-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figura&nbsp;9.3: Visualizando que <span class="math inline">\(\mathbf w\)</span> y la función discriminante son ortogonales.
</figcaption>
</figure>
</div>
</div>
</div>
<p>En la figura anterior, <span class="math inline">\(\ell \mathbf w\)</span> corresponde al vector <span class="math inline">\(\mathbf w\)</span> multiplicado por un factor <span class="math inline">\(\ell\)</span> de tal manera que intersecte con <span class="math inline">\(g(\mathbf x)=0.\)</span> El factor <span class="math inline">\(\ell\)</span> corresponde a la distancia que hay del origen a <span class="math inline">\(g(\mathbf x)=0\)</span> la cual es <span class="math inline">\(\ell = \frac{w_0}{\mid\mid \mathbf w \mid\mid}.\)</span> El signo de <span class="math inline">\(\ell\)</span> indica el lado donde se encuentra el origen con respecto a <span class="math inline">\(g(\mathbf x)=0.\)</span></p>
<p>La <a href="#fig-lineal-dist-hyp" class="quarto-xref">Figura&nbsp;<span>9.4</span></a> muestra en rojo la línea generada por <span class="math inline">\(\mathbf w \cdot \mathbf x=0\)</span>, la función discriminante <span class="math inline">\(g(\mathbf x)=0\)</span> (negro), la línea puntuada muestra la distancia entre ellas, que corresponde a <span class="math inline">\(\ell\)</span> y el vector <span class="math inline">\(\mathbf w\)</span>. Visualmente, se observa que <span class="math inline">\(\mathbf w\)</span> está pegado a la línea roja, pero esto solo es un efecto de la resolución y estos elementos no se tocan.</p>
<div id="cell-fig-lineal-dist-hyp" class="cell" data-execution_count="6">
<details class="code-fold">
<summary>Código</summary>
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a>vec <span class="op">=</span> np.array([<span class="dv">1</span>, (<span class="op">-</span> w_1 <span class="op">*</span> <span class="dv">1</span>) <span class="op">/</span> w_2])</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a>x_max <span class="op">=</span> T[:, <span class="dv">0</span>].<span class="bu">max</span>()</span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a>length <span class="op">=</span> np.linalg.norm(np.array([x_max, (<span class="op">-</span>w_0 <span class="op">-</span> w_1 <span class="op">*</span> x_max) <span class="op">/</span> w_2]) <span class="op">-</span></span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a>                        np.array([<span class="op">-</span>w[<span class="dv">0</span>]<span class="op">*</span>len_0, <span class="op">-</span>w[<span class="dv">1</span>]<span class="op">*</span>len_0]))</span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a>vec_der <span class="op">=</span> length <span class="op">*</span> vec <span class="op">/</span> np.linalg.norm(vec)</span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a>x_min <span class="op">=</span> T[:, <span class="dv">0</span>].<span class="bu">min</span>()</span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a>length <span class="op">=</span> np.linalg.norm(np.array([x_min, (<span class="op">-</span>w_0 <span class="op">-</span> w_1 <span class="op">*</span> x_min) <span class="op">/</span> w_2]) <span class="op">-</span></span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a>                        np.array([<span class="op">-</span>w[<span class="dv">0</span>]<span class="op">*</span>len_0, <span class="op">-</span>w[<span class="dv">1</span>]<span class="op">*</span>len_0]))</span>
<span id="cb5-9"><a href="#cb5-9" aria-hidden="true" tabindex="-1"></a>vec_izq <span class="op">=</span> <span class="op">-</span>length <span class="op">*</span> vec <span class="op">/</span> np.linalg.norm(vec)</span>
<span id="cb5-10"><a href="#cb5-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-11"><a href="#cb5-11" aria-hidden="true" tabindex="-1"></a>g <span class="op">=</span> [<span class="bu">dict</span>(x1<span class="op">=</span>x, x2<span class="op">=</span>(<span class="op">-</span> w_1 <span class="op">*</span> x) <span class="op">/</span> w_2, tipo<span class="op">=</span><span class="st">'wx=0'</span>)</span>
<span id="cb5-12"><a href="#cb5-12" aria-hidden="true" tabindex="-1"></a>     <span class="cf">for</span> x <span class="kw">in</span> np.linspace(vec_izq[<span class="dv">0</span>], vec_der[<span class="dv">0</span>])]</span>
<span id="cb5-13"><a href="#cb5-13" aria-hidden="true" tabindex="-1"></a>df <span class="op">=</span> pd.DataFrame([<span class="bu">dict</span>(x1<span class="op">=</span>x, x2<span class="op">=</span>y, clase<span class="op">=</span><span class="st">'P'</span>) <span class="cf">for</span> x, y <span class="kw">in</span> X_1] <span class="op">+</span> <span class="op">\</span></span>
<span id="cb5-14"><a href="#cb5-14" aria-hidden="true" tabindex="-1"></a>                  [<span class="bu">dict</span>(x1<span class="op">=</span>x, x2<span class="op">=</span>y, clase<span class="op">=</span><span class="st">'N'</span>) <span class="cf">for</span> x, y <span class="kw">in</span> X_2] <span class="op">+\</span></span>
<span id="cb5-15"><a href="#cb5-15" aria-hidden="true" tabindex="-1"></a>                  [<span class="bu">dict</span>(x1<span class="op">=</span>w_1, x2<span class="op">=</span>w_2, clase<span class="op">=</span><span class="st">'w'</span>)] <span class="op">+\</span></span>
<span id="cb5-16"><a href="#cb5-16" aria-hidden="true" tabindex="-1"></a>                  g_0 <span class="op">+</span> g)</span>
<span id="cb5-17"><a href="#cb5-17" aria-hidden="true" tabindex="-1"></a>ax <span class="op">=</span> sns.scatterplot(data<span class="op">=</span>df, x<span class="op">=</span><span class="st">'x1'</span>, y<span class="op">=</span><span class="st">'x2'</span>, hue<span class="op">=</span><span class="st">'clase'</span>, legend<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb5-18"><a href="#cb5-18" aria-hidden="true" tabindex="-1"></a>sns.lineplot(data<span class="op">=</span>df, x<span class="op">=</span><span class="st">'x1'</span>, y<span class="op">=</span><span class="st">'x2'</span>, ax<span class="op">=</span>ax, hue<span class="op">=</span><span class="st">'tipo'</span>,</span>
<span id="cb5-19"><a href="#cb5-19" aria-hidden="true" tabindex="-1"></a>             palette<span class="op">=</span>[<span class="st">'k'</span>] <span class="op">+</span> sns.color_palette()[<span class="dv">3</span>:<span class="dv">4</span>],</span>
<span id="cb5-20"><a href="#cb5-20" aria-hidden="true" tabindex="-1"></a>             legend<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb5-21"><a href="#cb5-21" aria-hidden="true" tabindex="-1"></a>ax.plot([vec_der[<span class="dv">0</span>], x_max], [vec_der[<span class="dv">1</span>], (<span class="op">-</span>w_0 <span class="op">-</span> w_1 <span class="op">*</span> x_max) <span class="op">/</span> w_2], <span class="st">'--'</span>,</span>
<span id="cb5-22"><a href="#cb5-22" aria-hidden="true" tabindex="-1"></a>        color<span class="op">=</span>sns.color_palette()[<span class="dv">4</span>])</span>
<span id="cb5-23"><a href="#cb5-23" aria-hidden="true" tabindex="-1"></a>_ <span class="op">=</span> ax.axis(<span class="st">'equal'</span>)</span></code><button title="Copiar al portapapeles" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div id="fig-lineal-dist-hyp" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-lineal-dist-hyp-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="09Lineal_files/figure-html/fig-lineal-dist-hyp-output-1.png" width="582" height="427" class="figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-lineal-dist-hyp-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figura&nbsp;9.4: Geometría de la función discriminante.
</figcaption>
</figure>
</div>
</div>
</div>
<p>Finalmente, será de utilidad representar a cada punto en <span class="math inline">\(\mathcal D\)</span> de la siguiente manera</p>
<p><span class="math display">\[
\mathbf x = \mathbf x_g + \ell \frac{\mathbf w}{\mid\mid \mathbf w \mid\mid},
\]</span></p>
<p>donde <span class="math inline">\(\mathbf x_g\)</span> corresponde a la proyección en el hiperplano (<span class="math inline">\(g(\mathbf x) = 0\)</span>) de <span class="math inline">\(\mathbf x\)</span> y <span class="math inline">\(\ell\)</span> es la distancia que hay del hiperplano a <span class="math inline">\(\mathbf x\)</span>. Utilizando esta representación se puede derivar la distancia <span class="math inline">\(\ell\)</span> de <span class="math inline">\(\mathbf x\)</span> con el siguiente procedimiento.</p>
<p><span id="eq-distancia-hiperplano"><span class="math display">\[
\begin{split}
g(\mathbf x) &amp;= g(\mathbf x_g + \ell \frac{\mathbf w}{\mid\mid \mathbf w \mid\mid})\\
&amp;= \mathbf w \cdot (\mathbf x_g + \ell \frac{\mathbf w}{\mid\mid \mathbf w \mid\mid}) + w_0\\
&amp;= \mathbf w \cdot (\mathbf x_g + \ell \frac{\mathbf w}{\mid\mid \mathbf w \mid\mid})\\
&amp;= \mathbf w \cdot \mathbf x_g + \ell \mathbf w \cdot \frac{\mathbf w}{\mid\mid \mathbf w \mid\mid}\\
&amp;= \ell \mathbf w \cdot \frac{\mathbf w}{\mid\mid \mathbf w \mid\mid}\\
&amp;= \ell \mid\mid\mathbf w\mid\mid\\
\ell &amp;= \frac{g(\mathbf x)}{\mid\mid\mathbf w \mid\mid}
\end{split}
\tag{9.1}\]</span></span></p>
<p>Como ya se había visto la distancia del origen al hiperplano está dada por <span class="math inline">\(\ell_0 = \frac{w_0}{\mid\mid\mathbf w \mid\mid}\)</span> y de cualquier elemento por <span class="math inline">\(\ell_{\mathbf x} = \frac{g(\mathbf x)}{\mid\mid\mathbf w \mid\mid}.\)</span> La <a href="#fig-lineal-dis-ele" class="quarto-xref">Figura&nbsp;<span>9.5</span></a> muestra la <span class="math inline">\(\ell_{\mathbf x}\)</span> en un elemento de la clase negativa. Se puede observar el punto <span class="math inline">\(\mathbf x_g\)</span> que es donde intersecta la línea con el hiperplano.</p>
<div id="cell-fig-lineal-dis-ele" class="cell" data-execution_count="7">
<details class="code-fold">
<summary>Código</summary>
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a>point <span class="op">=</span> X_2[X_2.argmax(axis<span class="op">=</span><span class="dv">0</span>)[<span class="dv">1</span>]]</span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a>point_g <span class="op">=</span> vec <span class="op">*</span>  np.dot(point, vec) <span class="op">/</span> np.dot(vec, vec) <span class="op">-</span> len_0 <span class="op">*</span> w</span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a>df <span class="op">=</span> pd.DataFrame(g_0 <span class="op">+</span> <span class="op">\</span></span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a>                  [<span class="bu">dict</span>(x1<span class="op">=</span>x, x2<span class="op">=</span>y, clase<span class="op">=</span><span class="st">'P'</span>) <span class="cf">for</span> x, y <span class="kw">in</span> X_1] <span class="op">+\</span></span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a>                  [<span class="bu">dict</span>(x1<span class="op">=</span>x, x2<span class="op">=</span>y, clase<span class="op">=</span><span class="st">'N'</span>) <span class="cf">for</span> x, y <span class="kw">in</span> X_2] <span class="op">+\</span></span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a>                  [<span class="bu">dict</span>(x1<span class="op">=</span>point_g[<span class="dv">0</span>], x2<span class="op">=</span>point_g[<span class="dv">1</span>], tipo<span class="op">=</span><span class="st">'lx'</span>)] <span class="op">+\</span></span>
<span id="cb6-7"><a href="#cb6-7" aria-hidden="true" tabindex="-1"></a>                  [<span class="bu">dict</span>(x1<span class="op">=</span>point[<span class="dv">0</span>], x2<span class="op">=</span>point[<span class="dv">1</span>], tipo<span class="op">=</span><span class="st">'lx'</span>)]                  </span>
<span id="cb6-8"><a href="#cb6-8" aria-hidden="true" tabindex="-1"></a>                 )</span>
<span id="cb6-9"><a href="#cb6-9" aria-hidden="true" tabindex="-1"></a>ax <span class="op">=</span> sns.scatterplot(data<span class="op">=</span>df, x<span class="op">=</span><span class="st">'x1'</span>, y<span class="op">=</span><span class="st">'x2'</span>, hue<span class="op">=</span><span class="st">'clase'</span>, legend<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb6-10"><a href="#cb6-10" aria-hidden="true" tabindex="-1"></a>sns.lineplot(data<span class="op">=</span>df, x<span class="op">=</span><span class="st">'x1'</span>, y<span class="op">=</span><span class="st">'x2'</span>, ax<span class="op">=</span>ax,</span>
<span id="cb6-11"><a href="#cb6-11" aria-hidden="true" tabindex="-1"></a>             hue<span class="op">=</span><span class="st">'tipo'</span>,palette<span class="op">=</span>[<span class="st">'k'</span>] <span class="op">+</span> sns.color_palette()[<span class="dv">4</span>:<span class="dv">5</span>], legend<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb6-12"><a href="#cb6-12" aria-hidden="true" tabindex="-1"></a>_ <span class="op">=</span> ax.axis(<span class="st">'equal'</span>)</span></code><button title="Copiar al portapapeles" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div id="fig-lineal-dis-ele" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-lineal-dis-ele-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="09Lineal_files/figure-html/fig-lineal-dis-ele-output-1.png" width="582" height="428" class="figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-lineal-dis-ele-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figura&nbsp;9.5: Distancia (<span class="math inline">\(\ell_{\mathbf x} = x\)</span>) de un elemento al hiperplano
</figcaption>
</figure>
</div>
</div>
</div>
<p>Considerando que el problema mostrado en la figura anterior está en <span class="math inline">\(\mathbb R^2\)</span>, entonces <span class="math inline">\(\mathbf x_g\)</span> está dado por</p>
<p><span class="math display">\[
\mathbf x_g = \frac{\mathbf x \cdot \mathbf x_0}{\mathbf x_0 \cdot \mathbf x_0} \mathbf x_0 - \ell_0 \frac{\mathbf w}{\mid\mid\mathbf w \mid\mid},
\]</span></p>
<p>donde <span class="math inline">\(\ell_0\)</span> es la distancia del origen al hiperplano y <span class="math inline">\(\mathbf x_0\)</span> es cualquier vector que está en <span class="math inline">\(\mathbf x_0 \cdot \mathbf w=0.\)</span> Para dimensiones mayores el término <span class="math inline">\(\frac{\mathbf x \cdot \mathbf x_0}{\mathbf x_0 \cdot \mathbf x_0}\)</span> es la proyección al hiperplano <span class="math inline">\(A\)</span> tal que <span class="math inline">\(A \mathbf w = 0.\)</span></p>
</section>
<section id="sec-multiples-clases" class="level3" data-number="9.3.3">
<h3 data-number="9.3.3" class="anchored" data-anchor-id="sec-multiples-clases"><span class="header-section-number">9.3.3</span> Múltiples Clases</h3>
<p>Una manera de tratar un problema de <span class="math inline">\(K\)</span> clases, es convertirlo en <span class="math inline">\(K\)</span> problemas de clasificación binarios, a este procedimiento se le conoce como <em>Uno vs Resto</em>. La idea es entrenar <span class="math inline">\(K\)</span> clasificadores donde la clase positiva corresponde a cada una de las clases y la clase de negativa se construye con todas las clases que no son la clase positiva en esa iteración. Finalmente, la clase predicha corresponde al clasificador que tiene el valor máximo en la función discriminante.</p>
<p>La <a href="#fig-lineal-multiclase" class="quarto-xref">Figura&nbsp;<span>9.6</span></a> ejemplifica el comportamiento de esta técnica en un problema de tres clases y utilizando un clasificador con discrimitante lineal. En la figura se muestra las tres funciones discriminantes <span class="math inline">\(g_k(\mathbf x)=0\)</span>, los parámetros escalados de esas funciones, i.e., <span class="math inline">\(\ell_k \mathbf w_k\)</span> y los datos. Por ejemplo se observa como la clase <span class="math inline">\(1\)</span> mostrada en azul, se separa de las otras dos clases con la función <span class="math inline">\(g_1(\mathbf x)=0\)</span>, es decir, para <span class="math inline">\(g_1(\mathbf x)=0\)</span> la clase positiva es <span class="math inline">\(1\)</span> y la clase negativa corresponde a los elementos que corresponde a las clases <span class="math inline">\(2\)</span> y <span class="math inline">\(3.\)</span></p>
<div id="cell-fig-lineal-multiclase" class="cell" data-execution_count="8">
<details class="code-fold">
<summary>Código</summary>
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a>seed <span class="op">=</span> <span class="dv">3</span></span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a>X_1 <span class="op">=</span> multivariate_normal(mean<span class="op">=</span>[<span class="dv">15</span>, <span class="dv">20</span>],</span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a>                          seed<span class="op">=</span>seed,</span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a>                          cov<span class="op">=</span>[[<span class="dv">3</span>, <span class="op">-</span><span class="dv">3</span>], [<span class="op">-</span><span class="dv">3</span>, <span class="dv">8</span>]]).rvs(<span class="dv">1000</span>)</span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a>X_2 <span class="op">=</span> multivariate_normal(mean<span class="op">=</span>[<span class="dv">5</span>, <span class="dv">5</span>],</span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a>                          seed<span class="op">=</span>seed,</span>
<span id="cb7-7"><a href="#cb7-7" aria-hidden="true" tabindex="-1"></a>                          cov<span class="op">=</span>[[<span class="dv">4</span>, <span class="dv">0</span>], [<span class="dv">0</span>, <span class="dv">2</span>]]).rvs(<span class="dv">1000</span>)</span>
<span id="cb7-8"><a href="#cb7-8" aria-hidden="true" tabindex="-1"></a>X_3 <span class="op">=</span> multivariate_normal(mean<span class="op">=</span>[<span class="op">-</span><span class="dv">5</span>, <span class="dv">20</span>],</span>
<span id="cb7-9"><a href="#cb7-9" aria-hidden="true" tabindex="-1"></a>                          seed<span class="op">=</span>seed,</span>
<span id="cb7-10"><a href="#cb7-10" aria-hidden="true" tabindex="-1"></a>                          cov<span class="op">=</span>[[<span class="dv">2</span>, <span class="dv">1</span>], [<span class="dv">1</span>, <span class="dv">2</span>]]).rvs(<span class="dv">1000</span>)</span>
<span id="cb7-11"><a href="#cb7-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-12"><a href="#cb7-12" aria-hidden="true" tabindex="-1"></a>T <span class="op">=</span> np.concatenate((X_1, X_2, X_3))</span>
<span id="cb7-13"><a href="#cb7-13" aria-hidden="true" tabindex="-1"></a>y_t <span class="op">=</span> np.array([<span class="st">'1'</span>] <span class="op">*</span> X_1.shape[<span class="dv">0</span>] <span class="op">+</span> [<span class="st">'2'</span>] <span class="op">*</span> X_2.shape[<span class="dv">0</span>] <span class="op">+</span> [<span class="st">'3'</span>] <span class="op">*</span> X_3.shape[<span class="dv">0</span>])</span>
<span id="cb7-14"><a href="#cb7-14" aria-hidden="true" tabindex="-1"></a>linear <span class="op">=</span> LinearSVC(dual<span class="op">=</span><span class="va">False</span>).fit(T, y_t)</span>
<span id="cb7-15"><a href="#cb7-15" aria-hidden="true" tabindex="-1"></a>g_0 <span class="op">=</span> []</span>
<span id="cb7-16"><a href="#cb7-16" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i, (w, w_0) <span class="kw">in</span> <span class="bu">enumerate</span>(<span class="bu">zip</span>(linear.coef_, linear.intercept_)):</span>
<span id="cb7-17"><a href="#cb7-17" aria-hidden="true" tabindex="-1"></a>    w_1, w_2 <span class="op">=</span> w</span>
<span id="cb7-18"><a href="#cb7-18" aria-hidden="true" tabindex="-1"></a>    g_0 <span class="op">+=</span> [<span class="bu">dict</span>(x1<span class="op">=</span>x, x2<span class="op">=</span>y, tipo<span class="op">=</span><span class="ss">f'g</span><span class="sc">{</span>i<span class="op">+</span><span class="dv">1</span><span class="sc">}</span><span class="ss">(x)=0'</span>)</span>
<span id="cb7-19"><a href="#cb7-19" aria-hidden="true" tabindex="-1"></a>            <span class="cf">for</span> x, y <span class="kw">in</span> <span class="bu">zip</span>(T[:, <span class="dv">0</span>], (<span class="op">-</span>w_0 <span class="op">-</span> w[<span class="dv">0</span>] <span class="op">*</span> T[:, <span class="dv">0</span>]) <span class="op">/</span> w[<span class="dv">1</span>])]</span>
<span id="cb7-20"><a href="#cb7-20" aria-hidden="true" tabindex="-1"></a>W <span class="op">=</span> [<span class="op">-</span>w0 <span class="op">*</span> w <span class="op">/</span> np.linalg.norm(w)<span class="op">**</span><span class="dv">2</span> </span>
<span id="cb7-21"><a href="#cb7-21" aria-hidden="true" tabindex="-1"></a>     <span class="cf">for</span> w, w0 <span class="kw">in</span> <span class="bu">zip</span>(linear.coef_, linear.intercept_)]    </span>
<span id="cb7-22"><a href="#cb7-22" aria-hidden="true" tabindex="-1"></a>df <span class="op">=</span> pd.DataFrame(g_0 <span class="op">+</span> <span class="op">\</span></span>
<span id="cb7-23"><a href="#cb7-23" aria-hidden="true" tabindex="-1"></a>                  [<span class="bu">dict</span>(x1<span class="op">=</span>x, x2<span class="op">=</span>y, clase<span class="op">=</span><span class="st">'1'</span>) <span class="cf">for</span> x, y <span class="kw">in</span> X_1] <span class="op">+\</span></span>
<span id="cb7-24"><a href="#cb7-24" aria-hidden="true" tabindex="-1"></a>                  [<span class="bu">dict</span>(x1<span class="op">=</span>x, x2<span class="op">=</span>y, clase<span class="op">=</span><span class="st">'2'</span>) <span class="cf">for</span> x, y <span class="kw">in</span> X_2] <span class="op">+\</span></span>
<span id="cb7-25"><a href="#cb7-25" aria-hidden="true" tabindex="-1"></a>                  [<span class="bu">dict</span>(x1<span class="op">=</span>x, x2<span class="op">=</span>y, clase<span class="op">=</span><span class="st">'3'</span>) <span class="cf">for</span> x, y <span class="kw">in</span> X_3] <span class="op">+\</span></span>
<span id="cb7-26"><a href="#cb7-26" aria-hidden="true" tabindex="-1"></a>                  [<span class="bu">dict</span>(x1<span class="op">=</span>w_1, x2<span class="op">=</span>w_2, clase<span class="op">=</span><span class="ss">f'lw</span><span class="sc">{</span>i<span class="op">+</span><span class="dv">1</span><span class="sc">}</span><span class="ss">'</span>)</span>
<span id="cb7-27"><a href="#cb7-27" aria-hidden="true" tabindex="-1"></a>                   <span class="cf">for</span> i, (w_1, w_2) <span class="kw">in</span> <span class="bu">enumerate</span>(W)]                  </span>
<span id="cb7-28"><a href="#cb7-28" aria-hidden="true" tabindex="-1"></a>                 )</span>
<span id="cb7-29"><a href="#cb7-29" aria-hidden="true" tabindex="-1"></a>ax <span class="op">=</span> sns.scatterplot(data<span class="op">=</span>df, x<span class="op">=</span><span class="st">'x1'</span>, y<span class="op">=</span><span class="st">'x2'</span>, hue<span class="op">=</span><span class="st">'clase'</span>, legend<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb7-30"><a href="#cb7-30" aria-hidden="true" tabindex="-1"></a>sns.lineplot(data<span class="op">=</span>df, x<span class="op">=</span><span class="st">'x1'</span>, y<span class="op">=</span><span class="st">'x2'</span>, ax<span class="op">=</span>ax, hue<span class="op">=</span><span class="st">'tipo'</span>,</span>
<span id="cb7-31"><a href="#cb7-31" aria-hidden="true" tabindex="-1"></a>             palette<span class="op">=</span>sns.color_palette()[<span class="dv">6</span>:<span class="dv">9</span>], legend<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb7-32"><a href="#cb7-32" aria-hidden="true" tabindex="-1"></a>_ <span class="op">=</span> ax.axis(<span class="st">'equal'</span>)</span></code><button title="Copiar al portapapeles" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div id="fig-lineal-multiclase" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-lineal-multiclase-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="09Lineal_files/figure-html/fig-lineal-multiclase-output-1.png" width="582" height="427" class="figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-lineal-multiclase-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figura&nbsp;9.6: Problema multiclase
</figcaption>
</figure>
</div>
</div>
</div>
</section>
</section>
<section id="sec-svm" class="level2" data-number="9.4">
<h2 data-number="9.4" class="anchored" data-anchor-id="sec-svm"><span class="header-section-number">9.4</span> Máquinas de Soporte Vectorial</h2>
<p>Es momento de describir algunos algoritmos para estimar los parámetros <span class="math inline">\(\mathbf w\)</span>, y <span class="math inline">\(w_0\)</span> empezando por las máquinas de soporte vectorial. En este clasificador se asume un problema binario y las clases están representadas por <span class="math inline">\(-1\)</span> y <span class="math inline">\(1\)</span>, es decir, <span class="math inline">\(y \in \{-1, 1\}\)</span>. Entonces, las máquinas de soporte vectorial tratan de encontrar una función con las siguientes características.</p>
<p>Sea <span class="math inline">\(\mathbf x_i\)</span> un ejemplo que corresponde a la clase <span class="math inline">\(1\)</span> entonces se busca <span class="math inline">\(\mathbf w\)</span> tal que</p>
<p><span class="math display">\[
\mathbf w \cdot \mathbf x_i + w_0 \geq +1.
\]</span></p>
<p>En el caso contrario, es decir, <span class="math inline">\(\mathbf x_i\)</span> un ejemplo de la clase <span class="math inline">\(-1\)</span>, entonces</p>
<p><span class="math display">\[
\mathbf w \cdot \mathbf x_i + w_0 \leq -1.
\]</span></p>
<p>Estas ecuaciones se pueden escribir como</p>
<p><span class="math display">\[
(\mathbf w \cdot \mathbf x_i + w_0) y_i \geq +1,
\]</span></p>
<p>donde <span class="math inline">\((\mathbf x_i, y_i) \in \mathcal D.\)</span></p>
<p>La función discriminante es <span class="math inline">\(g(\mathbf x) = \mathbf w \cdot \mathbf x + w_0\)</span> y la distancia (<a href="#eq-distancia-hiperplano" class="quarto-xref">Ecuación&nbsp;<span>9.1</span></a>) que existe entre cualquier punto <span class="math inline">\(\mathbf x_i\)</span> al discriminante está dada por</p>
<p><span class="math display">\[
\frac{g(\mathbf x_i)}{\mid\mid \mathbf w \mid\mid}y_i.
\]</span></p>
<p>Entonces, se puede ver que lo que se busca es encontrar <span class="math inline">\(\mathbf w\)</span> de tal manera que cualquier punto <span class="math inline">\(\mathbf x_i\)</span> esté lo mas alejada posible del discriminante, esto se logra minimizando <span class="math inline">\(\mathbf w\)</span>, es decir, resolviendo el siguiente problema de optimización:</p>
<p><span class="math display">\[
\min \frac{1}{2} \mid\mid\mathbf w \mid\mid
\]</span></p>
<p>sujeto a <span class="math inline">\((\mathbf w \cdot \mathbf x_i + w_0) y_i \geq +1, \forall (\mathbf x_i, y_i) \in \mathcal D.\)</span></p>
<section id="optimización" class="level3" data-number="9.4.1">
<h3 data-number="9.4.1" class="anchored" data-anchor-id="optimización"><span class="header-section-number">9.4.1</span> Optimización</h3>
<p>Este es un problema de optimización que se puede resolver utilizando multiplicadores de Lagrange lo cual quedaría como</p>
<p><span class="math display">\[
f_p = \frac{1}{2}\mid\mid\mathbf w \mid\mid - \sum_i^N \alpha_i ((\mathbf w \cdot \mathbf x_i + w_0) y_i - 1),
\]</span></p>
<p>donde el mínimo corresponde a maximizar con respecto a <span class="math inline">\(\alpha_i \geq 0\)</span> y minimizar con respecto a <span class="math inline">\(\mathbf w\)</span> y <span class="math inline">\(w_0.\)</span> En esta formulación existe el problema para aquellos problemas donde no es posible encontrar un hiperplano que separa las dos clases. Para estos casos donde no es posible encontrar una separación perfecta se propone utilizar</p>
<p><span class="math display">\[
(\mathbf w \cdot \mathbf x_i + w_0) y_i \geq 1 - \xi_i,
\]</span></p>
<p>donde <span class="math inline">\(\xi\)</span> captura los errores empezando por aquellos elementos que están del lado correcto del hiperplano, pero que no son mayores a <span class="math inline">\(1\)</span>. La <a href="#fig-lineal-xi" class="quarto-xref">Figura&nbsp;<span>9.7</span></a> muestra un ejemplo donde existe un elemento negativo que se encuentra entre la función de decisión y el hiperplano de margen, i.e., el que corresponde a la restricción <span class="math inline">\(\mathbf w \cdot \mathbf x_i + w_0 \geq 1\)</span>, es decir ese punto tiene un <span class="math inline">\(0 &lt; \xi &lt; 1.\)</span> También se observa un elemento positivo que está muy cerca a <span class="math inline">\(g(\mathbf x) = 1.\)</span></p>
<div id="cell-fig-lineal-xi" class="cell" data-execution_count="9">
<details class="code-fold">
<summary>Código</summary>
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a>seed <span class="op">=</span> <span class="dv">2</span></span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a>X_1 <span class="op">=</span> multivariate_normal(mean<span class="op">=</span>[<span class="dv">15</span>, <span class="dv">20</span>], cov<span class="op">=</span>[[<span class="dv">3</span>, <span class="op">-</span><span class="dv">3</span>], [<span class="op">-</span><span class="dv">3</span>, <span class="dv">8</span>]], seed<span class="op">=</span>seed).rvs(<span class="dv">1000</span>)</span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a>X_2 <span class="op">=</span> multivariate_normal(mean<span class="op">=</span>[<span class="dv">8</span>, <span class="dv">8</span>], cov<span class="op">=</span>[[<span class="dv">4</span>, <span class="dv">0</span>], [<span class="dv">0</span>, <span class="dv">2</span>]], seed<span class="op">=</span>seed).rvs(<span class="dv">1000</span>)</span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a>T <span class="op">=</span> np.concatenate((X_1, X_2))</span>
<span id="cb8-5"><a href="#cb8-5" aria-hidden="true" tabindex="-1"></a>y_t <span class="op">=</span> np.array([<span class="st">'P'</span>] <span class="op">*</span> X_1.shape[<span class="dv">0</span>] <span class="op">+</span> [<span class="st">'N'</span>] <span class="op">*</span> X_2.shape[<span class="dv">0</span>])</span>
<span id="cb8-6"><a href="#cb8-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-7"><a href="#cb8-7" aria-hidden="true" tabindex="-1"></a>linear <span class="op">=</span> LinearSVC(dual<span class="op">=</span><span class="va">False</span>).fit(T, y_t)</span>
<span id="cb8-8"><a href="#cb8-8" aria-hidden="true" tabindex="-1"></a>w_1, w_2 <span class="op">=</span> linear.coef_[<span class="dv">0</span>]</span>
<span id="cb8-9"><a href="#cb8-9" aria-hidden="true" tabindex="-1"></a>w_0 <span class="op">=</span> linear.intercept_[<span class="dv">0</span>]</span>
<span id="cb8-10"><a href="#cb8-10" aria-hidden="true" tabindex="-1"></a>w <span class="op">=</span> np.array([w_1, w_2]) <span class="op">/</span> np.linalg.norm([w_1, w_2])</span>
<span id="cb8-11"><a href="#cb8-11" aria-hidden="true" tabindex="-1"></a>g_0 <span class="op">=</span> [<span class="bu">dict</span>(x1<span class="op">=</span>x, x2<span class="op">=</span>y, tipo<span class="op">=</span><span class="st">'g(x)=0'</span>)</span>
<span id="cb8-12"><a href="#cb8-12" aria-hidden="true" tabindex="-1"></a>       <span class="cf">for</span> x, y <span class="kw">in</span> <span class="bu">zip</span>(T[:, <span class="dv">0</span>], (<span class="op">-</span>w_0 <span class="op">-</span> w_1 <span class="op">*</span> T[:, <span class="dv">0</span>]) <span class="op">/</span> w_2)]</span>
<span id="cb8-13"><a href="#cb8-13" aria-hidden="true" tabindex="-1"></a>g_p <span class="op">=</span> [<span class="bu">dict</span>(x1<span class="op">=</span>p[<span class="st">'x1'</span>] <span class="op">+</span> w[<span class="dv">0</span>], x2<span class="op">=</span>p[<span class="st">'x2'</span>] <span class="op">+</span> w[<span class="dv">1</span>], tipo<span class="op">=</span><span class="st">'g(x)=1'</span>)</span>
<span id="cb8-14"><a href="#cb8-14" aria-hidden="true" tabindex="-1"></a>       <span class="cf">for</span> p <span class="kw">in</span> g_0]</span>
<span id="cb8-15"><a href="#cb8-15" aria-hidden="true" tabindex="-1"></a>g_n <span class="op">=</span> [<span class="bu">dict</span>(x1<span class="op">=</span>p[<span class="st">'x1'</span>] <span class="op">-</span> w[<span class="dv">0</span>], x2<span class="op">=</span>p[<span class="st">'x2'</span>] <span class="op">-</span> w[<span class="dv">1</span>], tipo<span class="op">=</span><span class="st">'g(x)=-1'</span>)</span>
<span id="cb8-16"><a href="#cb8-16" aria-hidden="true" tabindex="-1"></a>       <span class="cf">for</span> p <span class="kw">in</span> g_0]</span>
<span id="cb8-17"><a href="#cb8-17" aria-hidden="true" tabindex="-1"></a>df <span class="op">=</span> pd.DataFrame(g_0 <span class="op">+</span> g_p <span class="op">+</span> g_n <span class="op">+\</span></span>
<span id="cb8-18"><a href="#cb8-18" aria-hidden="true" tabindex="-1"></a>                  [<span class="bu">dict</span>(x1<span class="op">=</span>x, x2<span class="op">=</span>y, clase<span class="op">=</span><span class="st">'P'</span>) <span class="cf">for</span> x, y <span class="kw">in</span> X_1] <span class="op">+</span> <span class="op">\</span></span>
<span id="cb8-19"><a href="#cb8-19" aria-hidden="true" tabindex="-1"></a>                  [<span class="bu">dict</span>(x1<span class="op">=</span>x, x2<span class="op">=</span>y, clase<span class="op">=</span><span class="st">'N'</span>) <span class="cf">for</span> x, y <span class="kw">in</span> X_2]</span>
<span id="cb8-20"><a href="#cb8-20" aria-hidden="true" tabindex="-1"></a>                 )</span>
<span id="cb8-21"><a href="#cb8-21" aria-hidden="true" tabindex="-1"></a>ax <span class="op">=</span> sns.scatterplot(data<span class="op">=</span>df, x<span class="op">=</span><span class="st">'x1'</span>, y<span class="op">=</span><span class="st">'x2'</span>, hue<span class="op">=</span><span class="st">'clase'</span>, legend<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb8-22"><a href="#cb8-22" aria-hidden="true" tabindex="-1"></a>sns.lineplot(data<span class="op">=</span>df, x<span class="op">=</span><span class="st">'x1'</span>, y<span class="op">=</span><span class="st">'x2'</span>, ax<span class="op">=</span>ax,</span>
<span id="cb8-23"><a href="#cb8-23" aria-hidden="true" tabindex="-1"></a>             hue<span class="op">=</span><span class="st">'tipo'</span>, palette<span class="op">=</span>[<span class="st">'k'</span>] <span class="op">+</span> sns.color_palette()[<span class="dv">2</span>:<span class="dv">4</span>], legend<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb8-24"><a href="#cb8-24" aria-hidden="true" tabindex="-1"></a>_ <span class="op">=</span> ax.axis(<span class="st">'equal'</span>)</span></code><button title="Copiar al portapapeles" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div id="fig-lineal-xi" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-lineal-xi-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="09Lineal_files/figure-html/fig-lineal-xi-output-1.png" width="582" height="427" class="figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-lineal-xi-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figura&nbsp;9.7: Hiperplanos
</figcaption>
</figure>
</div>
</div>
</div>
<p>Continuando con el problema de optimización, en las condiciones anteriores la función a optimizar es <span class="math inline">\(\min \frac{1}{2} \mid\mid\mathbf w \mid\mid + C \sum_i^N \xi_i,\)</span> utilizando multiplicadores de Lagrange queda como</p>
<p><span class="math display">\[
f_p = \frac{1}{2}\mid\mid\mathbf w \mid\mid - \sum_i^N \alpha_i ((\mathbf w \cdot \mathbf x_i + w_0) y_i - 1 + \xi_i) - \sum_i^N \beta_i \xi_i.
\]</span></p>
<p>Se observa que el parámetro <span class="math inline">\(C\)</span> controla la penalización que se hace a los elementos que se encuentran en el lado incorrecto del hiperplano o dentro del margen. La <a href="#fig-lineal-hip-c" class="quarto-xref">Figura&nbsp;<span>9.8</span></a> muestra el hiperplano generado utilizando <span class="math inline">\(C=1\)</span> y <span class="math inline">\(C=0.01.\)</span> Se observa como el elemento que está correctamente clasificado en <span class="math inline">\(C=1\)</span> pasa al lado incorrecto del hiperplano, ademas se ve como la función de decisión rota cuando el valor cambia.</p>
<div id="cell-fig-lineal-hip-c" class="cell" data-execution_count="10">
<details class="code-fold">
<summary>Código</summary>
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> k, (C, legend) <span class="kw">in</span> <span class="bu">enumerate</span>(<span class="bu">zip</span>([<span class="dv">1</span>, <span class="fl">0.01</span>], [<span class="va">False</span>, <span class="va">True</span>])):</span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a>     linear <span class="op">=</span> LinearSVC(dual<span class="op">=</span><span class="va">False</span>, C<span class="op">=</span>C).fit(T, y_t)</span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a>     w_1, w_2 <span class="op">=</span> linear.coef_[<span class="dv">0</span>]</span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a>     w_0 <span class="op">=</span> linear.intercept_[<span class="dv">0</span>]</span>
<span id="cb9-5"><a href="#cb9-5" aria-hidden="true" tabindex="-1"></a>     w <span class="op">=</span> np.array([w_1, w_2]) <span class="op">/</span> np.linalg.norm([w_1, w_2])</span>
<span id="cb9-6"><a href="#cb9-6" aria-hidden="true" tabindex="-1"></a>     g_0 <span class="op">=</span> [<span class="bu">dict</span>(x1<span class="op">=</span>x, x2<span class="op">=</span>y, tipo<span class="op">=</span><span class="st">'g(x)=0'</span>)</span>
<span id="cb9-7"><a href="#cb9-7" aria-hidden="true" tabindex="-1"></a>          <span class="cf">for</span> x, y <span class="kw">in</span> <span class="bu">zip</span>(T[:, <span class="dv">0</span>], (<span class="op">-</span>w_0 <span class="op">-</span> w_1 <span class="op">*</span> T[:, <span class="dv">0</span>]) <span class="op">/</span> w_2)]</span>
<span id="cb9-8"><a href="#cb9-8" aria-hidden="true" tabindex="-1"></a>     g_p <span class="op">=</span> [<span class="bu">dict</span>(x1<span class="op">=</span>p[<span class="st">'x1'</span>] <span class="op">+</span> w[<span class="dv">0</span>], x2<span class="op">=</span>p[<span class="st">'x2'</span>] <span class="op">+</span> w[<span class="dv">1</span>], tipo<span class="op">=</span><span class="st">'g(x)=1'</span>)</span>
<span id="cb9-9"><a href="#cb9-9" aria-hidden="true" tabindex="-1"></a>          <span class="cf">for</span> p <span class="kw">in</span> g_0]</span>
<span id="cb9-10"><a href="#cb9-10" aria-hidden="true" tabindex="-1"></a>     g_n <span class="op">=</span> [<span class="bu">dict</span>(x1<span class="op">=</span>p[<span class="st">'x1'</span>] <span class="op">-</span> w[<span class="dv">0</span>], x2<span class="op">=</span>p[<span class="st">'x2'</span>] <span class="op">-</span> w[<span class="dv">1</span>], tipo<span class="op">=</span><span class="st">'g(x)=-1'</span>)</span>
<span id="cb9-11"><a href="#cb9-11" aria-hidden="true" tabindex="-1"></a>          <span class="cf">for</span> p <span class="kw">in</span> g_0]</span>
<span id="cb9-12"><a href="#cb9-12" aria-hidden="true" tabindex="-1"></a>     df <span class="op">=</span> pd.DataFrame(g_0 <span class="op">+</span> g_p <span class="op">+</span> g_n <span class="op">+\</span></span>
<span id="cb9-13"><a href="#cb9-13" aria-hidden="true" tabindex="-1"></a>                    [<span class="bu">dict</span>(x1<span class="op">=</span>x, x2<span class="op">=</span>y, clase<span class="op">=</span><span class="st">'P'</span>) <span class="cf">for</span> x, y <span class="kw">in</span> X_1] <span class="op">+</span> <span class="op">\</span></span>
<span id="cb9-14"><a href="#cb9-14" aria-hidden="true" tabindex="-1"></a>                    [<span class="bu">dict</span>(x1<span class="op">=</span>x, x2<span class="op">=</span>y, clase<span class="op">=</span><span class="st">'N'</span>) <span class="cf">for</span> x, y <span class="kw">in</span> X_2]</span>
<span id="cb9-15"><a href="#cb9-15" aria-hidden="true" tabindex="-1"></a>                    )</span>
<span id="cb9-16"><a href="#cb9-16" aria-hidden="true" tabindex="-1"></a>     ax <span class="op">=</span> plt.subplot(<span class="dv">1</span>, <span class="dv">2</span>, k <span class="op">+</span> <span class="dv">1</span>)</span>
<span id="cb9-17"><a href="#cb9-17" aria-hidden="true" tabindex="-1"></a>     sns.scatterplot(data<span class="op">=</span>df, x<span class="op">=</span><span class="st">'x1'</span>, y<span class="op">=</span><span class="st">'x2'</span>, hue<span class="op">=</span><span class="st">'clase'</span>, legend<span class="op">=</span>legend, ax<span class="op">=</span>ax)</span>
<span id="cb9-18"><a href="#cb9-18" aria-hidden="true" tabindex="-1"></a>     sns.lineplot(data<span class="op">=</span>df, x<span class="op">=</span><span class="st">'x1'</span>, y<span class="op">=</span><span class="st">'x2'</span>, ax<span class="op">=</span>ax,</span>
<span id="cb9-19"><a href="#cb9-19" aria-hidden="true" tabindex="-1"></a>               hue<span class="op">=</span><span class="st">'tipo'</span>, palette<span class="op">=</span>[<span class="st">'k'</span>] <span class="op">+</span> sns.color_palette()[<span class="dv">2</span>:<span class="dv">4</span>], legend<span class="op">=</span>legend)</span>
<span id="cb9-20"><a href="#cb9-20" aria-hidden="true" tabindex="-1"></a>     ax.axis(<span class="st">'equal'</span>)</span>
<span id="cb9-21"><a href="#cb9-21" aria-hidden="true" tabindex="-1"></a>     ax.set_title(<span class="ss">f'C=</span><span class="sc">{</span>C<span class="sc">}</span><span class="ss">'</span>)</span>
<span id="cb9-22"><a href="#cb9-22" aria-hidden="true" tabindex="-1"></a>     <span class="cf">if</span> legend:</span>
<span id="cb9-23"><a href="#cb9-23" aria-hidden="true" tabindex="-1"></a>          sns.move_legend(ax, <span class="st">"upper left"</span>, bbox_to_anchor<span class="op">=</span>(<span class="dv">1</span>, <span class="fl">0.65</span>))</span>
<span id="cb9-24"><a href="#cb9-24" aria-hidden="true" tabindex="-1"></a>_ <span class="op">=</span> plt.tight_layout()</span></code><button title="Copiar al portapapeles" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div id="fig-lineal-hip-c" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-lineal-hip-c-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="09Lineal_files/figure-html/fig-lineal-hip-c-output-1.png" width="663" height="470" class="figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-lineal-hip-c-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figura&nbsp;9.8: Hiperplanos para diferentes valores de <span class="math inline">\(C\)</span>. Se observa que en <span class="math inline">\(C=0.01\)</span> se clasifica incorrectamente un elemento positivo.
</figcaption>
</figure>
</div>
</div>
</div>

<p>Este problema de optimización cumple con todas las características para poder encontrar su solución optimizando el problema dual. El problema dual corresponde a maximizar <span class="math inline">\(f_p\)</span> con respecto a <span class="math inline">\(\alpha_i,\)</span> sujeto a que las restricciones de que el gradiente de <span class="math inline">\(f_p\)</span> con respecto a <span class="math inline">\(\mid\mid\mathbf w \mid\mid\)</span>, <span class="math inline">\(w_0\)</span> y <span class="math inline">\(\xi_i\)</span> sean cero. Utilizando estas características el problema dual corresponde a</p>
<p><span class="math display">\[
f_d = \sum_i^N \alpha_i - \frac{1}{2} \sum_i^N \sum_j^N \alpha_i \alpha_j y_i y_j \mathbf x_i \cdot \mathbf x_j,
\]</span></p>
<p>sujeto a las restricciones <span class="math inline">\(\sum_i^N \alpha_i y_i = 0\)</span> y <span class="math inline">\(0 \leq \alpha_i \leq C.\)</span></p>
<p>El problema de optimización dual tiene unas características que lo hacen deseable en ciertos casos, por ejemplo, el problema depende del número de ejemplos (<span class="math inline">\(N\)</span>) en lugar de la dimensión. Entonces en problemas donde <span class="math inline">\(d &gt; N\)</span> es más conveniente utilizar el dual.</p>
</section>
<section id="kernel" class="level3" data-number="9.4.2">
<h3 data-number="9.4.2" class="anchored" data-anchor-id="kernel"><span class="header-section-number">9.4.2</span> Kernel</h3>
<p>La otra característica del problema dual es que permite visualizar lo siguiente. Suponiendo que se usa una función <span class="math inline">\(\phi: \mathbb R^d \leftarrow \mathbf R^{\hat d},\)</span> de tal manera, que en el espacio <span class="math inline">\(\phi\)</span> se puede encontrar un hiperplano que separa las clases. Incorporando la función <span class="math inline">\(\phi\)</span> produce la siguiente función a optimizar</p>
<p><span class="math display">\[
f_d = \sum_i^N \alpha_i - \frac{1}{2} \sum_i^N \sum_j^N \alpha_i \alpha_j y_i y_j \phi(\mathbf x_i) \cdot \phi(\mathbf x_j),
\]</span></p>
<p>donde primero se transforman todos los datos al espacio generado por <span class="math inline">\(\phi\)</span> y después se calcula el producto punto. El producto punto se puede cambiar por una función <strong>Kernel</strong>, i.e., <span class="math inline">\(K(\mathbf x_i, \mathbf x_j) = \phi(\mathbf x_i) \cdot \phi(\mathbf x_j)\)</span> lo cual hace que innecesaria la transformación al espacio <span class="math inline">\(\phi.\)</span> Utilizando la función de kernel, el problema de optimización dual queda como:</p>
<p><span class="math display">\[
f_d = \sum_i^N \alpha_i - \frac{1}{2} \sum_i^N \sum_j^N \alpha_i \alpha_j y_i y_j K(\mathbf x_i, \mathbf x_j).
\]</span></p>
<p>La función discriminante está dada por <span class="math inline">\(g(\mathbf x) = \sum_i^N \alpha_i y_i K(\mathbf x_i, \mathbf x),\)</span> donde aquellos elementos donde <span class="math inline">\(\alpha \neq 0\)</span> se les conoce como los vectores de soporte. Estos elementos son los que se encuentran en el margen, dentro del margen y en el lado incorrecto de la función discriminante.</p>
<p>La <a href="#fig-lineal-kernel" class="quarto-xref">Figura&nbsp;<span>9.9</span></a> muestra los datos del iris (proyectados con Análisis de Componentes Principales <a href="05ReduccionDim.html#sec-pca" class="quarto-xref"><span>Sección 5.5</span></a>), las clases se encuentran en color azul, naranja y verde; en color rojo se muestran los vectores de soporte. La figura derecha muestra en color negro aquellos vectores de soporte que se encuentran en el lado incorrecto del hiperplano. Por otro lado se puede observar como los vectores de soporte separan las clases, del lado izquierdo se encuentran todos los elementos de la clase <span class="math inline">\(0\)</span>, después se observan las clases <span class="math inline">\(1\)</span> y del lado derecho las clases <span class="math inline">\(2\)</span>. Los vectores de soporte están en la frontera de las clases y los errores se encuentran entre las clases <span class="math inline">\(1\)</span> y <span class="math inline">\(2\)</span> que corresponden a las que no son linealmente separables.</p>
<div id="cell-fig-lineal-kernel" class="cell" data-execution_count="11">
<details class="code-fold">
<summary>Código</summary>
<div class="sourceCode cell-code" id="cb10"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a>X, y <span class="op">=</span> load_iris(return_X_y<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a>linear <span class="op">=</span> SVC(kernel<span class="op">=</span><span class="st">'poly'</span>, degree<span class="op">=</span><span class="dv">2</span>, C<span class="op">=</span><span class="dv">10</span>).fit(X, y)</span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a>hy <span class="op">=</span> linear.predict(X)</span>
<span id="cb10-4"><a href="#cb10-4" aria-hidden="true" tabindex="-1"></a>D <span class="op">=</span> PCA(n_components<span class="op">=</span><span class="dv">2</span>).fit_transform(X)</span>
<span id="cb10-5"><a href="#cb10-5" aria-hidden="true" tabindex="-1"></a>mask <span class="op">=</span> np.zeros(D.shape[<span class="dv">0</span>])</span>
<span id="cb10-6"><a href="#cb10-6" aria-hidden="true" tabindex="-1"></a>mask[linear.support_] <span class="op">=</span> <span class="va">True</span></span>
<span id="cb10-7"><a href="#cb10-7" aria-hidden="true" tabindex="-1"></a>s <span class="op">=</span> <span class="st">'S'</span></span>
<span id="cb10-8"><a href="#cb10-8" aria-hidden="true" tabindex="-1"></a>df <span class="op">=</span> pd.DataFrame([<span class="bu">dict</span>(x1<span class="op">=</span>x1, x2<span class="op">=</span>x2, tipo<span class="op">=</span><span class="ss">f'</span><span class="sc">{</span>l <span class="cf">if</span> <span class="kw">not</span> c <span class="cf">else</span> s<span class="sc">}</span><span class="ss">'</span>, error<span class="op">=</span>err)</span>
<span id="cb10-9"><a href="#cb10-9" aria-hidden="true" tabindex="-1"></a>                   <span class="cf">for</span> (x1, x2), c, l, err <span class="kw">in</span> <span class="bu">zip</span>(D, mask, y, y <span class="op">!=</span> hy)])</span>
<span id="cb10-10"><a href="#cb10-10" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> k, legend <span class="kw">in</span> <span class="bu">enumerate</span>([<span class="va">False</span>, <span class="va">True</span>]):     </span>
<span id="cb10-11"><a href="#cb10-11" aria-hidden="true" tabindex="-1"></a>     <span class="cf">if</span> legend:</span>
<span id="cb10-12"><a href="#cb10-12" aria-hidden="true" tabindex="-1"></a>          df.loc[df.error, <span class="st">'tipo'</span>] <span class="op">=</span> <span class="st">'X'</span></span>
<span id="cb10-13"><a href="#cb10-13" aria-hidden="true" tabindex="-1"></a>     df.sort_values(by<span class="op">=</span><span class="st">'tipo'</span>, inplace<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb10-14"><a href="#cb10-14" aria-hidden="true" tabindex="-1"></a>     ax <span class="op">=</span> plt.subplot(<span class="dv">1</span>, <span class="dv">2</span>, k <span class="op">+</span> <span class="dv">1</span>)          </span>
<span id="cb10-15"><a href="#cb10-15" aria-hidden="true" tabindex="-1"></a>     sns.scatterplot(data<span class="op">=</span>df, x<span class="op">=</span><span class="st">'x1'</span>, y<span class="op">=</span><span class="st">'x2'</span>,</span>
<span id="cb10-16"><a href="#cb10-16" aria-hidden="true" tabindex="-1"></a>                     palette<span class="op">=</span>sns.color_palette()[:<span class="dv">4</span>] <span class="op">+</span> [<span class="st">'k'</span>],</span>
<span id="cb10-17"><a href="#cb10-17" aria-hidden="true" tabindex="-1"></a>                     hue<span class="op">=</span><span class="st">'tipo'</span>, legend<span class="op">=</span>legend, ax<span class="op">=</span>ax)</span>
<span id="cb10-18"><a href="#cb10-18" aria-hidden="true" tabindex="-1"></a>     <span class="cf">if</span> legend:</span>
<span id="cb10-19"><a href="#cb10-19" aria-hidden="true" tabindex="-1"></a>          sns.move_legend(ax, <span class="st">"upper left"</span>, bbox_to_anchor<span class="op">=</span>(<span class="dv">1</span>, <span class="fl">0.65</span>))</span>
<span id="cb10-20"><a href="#cb10-20" aria-hidden="true" tabindex="-1"></a>_ <span class="op">=</span> plt.tight_layout()</span></code><button title="Copiar al portapapeles" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div id="fig-lineal-kernel" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-lineal-kernel-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="09Lineal_files/figure-html/fig-lineal-kernel-output-1.png" width="662" height="471" class="figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-lineal-kernel-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figura&nbsp;9.9: Visualización de los vectores de soporte usando PCA.
</figcaption>
</figure>
</div>
</div>
</div>
</section>
</section>
<section id="sec-regresion-logistica" class="level2" data-number="9.5">
<h2 data-number="9.5" class="anchored" data-anchor-id="sec-regresion-logistica"><span class="header-section-number">9.5</span> Regresión Logística</h2>
<p>En clasificación binaria (<a href="#sec-binaria" class="quarto-xref"><span>Sección 9.3.1</span></a>) se describió que la función discriminante se puede definir como la resta, i.e., <span class="math inline">\(g_1(\mathbf x) - g_2(\mathbf x);\)</span> equivalentemente se pudo haber seleccionado la división (<span class="math inline">\(\frac{g_1(\mathbf x)}{g_2(\mathbf x)}\)</span>) para generar la función discriminante o el logaritmo de la división, i.e., <span class="math inline">\(\log \frac{g_1(\mathbf x)}{g_2(\mathbf x)}.\)</span> Esta última ecuación en el caso de <span class="math inline">\(g_i(\mathbf x)=\mathbb P(\mathcal Y=i \mid \mathcal X=\mathbf x)\)</span> corresponde a la función <span class="math inline">\(\textsf{logit}\)</span>, tal y como se muestra a continuación.</p>
<p><span class="math display">\[
\begin{split}
\log \frac{\mathbb P(\mathcal Y=1 \mid \mathcal X=\mathbf x)}{\mathbb P(\mathcal Y=2 \mid \mathcal X=\mathbf x)} &amp;= \frac{\mathbb P(\mathcal Y=1 \mid \mathcal X=\mathbf x)}{1 - \mathbb P(\mathcal Y=1 \mid \mathcal X=\mathbf x)}\\
&amp;= \textsf{logit}(\mathbb P(\mathcal Y=1 \mid \mathcal X=\mathbf x)),
\end{split}
\]</span></p>
<p>donde la inversa del <span class="math inline">\(\textsf{logit}\)</span> es la función sigmoide, <span class="math inline">\(\textsf{sigmoid}(x) = \frac{1}{1 + \exp(-x)}\)</span>, es decir <span class="math inline">\(\textsf{sigmoid}(\textsf{logit}(y)) = y\)</span>.</p>
<p>Trabajando un poco con el <span class="math inline">\(\textsf{logit}\)</span> se puede observar que para el caso de dos clases está función queda como</p>
<p><span class="math display">\[
\begin{split}
\textsf{logit}(\mathbb P(\mathcal Y=1 \mid \mathcal X=\mathbf x)) &amp;= \log\frac{\mathbb P(\mathcal Y=1 \mid \mathcal X=\mathbf x)}{\mathbb P(\mathcal Y=2 \mid \mathcal X=\mathbf x)}\\
&amp;= \log\frac{\mathbb P(\mathcal X=\mathbf x \mid \mathcal Y=1)\mathbb P(\mathcal Y=1)}{\mathbb P(\mathcal X=\mathbf x \mid \mathcal Y=2)\mathbb P(\mathcal Y=2)}\\
&amp;= \log\frac{\mathbb P(\mathcal X=\mathbf x \mid \mathcal Y=1)}{\mathbb P(\mathcal X=\mathbf x \mid \mathcal Y=2)} + \log\frac{\mathbb P(\mathcal Y=1)}{\mathbb P(\mathcal Y=2)}
\end{split},
\]</span></p>
<p>asumiendo que la matriz de covarianza (<span class="math inline">\(\Sigma\)</span>) es compartida entre las dos clases la ecuación anterior quedaría como:</p>
<p><span class="math display">\[
\begin{split}
\textsf{logit}(\mathbb P(\mathcal Y=1 \mid \mathcal X=\mathbf x)) &amp;= \log \frac{(2\pi)^{-\frac{d}{2}} \mid\Sigma \mid^{-\frac{1}{2}}\exp{(-\frac{1}{2}(\mathbf x - \mathbf \mu_1)^\intercal \Sigma^{-1}(\mathbf x - \mathbf \mu_1))}}{(2\pi)^{-\frac{d}{2}} \mid\Sigma \mid^{-\frac{1}{2}}\exp{(-\frac{1}{2}(\mathbf x - \mathbf \mu_2)^\intercal \Sigma^{-1}(\mathbf x - \mathbf \mu_2))}}\\
&amp;+ \log\frac{\mathbb P(\mathcal Y=1)}{\mathbb P(\mathcal Y=2)}\\
&amp;= \mathbf w \cdot \mathbf x + w_0
\end{split}
\]</span></p>
<p>donde <span class="math inline">\(\mathbf w=\Sigma^{-1}(\mathbf \mu_1 - \mathbf \mu_2)\)</span> y <span class="math inline">\(w_0=-\frac{1}{2}(\mathbf \mu_1 + \mathbf \mu_2)^\intercal \Sigma^{-1}(\mathbf \mu_1 + \mathbf \mu_2)+ \log\frac{\mathbb P(\mathcal Y=1)}{\mathbb P(\mathcal Y=2)}.\)</span></p>
<p>En el caso de regresión logística, se asume que <span class="math inline">\(\textsf{logit}(\mathbb P(\mathcal Y=1 \mid \mathcal X=\mathbf x)) = \mathbf w \cdot \mathbf x + w_0\)</span> y se realiza ninguna asunción sobre la distribución que tienen los datos. Equivalentemente, se puede asumir que <span class="math inline">\(\log\frac{\mathbb P(\mathcal Y=1 \mid \mathcal X=\mathbf x)}{\mathbb P(\mathcal Y=2 \mid \mathcal X=\mathbf x)} = \mathbf w \cdot \mathbf x + w_0^0,\)</span> realizando algunas substituciones se puede ver que <span class="math inline">\(w_0 = w_0^0 + \log\frac{\mathbb P(\mathcal Y=1)}{\mathbb P(\mathcal Y=2)}.\)</span></p>
<section id="optimización-1" class="level3" data-number="9.5.1">
<h3 data-number="9.5.1" class="anchored" data-anchor-id="optimización-1"><span class="header-section-number">9.5.1</span> Optimización</h3>
<p>Se puede asumir que <span class="math inline">\(\mathcal Y \mid \mathcal X\)</span> sigue una distribución Bernoulli en el caso de dos clases, entonces el logaritmo de la verosimilitud (<a href="03Parametricos.html#sec-verosimilitud" class="quarto-xref"><span>Sección 3.3.1</span></a>) quedaría como:</p>
<p><span class="math display">\[
\ell(\mathbf w, w_0 \mid \mathcal D) = \prod_{(\mathbf x, y) \in \mathcal D} (C(\mathbf x))^{y} (1 -  C(\mathbf x)))^{1-y},
\]</span></p>
<p>donde <span class="math inline">\(C(\mathbf x)\)</span> es la clase estimada por el clasificador.</p>
<p>Siempre que se tiene que obtener el máximo de una función esta se puede transformar a un problema de minimización, por ejemplo, para el caso anterior definiendo como <span class="math inline">\(E = -\log \ell\)</span>, utilizando esta transformación el problema sería minimizar la siguiente función:</p>
<p><span id="eq-lineal-entropia-cruzada"><span class="math display">\[
E(\mathbf w, w_0 \mid \mathcal D) = - \sum_{(\mathbf x, y) \in \mathcal D} y \log C(x) + (1-y) \log (1 -  C(x)).
\tag{9.2}\]</span></span></p>
<p>Es importante notar que la ecuación anterior corresponde a Entropía cruzada (<a href="04Rendimiento.html#sec-entropia-cruzada" class="quarto-xref"><span>Sección 4.2.7</span></a>), donde <span class="math inline">\(y=\mathbb P(\mathcal Y=y \mid \mathcal X=\mathbf x))\)</span> y <span class="math inline">\(C(\mathbf x)=\mathbb{\hat P}(\mathcal Y=y \mid \mathcal X=\mathbf x)\)</span> y los términos <span class="math inline">\(1-y\)</span> y <span class="math inline">\(1-C(\mathbf x)\)</span> corresponde a la otra clase.</p>
<p>Otra característica de <span class="math inline">\(E(\mathbf w, w_0 \mid \mathcal D)\)</span> es que no tiene una solución cerrada y por lo tanto es necesario utilizar un método de optimización (<a href="10Optimizacion.html" class="quarto-xref"><span>Capítulo 10</span></a>) para encontrar los parámetros <span class="math inline">\(\mathbf w\)</span> y <span class="math inline">\(w_0\)</span>.</p>
</section>
</section>
<section id="comparación" class="level2" data-number="9.6">
<h2 data-number="9.6" class="anchored" data-anchor-id="comparación"><span class="header-section-number">9.6</span> Comparación</h2>
<p>Es momento de comparar el comportamiento de los dos métodos de discriminantes lineales visto en la unidad, estos son, Máquinas de Soporte Vectorial (MSV) y Regresión Logística (RL). La <a href="#fig-lineal-comparacion" class="quarto-xref">Figura&nbsp;<span>9.10</span></a> muestra el hiperplano generado por MSV y RL, además se puede observar los valores de los pesos <span class="math inline">\(\mathbf w\)</span> para cada uno de los algoritmos.</p>
<div id="cell-fig-lineal-comparacion" class="cell" data-execution_count="12">
<details class="code-fold">
<summary>Código</summary>
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a>svm <span class="op">=</span> LinearSVC(dual<span class="op">=</span><span class="va">False</span>).fit(T, y_t)</span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a>lr <span class="op">=</span> LogisticRegression().fit(T, y_t)</span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-4"><a href="#cb11-4" aria-hidden="true" tabindex="-1"></a>g <span class="op">=</span> []</span>
<span id="cb11-5"><a href="#cb11-5" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> model, tipo <span class="kw">in</span> <span class="bu">zip</span>([svm, lr], [<span class="st">'MSV'</span>, <span class="st">'RL'</span>]):</span>
<span id="cb11-6"><a href="#cb11-6" aria-hidden="true" tabindex="-1"></a>     w_1, w_2 <span class="op">=</span> model.coef_[<span class="dv">0</span>]</span>
<span id="cb11-7"><a href="#cb11-7" aria-hidden="true" tabindex="-1"></a>     w_0 <span class="op">=</span> model.intercept_[<span class="dv">0</span>]</span>
<span id="cb11-8"><a href="#cb11-8" aria-hidden="true" tabindex="-1"></a>     g <span class="op">+=</span> [<span class="bu">dict</span>(x1<span class="op">=</span>x, x2<span class="op">=</span>y, tipo<span class="op">=</span>tipo)</span>
<span id="cb11-9"><a href="#cb11-9" aria-hidden="true" tabindex="-1"></a>           <span class="cf">for</span> x, y <span class="kw">in</span> <span class="bu">zip</span>(T[:, <span class="dv">0</span>], (<span class="op">-</span>w_0 <span class="op">-</span> w_1 <span class="op">*</span> T[:, <span class="dv">0</span>]) <span class="op">/</span> w_2)]</span>
<span id="cb11-10"><a href="#cb11-10" aria-hidden="true" tabindex="-1"></a>     g.append(<span class="bu">dict</span>(x1<span class="op">=</span>w_1, x2<span class="op">=</span>w_2, clase<span class="op">=</span>tipo))</span>
<span id="cb11-11"><a href="#cb11-11" aria-hidden="true" tabindex="-1"></a>df <span class="op">=</span> pd.DataFrame(g <span class="op">+</span> <span class="op">\</span></span>
<span id="cb11-12"><a href="#cb11-12" aria-hidden="true" tabindex="-1"></a>                  [<span class="bu">dict</span>(x1<span class="op">=</span>x, x2<span class="op">=</span>y, clase<span class="op">=</span><span class="st">'P'</span>) <span class="cf">for</span> x, y <span class="kw">in</span> X_1] <span class="op">+</span> <span class="op">\</span></span>
<span id="cb11-13"><a href="#cb11-13" aria-hidden="true" tabindex="-1"></a>                  [<span class="bu">dict</span>(x1<span class="op">=</span>x, x2<span class="op">=</span>y, clase<span class="op">=</span><span class="st">'N'</span>) <span class="cf">for</span> x, y <span class="kw">in</span> X_2]</span>
<span id="cb11-14"><a href="#cb11-14" aria-hidden="true" tabindex="-1"></a>                 )</span>
<span id="cb11-15"><a href="#cb11-15" aria-hidden="true" tabindex="-1"></a>ax <span class="op">=</span> sns.scatterplot(data<span class="op">=</span>df, x<span class="op">=</span><span class="st">'x1'</span>, y<span class="op">=</span><span class="st">'x2'</span>, hue<span class="op">=</span><span class="st">'clase'</span>, legend<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb11-16"><a href="#cb11-16" aria-hidden="true" tabindex="-1"></a>sns.lineplot(data<span class="op">=</span>df, x<span class="op">=</span><span class="st">'x1'</span>, y<span class="op">=</span><span class="st">'x2'</span>, </span>
<span id="cb11-17"><a href="#cb11-17" aria-hidden="true" tabindex="-1"></a>             ax<span class="op">=</span>ax, hue<span class="op">=</span><span class="st">'tipo'</span>, </span>
<span id="cb11-18"><a href="#cb11-18" aria-hidden="true" tabindex="-1"></a>             palette<span class="op">=</span>sns.color_palette()[:<span class="dv">2</span>], </span>
<span id="cb11-19"><a href="#cb11-19" aria-hidden="true" tabindex="-1"></a>             legend<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb11-20"><a href="#cb11-20" aria-hidden="true" tabindex="-1"></a>_ <span class="op">=</span> ax.axis(<span class="st">'equal'</span>)</span></code><button title="Copiar al portapapeles" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div id="fig-lineal-comparacion" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-lineal-comparacion-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="09Lineal_files/figure-html/fig-lineal-comparacion-output-1.png" width="582" height="427" class="figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-lineal-comparacion-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figura&nbsp;9.10: Comparación de dos métodos lineales
</figcaption>
</figure>
</div>
</div>
</div>
<p>Complementando la comparación anterior con los datos del iris que se pueden obtener con las siguientes dos instrucciones.</p>
<div id="192110b6" class="cell" data-execution_count="13">
<div class="sourceCode cell-code" id="cb12"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a>D, y <span class="op">=</span> load_iris(return_X_y<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a>T, G, y_t, y_g <span class="op">=</span> train_test_split(D, y,</span>
<span id="cb12-3"><a href="#cb12-3" aria-hidden="true" tabindex="-1"></a>                                  test_size<span class="op">=</span><span class="fl">0.4</span>,</span>
<span id="cb12-4"><a href="#cb12-4" aria-hidden="true" tabindex="-1"></a>                                  random_state<span class="op">=</span><span class="dv">3</span>)</span></code><button title="Copiar al portapapeles" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Los clasificadores a comparar son una máquina de soporte vectorial lineal, una máquina de soporte vectorial usando un kernel polinomial de grado <span class="math inline">\(1\)</span> y una regresión logística, tal y como se muestra en el siguiente código.</p>
<div id="0d8ceb51" class="cell" data-execution_count="14">
<div class="sourceCode cell-code" id="cb13"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a>svm <span class="op">=</span> LinearSVC(dual<span class="op">=</span><span class="va">False</span>).fit(T, y_t)</span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a>svm_k <span class="op">=</span> SVC(kernel<span class="op">=</span><span class="st">'poly'</span>, degree<span class="op">=</span><span class="dv">1</span>).fit(T, y_t)</span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a>lr <span class="op">=</span> LogisticRegression().fit(T, y_t)</span></code><button title="Copiar al portapapeles" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>La <a href="#tbl-lineal-rendimiento" class="quarto-xref">Tabla&nbsp;<span>9.1</span></a> muestra el rendimiento (en medidas macro) de estos algoritmos en el conjunto de prueba, se puede observar que estos algoritmos tienen rendimientos diferentes para esta selección del conjunto de entrenamiento y prueba. También en esta ocasión la regresión lineal es la que presenta el mejor rendimiento. Aunque es importante aclarar que este rendimiento es resultado del proceso aleatorio de selección del conjunto de entrenamiento y prueba.</p>
<div class="cell" data-execution_count="15">
<div id="tbl-lineal-rendimiento" class="cell quarto-float quarto-figure quarto-figure-center anchored" data-execution_count="15">
<figure class="quarto-float quarto-float-tbl figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-lineal-rendimiento-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Tabla&nbsp;9.1: Rendimiento de clasificadores lineales
</figcaption>
<div aria-describedby="tbl-lineal-rendimiento-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="cell-output cell-output-display" data-execution_count="15">
<table class="do-not-create-environment cell caption-top table table-sm table-striped small">
<thead>
<tr class="header">
<th>Clasificador</th>
<th>Precisión</th>
<th>&nbsp;Recall</th>
<th><span class="math inline">\(F_1\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>MSV - Lineal</td>
<td><span class="math inline">\(0.9524\)</span></td>
<td><span class="math inline">\(0.9500\)</span></td>
<td><span class="math inline">\(0.9473\)</span></td>
</tr>
<tr class="even">
<td>MSV - Kernel</td>
<td><span class="math inline">\(0.9474\)</span></td>
<td><span class="math inline">\(0.9481\)</span></td>
<td><span class="math inline">\(0.9473\)</span></td>
</tr>
<tr class="odd">
<td>RL</td>
<td><span class="math inline">\(0.9667\)</span></td>
<td><span class="math inline">\(0.9667\)</span></td>
<td><span class="math inline">\(0.9649\)</span></td>
</tr>
</tbody>
</table>
</div>
</div>
</figure>
</div>
</div>


<!-- -->

</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copiado");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copiado");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    // For code content inside modals, clipBoardJS needs to be initialized with a container option
    // TODO: Check when it could be a function (https://github.com/zenorocha/clipboard.js/issues/860)
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
  const viewSource = window.document.getElementById('quarto-view-source') ||
                     window.document.getElementById('quarto-code-tools-source');
  if (viewSource) {
    const sourceUrl = viewSource.getAttribute("data-quarto-source-url");
    viewSource.addEventListener("click", function(e) {
      if (sourceUrl) {
        // rstudio viewer pane
        if (/\bcapabilities=\b/.test(window.location)) {
          window.open(sourceUrl);
        } else {
          window.location.href = sourceUrl;
        }
      } else {
        const modal = new bootstrap.Modal(document.getElementById('quarto-embedded-source-code-modal'));
        modal.show();
      }
      return false;
    });
  }
  function toggleCodeHandler(show) {
    return function(e) {
      const detailsSrc = window.document.querySelectorAll(".cell > details > .sourceCode");
      for (let i=0; i<detailsSrc.length; i++) {
        const details = detailsSrc[i].parentElement;
        if (show) {
          details.open = true;
        } else {
          details.removeAttribute("open");
        }
      }
      const cellCodeDivs = window.document.querySelectorAll(".cell > .sourceCode");
      const fromCls = show ? "hidden" : "unhidden";
      const toCls = show ? "unhidden" : "hidden";
      for (let i=0; i<cellCodeDivs.length; i++) {
        const codeDiv = cellCodeDivs[i];
        if (codeDiv.classList.contains(fromCls)) {
          codeDiv.classList.remove(fromCls);
          codeDiv.classList.add(toCls);
        } 
      }
      return false;
    }
  }
  const hideAllCode = window.document.getElementById("quarto-hide-all-code");
  if (hideAllCode) {
    hideAllCode.addEventListener("click", toggleCodeHandler(false));
  }
  const showAllCode = window.document.getElementById("quarto-show-all-code");
  if (showAllCode) {
    showAllCode.addEventListener("click", toggleCodeHandler(true));
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp("https:\/\/ingeotec\.github\.io\/AprendizajeComputacional");
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="../capitulos/08Arboles.html" class="pagination-link" aria-label="Árboles de Decisión">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Árboles de Decisión</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="../capitulos/10Optimizacion.html" class="pagination-link" aria-label="Optimización">
        <span class="nav-page-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Optimización</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav><div class="modal fade" id="quarto-embedded-source-code-modal" tabindex="-1" aria-labelledby="quarto-embedded-source-code-modal-label" aria-hidden="true"><div class="modal-dialog modal-dialog-scrollable"><div class="modal-content"><div class="modal-header"><h5 class="modal-title" id="quarto-embedded-source-code-modal-label">Ejecutar el código</h5><button class="btn-close" data-bs-dismiss="modal"></button></div><div class="modal-body"><div class="">
<div class="sourceCode" id="cb14" data-shortcodes="false"><pre class="sourceCode markdown code-with-copy"><code class="sourceCode markdown"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a><span class="fu"># Discriminantes Lineales {#sec-discriminantes-lineales}</span></span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-3"><a href="#cb14-3" aria-hidden="true" tabindex="-1"></a>El **objetivo** de la unidad es conocer y aplicar diferentes métodos lineales de discriminación para atacar problemas de clasificación.</span>
<span id="cb14-4"><a href="#cb14-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-5"><a href="#cb14-5" aria-hidden="true" tabindex="-1"></a><span class="fu">## Paquetes usados</span></span>
<span id="cb14-6"><a href="#cb14-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-9"><a href="#cb14-9" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb14-10"><a href="#cb14-10" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: true</span></span>
<span id="cb14-11"><a href="#cb14-11" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.svm <span class="im">import</span> LinearSVC, SVC</span>
<span id="cb14-12"><a href="#cb14-12" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.linear_model <span class="im">import</span> LogisticRegression</span>
<span id="cb14-13"><a href="#cb14-13" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.decomposition <span class="im">import</span> PCA</span>
<span id="cb14-14"><a href="#cb14-14" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> train_test_split</span>
<span id="cb14-15"><a href="#cb14-15" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> f1_score, recall_score, precision_score</span>
<span id="cb14-16"><a href="#cb14-16" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> scipy.stats <span class="im">import</span> multivariate_normal</span>
<span id="cb14-17"><a href="#cb14-17" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.datasets <span class="im">import</span> load_iris</span>
<span id="cb14-18"><a href="#cb14-18" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> matplotlib <span class="im">import</span> pylab <span class="im">as</span> plt</span>
<span id="cb14-19"><a href="#cb14-19" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb14-20"><a href="#cb14-20" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb14-21"><a href="#cb14-21" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> seaborn <span class="im">as</span> sns</span>
<span id="cb14-22"><a href="#cb14-22" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb14-23"><a href="#cb14-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-26"><a href="#cb14-26" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb14-27"><a href="#cb14-27" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: false</span></span>
<span id="cb14-28"><a href="#cb14-28" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> IPython.display <span class="im">import</span> Markdown</span>
<span id="cb14-29"><a href="#cb14-29" aria-hidden="true" tabindex="-1"></a>sns.set_style(<span class="st">'whitegrid'</span>)</span>
<span id="cb14-30"><a href="#cb14-30" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb14-31"><a href="#cb14-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-32"><a href="#cb14-32" aria-hidden="true" tabindex="-1"></a>::: {.content-visible when-format="html"}</span>
<span id="cb14-33"><a href="#cb14-33" aria-hidden="true" tabindex="-1"></a>---</span>
<span id="cb14-34"><a href="#cb14-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-35"><a href="#cb14-35" aria-hidden="true" tabindex="-1"></a>{{&lt; video https://www.youtube.com/embed/BgLm0tVxW8A width="560" height="315" &gt;}}</span>
<span id="cb14-36"><a href="#cb14-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-37"><a href="#cb14-37" aria-hidden="true" tabindex="-1"></a>---</span>
<span id="cb14-38"><a href="#cb14-38" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb14-39"><a href="#cb14-39" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-40"><a href="#cb14-40" aria-hidden="true" tabindex="-1"></a><span class="fu">## Introducción</span></span>
<span id="cb14-41"><a href="#cb14-41" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-42"><a href="#cb14-42" aria-hidden="true" tabindex="-1"></a>En unidades anteriores se han visto diferentes técnicas para discriminar entre clases; en particular se ha descrito el uso de la probabilidad $\mathbb P(\mathcal Y \mid \mathcal X)$ para encontrar la clase más probable. Los parámetros de $\mathbb P(\mathcal Y \mid \mathcal X)$ se han estimado utilizando métodos <span class="co">[</span><span class="ot">paramétricos</span><span class="co">](@sec-metodos-parametricos)</span> y <span class="co">[</span><span class="ot">no paramétricos</span><span class="co">](@sec-metodos-no-parametricos)</span>. En está unidad se describe el uso de funciones discriminantes para la clasificación y su similitud con el uso de $\mathbb P(\mathcal Y \mid \mathcal X).$</span>
<span id="cb14-43"><a href="#cb14-43" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-44"><a href="#cb14-44" aria-hidden="true" tabindex="-1"></a><span class="fu">## Función Discriminante {#sec-discriminante}</span></span>
<span id="cb14-45"><a href="#cb14-45" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-46"><a href="#cb14-46" aria-hidden="true" tabindex="-1"></a>En la unidad de Teoría de Decisión Bayesiana (@sec-teoria-decision-bayesianas) se describió el uso de $\mathbb P(\mathcal Y \mid \mathcal X)$ para clasificar, se mencionó que la clase a la que pertenece $\mathcal X=x$ es la de mayor probabilidad, es decir,  </span>
<span id="cb14-47"><a href="#cb14-47" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-48"><a href="#cb14-48" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb14-49"><a href="#cb14-49" aria-hidden="true" tabindex="-1"></a>C(x) = \textsf{argmax}_{k=1}^K \mathbb P(\mathcal Y=k \mid \mathcal X=x),</span>
<span id="cb14-50"><a href="#cb14-50" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb14-51"><a href="#cb14-51" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-52"><a href="#cb14-52" aria-hidden="true" tabindex="-1"></a>donde $K$ es el número de clases y $\mathcal Y=k$ representa la $k$-ésima clase. Considerando que la <span class="co">[</span><span class="ot">evidencia</span><span class="co">](@eq-evidencia)</span> es un factor que normaliza, entonces, $C(x)$ se puede definir de la siguiente manera. </span>
<span id="cb14-53"><a href="#cb14-53" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-54"><a href="#cb14-54" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb14-55"><a href="#cb14-55" aria-hidden="true" tabindex="-1"></a>C(x) = \textsf{argmax}_{k=1}^K \mathbb P(\mathcal X=x \mid \mathcal Y=k)\mathbb P(\mathcal Y=k).</span>
<span id="cb14-56"><a href="#cb14-56" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb14-57"><a href="#cb14-57" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-58"><a href="#cb14-58" aria-hidden="true" tabindex="-1"></a>Agrupando la probabilidad a priori y verosimilitud en una función $g_k,$ es decir, $g_k(x) = P(\mathcal X=x \mid \mathcal Y=k)\mathbb P(\mathcal Y=k),$  hace que $C(x)$ se sea:</span>
<span id="cb14-59"><a href="#cb14-59" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-60"><a href="#cb14-60" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb14-61"><a href="#cb14-61" aria-hidden="true" tabindex="-1"></a>C(x) = \textsf{argmax}_{k=1}^K g_k(x).</span>
<span id="cb14-62"><a href="#cb14-62" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb14-63"><a href="#cb14-63" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-64"><a href="#cb14-64" aria-hidden="true" tabindex="-1"></a>Observando $C(x)$ y olvidando los pasos utilizados para derivarla, uno se puede imaginar que lo único necesario para generar un clasificador de $K$ clases es definir un conjunto de functions $g_k$ que separen las clases correctamente. En esta unidad se presentan diferentes maneras para definir $g_k$ con la característica de que todas ellas son lineales, e.g., $g_k(\mathbf x) = \mathbf w_k \cdot \mathbf x + w_{k_0}.$</span>
<span id="cb14-65"><a href="#cb14-65" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-66"><a href="#cb14-66" aria-hidden="true" tabindex="-1"></a><span class="fu">### Clasificación Binaria {#sec-binaria}</span></span>
<span id="cb14-67"><a href="#cb14-67" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-68"><a href="#cb14-68" aria-hidden="true" tabindex="-1"></a>La descripción de discriminantes lineales empieza con el caso particular de dos clases, i.e., $K=2$. En este caso $C(\mathbf x)$ es encontrar el máximo de las dos funciones $g_1$ y $g_2$. Una manear equivalente sería definir a $C(\mathbf x)$ como </span>
<span id="cb14-69"><a href="#cb14-69" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-70"><a href="#cb14-70" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb14-71"><a href="#cb14-71" aria-hidden="true" tabindex="-1"></a>C(\mathbf x) = \textsf{sign}(g_1(\mathbf x) - g_2(\mathbf x)),</span>
<span id="cb14-72"><a href="#cb14-72" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb14-73"><a href="#cb14-73" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-74"><a href="#cb14-74" aria-hidden="true" tabindex="-1"></a>donde $\textsf{sign}$ es la función que regresa el signo, entonces solo queda asociar el signo positivo a la clase 1 y el negativo a la clase 2. Utilizando esta definición se observa lo siguiente</span>
<span id="cb14-75"><a href="#cb14-75" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-76"><a href="#cb14-76" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb14-77"><a href="#cb14-77" aria-hidden="true" tabindex="-1"></a>\begin{split}</span>
<span id="cb14-78"><a href="#cb14-78" aria-hidden="true" tabindex="-1"></a>    g_1(\mathbf x) - g_2(\mathbf x) &amp;= (\mathbf w_1 \cdot \mathbf x + w_{1_0}) - (\mathbf w_2 \cdot \mathbf x + w_{2_0}) <span class="sc">\\</span></span>
<span id="cb14-79"><a href="#cb14-79" aria-hidden="true" tabindex="-1"></a>         &amp;= (\mathbf w_1 + \mathbf w_2) \cdot \mathbf x + (w_{1_0} - w_{2_0}) <span class="sc">\\</span></span>
<span id="cb14-80"><a href="#cb14-80" aria-hidden="true" tabindex="-1"></a>         &amp;= \mathbf w \cdot \mathbf x + w_0</span>
<span id="cb14-81"><a href="#cb14-81" aria-hidden="true" tabindex="-1"></a>\end{split},</span>
<span id="cb14-82"><a href="#cb14-82" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb14-83"><a href="#cb14-83" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-84"><a href="#cb14-84" aria-hidden="true" tabindex="-1"></a>donde se concluye que para el caso binario es necesario definir solamente una función discriminante y que los parámetros de esta función son $\mathbf w$ y $\mathbf w_0.$ Otra característica que se ilustra es que el parámetro $\mathbf w_0$ está actuando como un umbral, es decir, $\mathbf x$ corresponde a la clase positiva si $\mathbf w \cdot \mathbf x &gt; -w_0.$</span>
<span id="cb14-85"><a href="#cb14-85" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-86"><a href="#cb14-86" aria-hidden="true" tabindex="-1"></a>En la @fig-lineal-discriminante se observa el plano (linea) que divide las dos clases, este plano representa los puntos que satisfacen $g(\mathbf x)=0$. </span>
<span id="cb14-87"><a href="#cb14-87" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-90"><a href="#cb14-90" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb14-91"><a href="#cb14-91" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-fold: true</span></span>
<span id="cb14-92"><a href="#cb14-92" aria-hidden="true" tabindex="-1"></a><span class="co">#| warning: false</span></span>
<span id="cb14-93"><a href="#cb14-93" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: Función Discriminante</span></span>
<span id="cb14-94"><a href="#cb14-94" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-lineal-discriminante</span></span>
<span id="cb14-95"><a href="#cb14-95" aria-hidden="true" tabindex="-1"></a>X_1 <span class="op">=</span> multivariate_normal(mean<span class="op">=</span>[<span class="dv">15</span>, <span class="dv">20</span>],</span>
<span id="cb14-96"><a href="#cb14-96" aria-hidden="true" tabindex="-1"></a>                          seed<span class="op">=</span><span class="dv">0</span>,</span>
<span id="cb14-97"><a href="#cb14-97" aria-hidden="true" tabindex="-1"></a>                          cov<span class="op">=</span>[[<span class="dv">3</span>, <span class="op">-</span><span class="dv">3</span>], [<span class="op">-</span><span class="dv">3</span>, <span class="dv">8</span>]]).rvs(<span class="dv">1000</span>)</span>
<span id="cb14-98"><a href="#cb14-98" aria-hidden="true" tabindex="-1"></a>X_2 <span class="op">=</span> multivariate_normal(mean<span class="op">=</span>[<span class="dv">5</span>, <span class="dv">5</span>],</span>
<span id="cb14-99"><a href="#cb14-99" aria-hidden="true" tabindex="-1"></a>                          seed<span class="op">=</span><span class="dv">0</span>,</span>
<span id="cb14-100"><a href="#cb14-100" aria-hidden="true" tabindex="-1"></a>                          cov<span class="op">=</span>[[<span class="dv">4</span>, <span class="dv">0</span>], [<span class="dv">0</span>, <span class="dv">2</span>]]).rvs(<span class="dv">1000</span>)</span>
<span id="cb14-101"><a href="#cb14-101" aria-hidden="true" tabindex="-1"></a>T <span class="op">=</span> np.concatenate((X_1, X_2))</span>
<span id="cb14-102"><a href="#cb14-102" aria-hidden="true" tabindex="-1"></a>y_t <span class="op">=</span> np.array([<span class="st">'P'</span>] <span class="op">*</span> X_1.shape[<span class="dv">0</span>] <span class="op">+</span> [<span class="st">'N'</span>] <span class="op">*</span> X_2.shape[<span class="dv">0</span>])</span>
<span id="cb14-103"><a href="#cb14-103" aria-hidden="true" tabindex="-1"></a>linear <span class="op">=</span> LinearSVC(dual<span class="op">=</span><span class="va">False</span>).fit(T, y_t)</span>
<span id="cb14-104"><a href="#cb14-104" aria-hidden="true" tabindex="-1"></a>w_1, w_2 <span class="op">=</span> linear.coef_[<span class="dv">0</span>]</span>
<span id="cb14-105"><a href="#cb14-105" aria-hidden="true" tabindex="-1"></a>w_0 <span class="op">=</span> linear.intercept_[<span class="dv">0</span>]</span>
<span id="cb14-106"><a href="#cb14-106" aria-hidden="true" tabindex="-1"></a>g_0 <span class="op">=</span> [<span class="bu">dict</span>(x1<span class="op">=</span>x, x2<span class="op">=</span>y, tipo<span class="op">=</span><span class="st">'g(x)=0'</span>)</span>
<span id="cb14-107"><a href="#cb14-107" aria-hidden="true" tabindex="-1"></a>       <span class="cf">for</span> x, y <span class="kw">in</span> <span class="bu">zip</span>(T[:, <span class="dv">0</span>], (<span class="op">-</span>w_0 <span class="op">-</span> w_1 <span class="op">*</span> T[:, <span class="dv">0</span>]) <span class="op">/</span> w_2)]</span>
<span id="cb14-108"><a href="#cb14-108" aria-hidden="true" tabindex="-1"></a>df <span class="op">=</span> pd.DataFrame(g_0 <span class="op">+</span> <span class="op">\</span></span>
<span id="cb14-109"><a href="#cb14-109" aria-hidden="true" tabindex="-1"></a>                  [<span class="bu">dict</span>(x1<span class="op">=</span>x, x2<span class="op">=</span>y, clase<span class="op">=</span><span class="st">'P'</span>) <span class="cf">for</span> x, y <span class="kw">in</span> X_1] <span class="op">+</span> <span class="op">\</span></span>
<span id="cb14-110"><a href="#cb14-110" aria-hidden="true" tabindex="-1"></a>                  [<span class="bu">dict</span>(x1<span class="op">=</span>x, x2<span class="op">=</span>y, clase<span class="op">=</span><span class="st">'N'</span>) <span class="cf">for</span> x, y <span class="kw">in</span> X_2]</span>
<span id="cb14-111"><a href="#cb14-111" aria-hidden="true" tabindex="-1"></a>                 )</span>
<span id="cb14-112"><a href="#cb14-112" aria-hidden="true" tabindex="-1"></a>ax <span class="op">=</span> sns.scatterplot(data<span class="op">=</span>df, x<span class="op">=</span><span class="st">'x1'</span>, y<span class="op">=</span><span class="st">'x2'</span>, hue<span class="op">=</span><span class="st">'clase'</span>, legend<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb14-113"><a href="#cb14-113" aria-hidden="true" tabindex="-1"></a>sns.lineplot(data<span class="op">=</span>df, x<span class="op">=</span><span class="st">'x1'</span>, y<span class="op">=</span><span class="st">'x2'</span>, ax<span class="op">=</span>ax, hue<span class="op">=</span><span class="st">'tipo'</span>, palette<span class="op">=</span>[<span class="st">'k'</span>], legend<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb14-114"><a href="#cb14-114" aria-hidden="true" tabindex="-1"></a>_ <span class="op">=</span> ax.axis(<span class="st">'equal'</span>)</span>
<span id="cb14-115"><a href="#cb14-115" aria-hidden="true" tabindex="-1"></a><span class="in">```</span>  </span>
<span id="cb14-116"><a href="#cb14-116" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-117"><a href="#cb14-117" aria-hidden="true" tabindex="-1"></a><span class="fu">### Geometría de la Función de Decisión {#sec-geometria-funcion-decision}</span></span>
<span id="cb14-118"><a href="#cb14-118" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-119"><a href="#cb14-119" aria-hidden="true" tabindex="-1"></a>La función discriminante $g(\mathbf x) = \mathbf w \cdot \mathbf x + w_0$ tiene una representación gráfica. Lo primero que se observa es que los parámetros $\mathbf w$ viven en al mismo espacio que los datos, tal y como se puede observar en la @fig-lineal-repr-df. </span>
<span id="cb14-120"><a href="#cb14-120" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-123"><a href="#cb14-123" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb14-124"><a href="#cb14-124" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-fold: true</span></span>
<span id="cb14-125"><a href="#cb14-125" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: Función discriminante</span></span>
<span id="cb14-126"><a href="#cb14-126" aria-hidden="true" tabindex="-1"></a><span class="co">#| warning: false</span></span>
<span id="cb14-127"><a href="#cb14-127" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-lineal-repr-df</span></span>
<span id="cb14-128"><a href="#cb14-128" aria-hidden="true" tabindex="-1"></a>_ <span class="op">=</span> pd.DataFrame([<span class="bu">dict</span>(x1<span class="op">=</span>w_1, x2<span class="op">=</span>w_2, clase<span class="op">=</span><span class="st">'w'</span>)])</span>
<span id="cb14-129"><a href="#cb14-129" aria-hidden="true" tabindex="-1"></a>df <span class="op">=</span> pd.concat((df, _), axis<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb14-130"><a href="#cb14-130" aria-hidden="true" tabindex="-1"></a>ax <span class="op">=</span> sns.scatterplot(data<span class="op">=</span>df, x<span class="op">=</span><span class="st">'x1'</span>, y<span class="op">=</span><span class="st">'x2'</span>, hue<span class="op">=</span><span class="st">'clase'</span>, legend<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb14-131"><a href="#cb14-131" aria-hidden="true" tabindex="-1"></a>sns.lineplot(data<span class="op">=</span>df, x<span class="op">=</span><span class="st">'x1'</span>, y<span class="op">=</span><span class="st">'x2'</span>, ax<span class="op">=</span>ax, hue<span class="op">=</span><span class="st">'tipo'</span>, palette<span class="op">=</span>[<span class="st">'k'</span>], legend<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb14-132"><a href="#cb14-132" aria-hidden="true" tabindex="-1"></a>_ <span class="op">=</span> ax.axis(<span class="st">'equal'</span>)</span>
<span id="cb14-133"><a href="#cb14-133" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb14-134"><a href="#cb14-134" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-135"><a href="#cb14-135" aria-hidden="true" tabindex="-1"></a>Siguiendo con la descripción, los parámetros $\mathbf w$ y la función $g(\mathbf x)$ son ortogonales, tal y como se muestra en la @fig-lineal-ort. Analiticamente la ortogonalidad se define de la siguiente manera. Sea $\mathbf x_a$ y $\mathbf x_b$ dos puntos en $g(\mathbf x)=0$, es decir, </span>
<span id="cb14-136"><a href="#cb14-136" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-137"><a href="#cb14-137" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb14-138"><a href="#cb14-138" aria-hidden="true" tabindex="-1"></a>\begin{split}</span>
<span id="cb14-139"><a href="#cb14-139" aria-hidden="true" tabindex="-1"></a>g(\mathbf x_a) &amp;= g(\mathbf x_b) <span class="sc">\\</span></span>
<span id="cb14-140"><a href="#cb14-140" aria-hidden="true" tabindex="-1"></a>\mathbf w \cdot \mathbf x_a + w_0 &amp;= \mathbf w \cdot \mathbf x_b + w_0<span class="sc">\\</span></span>
<span id="cb14-141"><a href="#cb14-141" aria-hidden="true" tabindex="-1"></a>\mathbf w \cdot (\mathbf x_a -  \mathbf x_b) &amp;= 0,</span>
<span id="cb14-142"><a href="#cb14-142" aria-hidden="true" tabindex="-1"></a>\end{split}</span>
<span id="cb14-143"><a href="#cb14-143" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb14-144"><a href="#cb14-144" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-145"><a href="#cb14-145" aria-hidden="true" tabindex="-1"></a>donde el vector $\mathbf x_a -  \mathbf x_b$ es paralelo a $g(\mathbf x)=0$, ortogonal a $\mathbf w$ y el sub-espacio generado por $\mathbf w \cdot (\mathbf x_a -  \mathbf x_b) = 0$ pasa por el origen. </span>
<span id="cb14-146"><a href="#cb14-146" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-149"><a href="#cb14-149" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb14-150"><a href="#cb14-150" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-fold: true</span></span>
<span id="cb14-151"><a href="#cb14-151" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: Visualizando que $\mathbf w$ y la función discriminante son ortogonales.</span></span>
<span id="cb14-152"><a href="#cb14-152" aria-hidden="true" tabindex="-1"></a><span class="co">#| warning: false</span></span>
<span id="cb14-153"><a href="#cb14-153" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-lineal-ort</span></span>
<span id="cb14-154"><a href="#cb14-154" aria-hidden="true" tabindex="-1"></a>w <span class="op">=</span> np.array([w_1, w_2]) <span class="op">/</span> np.linalg.norm([w_1, w_2])</span>
<span id="cb14-155"><a href="#cb14-155" aria-hidden="true" tabindex="-1"></a>len_0 <span class="op">=</span> w_0 <span class="op">/</span> np.linalg.norm([w_1, w_2])</span>
<span id="cb14-156"><a href="#cb14-156" aria-hidden="true" tabindex="-1"></a>df <span class="op">=</span> pd.DataFrame(g_0 <span class="op">+</span> <span class="op">\</span></span>
<span id="cb14-157"><a href="#cb14-157" aria-hidden="true" tabindex="-1"></a>                  [<span class="bu">dict</span>(x1<span class="op">=</span>x, x2<span class="op">=</span>y, clase<span class="op">=</span><span class="st">'P'</span>) <span class="cf">for</span> x, y <span class="kw">in</span> X_1] <span class="op">+</span> <span class="op">\</span></span>
<span id="cb14-158"><a href="#cb14-158" aria-hidden="true" tabindex="-1"></a>                  [<span class="bu">dict</span>(x1<span class="op">=</span>x, x2<span class="op">=</span>y, clase<span class="op">=</span><span class="st">'N'</span>) <span class="cf">for</span> x, y <span class="kw">in</span> X_2] <span class="op">+</span> <span class="op">\</span></span>
<span id="cb14-159"><a href="#cb14-159" aria-hidden="true" tabindex="-1"></a>                  [<span class="bu">dict</span>(x1<span class="op">=</span><span class="dv">0</span>, x2<span class="op">=</span><span class="dv">0</span>, tipo<span class="op">=</span><span class="st">'lw'</span>),</span>
<span id="cb14-160"><a href="#cb14-160" aria-hidden="true" tabindex="-1"></a>                   <span class="bu">dict</span>(x1<span class="op">=-</span>w[<span class="dv">0</span>]<span class="op">*</span>len_0, x2<span class="op">=-</span>w[<span class="dv">1</span>]<span class="op">*</span>len_0, tipo<span class="op">=</span><span class="st">'lw'</span>)]</span>
<span id="cb14-161"><a href="#cb14-161" aria-hidden="true" tabindex="-1"></a>                 )</span>
<span id="cb14-162"><a href="#cb14-162" aria-hidden="true" tabindex="-1"></a>ax <span class="op">=</span> sns.scatterplot(data<span class="op">=</span>df, x<span class="op">=</span><span class="st">'x1'</span>, y<span class="op">=</span><span class="st">'x2'</span>, hue<span class="op">=</span><span class="st">'clase'</span>, legend<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb14-163"><a href="#cb14-163" aria-hidden="true" tabindex="-1"></a>sns.lineplot(data<span class="op">=</span>df, x<span class="op">=</span><span class="st">'x1'</span>, y<span class="op">=</span><span class="st">'x2'</span>, ax<span class="op">=</span>ax, hue<span class="op">=</span><span class="st">'tipo'</span>,</span>
<span id="cb14-164"><a href="#cb14-164" aria-hidden="true" tabindex="-1"></a>             palette<span class="op">=</span>[<span class="st">'k'</span>] <span class="op">+</span> sns.color_palette()[<span class="dv">2</span>:<span class="dv">3</span>],</span>
<span id="cb14-165"><a href="#cb14-165" aria-hidden="true" tabindex="-1"></a>             legend<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb14-166"><a href="#cb14-166" aria-hidden="true" tabindex="-1"></a>_ <span class="op">=</span> ax.axis(<span class="st">'equal'</span>)</span>
<span id="cb14-167"><a href="#cb14-167" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb14-168"><a href="#cb14-168" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-169"><a href="#cb14-169" aria-hidden="true" tabindex="-1"></a>En la figura anterior, $\ell \mathbf w$ corresponde al vector $\mathbf w$ multiplicado por un factor $\ell$ de tal manera que intersecte con $g(\mathbf x)=0.$ El factor $\ell$ corresponde a la distancia que hay del origen a $g(\mathbf x)=0$ la cual es $\ell = \frac{w_0}{\mid\mid \mathbf w \mid\mid}.$ El signo de $\ell$ indica el lado donde se encuentra el origen con respecto a $g(\mathbf x)=0.$</span>
<span id="cb14-170"><a href="#cb14-170" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-171"><a href="#cb14-171" aria-hidden="true" tabindex="-1"></a>La @fig-lineal-dist-hyp muestra en rojo la línea generada por $\mathbf w \cdot \mathbf x=0$, la función discriminante $g(\mathbf x)=0$ (negro), la línea puntuada muestra la distancia entre ellas, que corresponde a $\ell$ y el vector $\mathbf w$. Visualmente, se observa que $\mathbf w$ está pegado a la línea roja, pero esto solo es un efecto de la resolución y estos elementos no se tocan. </span>
<span id="cb14-172"><a href="#cb14-172" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-175"><a href="#cb14-175" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb14-176"><a href="#cb14-176" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-fold: true</span></span>
<span id="cb14-177"><a href="#cb14-177" aria-hidden="true" tabindex="-1"></a><span class="co">#| warning: false</span></span>
<span id="cb14-178"><a href="#cb14-178" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: Geometría de la función discriminante.</span></span>
<span id="cb14-179"><a href="#cb14-179" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-lineal-dist-hyp</span></span>
<span id="cb14-180"><a href="#cb14-180" aria-hidden="true" tabindex="-1"></a>vec <span class="op">=</span> np.array([<span class="dv">1</span>, (<span class="op">-</span> w_1 <span class="op">*</span> <span class="dv">1</span>) <span class="op">/</span> w_2])</span>
<span id="cb14-181"><a href="#cb14-181" aria-hidden="true" tabindex="-1"></a>x_max <span class="op">=</span> T[:, <span class="dv">0</span>].<span class="bu">max</span>()</span>
<span id="cb14-182"><a href="#cb14-182" aria-hidden="true" tabindex="-1"></a>length <span class="op">=</span> np.linalg.norm(np.array([x_max, (<span class="op">-</span>w_0 <span class="op">-</span> w_1 <span class="op">*</span> x_max) <span class="op">/</span> w_2]) <span class="op">-</span></span>
<span id="cb14-183"><a href="#cb14-183" aria-hidden="true" tabindex="-1"></a>                        np.array([<span class="op">-</span>w[<span class="dv">0</span>]<span class="op">*</span>len_0, <span class="op">-</span>w[<span class="dv">1</span>]<span class="op">*</span>len_0]))</span>
<span id="cb14-184"><a href="#cb14-184" aria-hidden="true" tabindex="-1"></a>vec_der <span class="op">=</span> length <span class="op">*</span> vec <span class="op">/</span> np.linalg.norm(vec)</span>
<span id="cb14-185"><a href="#cb14-185" aria-hidden="true" tabindex="-1"></a>x_min <span class="op">=</span> T[:, <span class="dv">0</span>].<span class="bu">min</span>()</span>
<span id="cb14-186"><a href="#cb14-186" aria-hidden="true" tabindex="-1"></a>length <span class="op">=</span> np.linalg.norm(np.array([x_min, (<span class="op">-</span>w_0 <span class="op">-</span> w_1 <span class="op">*</span> x_min) <span class="op">/</span> w_2]) <span class="op">-</span></span>
<span id="cb14-187"><a href="#cb14-187" aria-hidden="true" tabindex="-1"></a>                        np.array([<span class="op">-</span>w[<span class="dv">0</span>]<span class="op">*</span>len_0, <span class="op">-</span>w[<span class="dv">1</span>]<span class="op">*</span>len_0]))</span>
<span id="cb14-188"><a href="#cb14-188" aria-hidden="true" tabindex="-1"></a>vec_izq <span class="op">=</span> <span class="op">-</span>length <span class="op">*</span> vec <span class="op">/</span> np.linalg.norm(vec)</span>
<span id="cb14-189"><a href="#cb14-189" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-190"><a href="#cb14-190" aria-hidden="true" tabindex="-1"></a>g <span class="op">=</span> [<span class="bu">dict</span>(x1<span class="op">=</span>x, x2<span class="op">=</span>(<span class="op">-</span> w_1 <span class="op">*</span> x) <span class="op">/</span> w_2, tipo<span class="op">=</span><span class="st">'wx=0'</span>)</span>
<span id="cb14-191"><a href="#cb14-191" aria-hidden="true" tabindex="-1"></a>     <span class="cf">for</span> x <span class="kw">in</span> np.linspace(vec_izq[<span class="dv">0</span>], vec_der[<span class="dv">0</span>])]</span>
<span id="cb14-192"><a href="#cb14-192" aria-hidden="true" tabindex="-1"></a>df <span class="op">=</span> pd.DataFrame([<span class="bu">dict</span>(x1<span class="op">=</span>x, x2<span class="op">=</span>y, clase<span class="op">=</span><span class="st">'P'</span>) <span class="cf">for</span> x, y <span class="kw">in</span> X_1] <span class="op">+</span> <span class="op">\</span></span>
<span id="cb14-193"><a href="#cb14-193" aria-hidden="true" tabindex="-1"></a>                  [<span class="bu">dict</span>(x1<span class="op">=</span>x, x2<span class="op">=</span>y, clase<span class="op">=</span><span class="st">'N'</span>) <span class="cf">for</span> x, y <span class="kw">in</span> X_2] <span class="op">+\</span></span>
<span id="cb14-194"><a href="#cb14-194" aria-hidden="true" tabindex="-1"></a>                  [<span class="bu">dict</span>(x1<span class="op">=</span>w_1, x2<span class="op">=</span>w_2, clase<span class="op">=</span><span class="st">'w'</span>)] <span class="op">+\</span></span>
<span id="cb14-195"><a href="#cb14-195" aria-hidden="true" tabindex="-1"></a>                  g_0 <span class="op">+</span> g)</span>
<span id="cb14-196"><a href="#cb14-196" aria-hidden="true" tabindex="-1"></a>ax <span class="op">=</span> sns.scatterplot(data<span class="op">=</span>df, x<span class="op">=</span><span class="st">'x1'</span>, y<span class="op">=</span><span class="st">'x2'</span>, hue<span class="op">=</span><span class="st">'clase'</span>, legend<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb14-197"><a href="#cb14-197" aria-hidden="true" tabindex="-1"></a>sns.lineplot(data<span class="op">=</span>df, x<span class="op">=</span><span class="st">'x1'</span>, y<span class="op">=</span><span class="st">'x2'</span>, ax<span class="op">=</span>ax, hue<span class="op">=</span><span class="st">'tipo'</span>,</span>
<span id="cb14-198"><a href="#cb14-198" aria-hidden="true" tabindex="-1"></a>             palette<span class="op">=</span>[<span class="st">'k'</span>] <span class="op">+</span> sns.color_palette()[<span class="dv">3</span>:<span class="dv">4</span>],</span>
<span id="cb14-199"><a href="#cb14-199" aria-hidden="true" tabindex="-1"></a>             legend<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb14-200"><a href="#cb14-200" aria-hidden="true" tabindex="-1"></a>ax.plot([vec_der[<span class="dv">0</span>], x_max], [vec_der[<span class="dv">1</span>], (<span class="op">-</span>w_0 <span class="op">-</span> w_1 <span class="op">*</span> x_max) <span class="op">/</span> w_2], <span class="st">'--'</span>,</span>
<span id="cb14-201"><a href="#cb14-201" aria-hidden="true" tabindex="-1"></a>        color<span class="op">=</span>sns.color_palette()[<span class="dv">4</span>])</span>
<span id="cb14-202"><a href="#cb14-202" aria-hidden="true" tabindex="-1"></a>_ <span class="op">=</span> ax.axis(<span class="st">'equal'</span>)</span>
<span id="cb14-203"><a href="#cb14-203" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb14-204"><a href="#cb14-204" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-205"><a href="#cb14-205" aria-hidden="true" tabindex="-1"></a>Finalmente, será de utilidad representar a cada punto en $\mathcal D$ de la siguiente manera </span>
<span id="cb14-206"><a href="#cb14-206" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-207"><a href="#cb14-207" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb14-208"><a href="#cb14-208" aria-hidden="true" tabindex="-1"></a>\mathbf x = \mathbf x_g + \ell \frac{\mathbf w}{\mid\mid \mathbf w \mid\mid},</span>
<span id="cb14-209"><a href="#cb14-209" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb14-210"><a href="#cb14-210" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-211"><a href="#cb14-211" aria-hidden="true" tabindex="-1"></a>donde $\mathbf x_g$ corresponde a la proyección en el hiperplano ($g(\mathbf x) = 0$) de $\mathbf x$ y $\ell$ es la distancia que hay del hiperplano a $\mathbf x$. Utilizando esta representación se puede derivar la distancia $\ell$ de $\mathbf x$ con el siguiente procedimiento. </span>
<span id="cb14-212"><a href="#cb14-212" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-213"><a href="#cb14-213" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb14-214"><a href="#cb14-214" aria-hidden="true" tabindex="-1"></a>\begin{split}</span>
<span id="cb14-215"><a href="#cb14-215" aria-hidden="true" tabindex="-1"></a>g(\mathbf x) &amp;= g(\mathbf x_g + \ell \frac{\mathbf w}{\mid\mid \mathbf w \mid\mid})<span class="sc">\\</span></span>
<span id="cb14-216"><a href="#cb14-216" aria-hidden="true" tabindex="-1"></a>&amp;= \mathbf w \cdot (\mathbf x_g + \ell \frac{\mathbf w}{\mid\mid \mathbf w \mid\mid}) + w_0<span class="sc">\\</span></span>
<span id="cb14-217"><a href="#cb14-217" aria-hidden="true" tabindex="-1"></a>&amp;= \mathbf w \cdot (\mathbf x_g + \ell \frac{\mathbf w}{\mid\mid \mathbf w \mid\mid})<span class="sc">\\</span></span>
<span id="cb14-218"><a href="#cb14-218" aria-hidden="true" tabindex="-1"></a>&amp;= \mathbf w \cdot \mathbf x_g + \ell \mathbf w \cdot \frac{\mathbf w}{\mid\mid \mathbf w \mid\mid}<span class="sc">\\</span></span>
<span id="cb14-219"><a href="#cb14-219" aria-hidden="true" tabindex="-1"></a>&amp;= \ell \mathbf w \cdot \frac{\mathbf w}{\mid\mid \mathbf w \mid\mid}<span class="sc">\\</span></span>
<span id="cb14-220"><a href="#cb14-220" aria-hidden="true" tabindex="-1"></a>&amp;= \ell \mid\mid\mathbf w\mid\mid<span class="sc">\\</span></span>
<span id="cb14-221"><a href="#cb14-221" aria-hidden="true" tabindex="-1"></a>\ell &amp;= \frac{g(\mathbf x)}{\mid\mid\mathbf w \mid\mid}</span>
<span id="cb14-222"><a href="#cb14-222" aria-hidden="true" tabindex="-1"></a>\end{split}</span>
<span id="cb14-223"><a href="#cb14-223" aria-hidden="true" tabindex="-1"></a>$$ {#eq-distancia-hiperplano}</span>
<span id="cb14-224"><a href="#cb14-224" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-225"><a href="#cb14-225" aria-hidden="true" tabindex="-1"></a>Como ya se había visto la distancia del origen al hiperplano está dada por $\ell_0 = \frac{w_0}{\mid\mid\mathbf w \mid\mid}$ y de cualquier elemento por $\ell_{\mathbf x} = \frac{g(\mathbf x)}{\mid\mid\mathbf w \mid\mid}.$ La @fig-lineal-dis-ele muestra la $\ell_{\mathbf x}$ en un elemento de la clase negativa. Se puede observar el punto $\mathbf x_g$ que es donde intersecta la línea con el hiperplano.</span>
<span id="cb14-226"><a href="#cb14-226" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-229"><a href="#cb14-229" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb14-230"><a href="#cb14-230" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-fold: true</span></span>
<span id="cb14-231"><a href="#cb14-231" aria-hidden="true" tabindex="-1"></a><span class="co">#| warning: false</span></span>
<span id="cb14-232"><a href="#cb14-232" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: Distancia ($\ell_{\mathbf x} = x$) de un elemento al hiperplano</span></span>
<span id="cb14-233"><a href="#cb14-233" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-lineal-dis-ele</span></span>
<span id="cb14-234"><a href="#cb14-234" aria-hidden="true" tabindex="-1"></a>point <span class="op">=</span> X_2[X_2.argmax(axis<span class="op">=</span><span class="dv">0</span>)[<span class="dv">1</span>]]</span>
<span id="cb14-235"><a href="#cb14-235" aria-hidden="true" tabindex="-1"></a>point_g <span class="op">=</span> vec <span class="op">*</span>  np.dot(point, vec) <span class="op">/</span> np.dot(vec, vec) <span class="op">-</span> len_0 <span class="op">*</span> w</span>
<span id="cb14-236"><a href="#cb14-236" aria-hidden="true" tabindex="-1"></a>df <span class="op">=</span> pd.DataFrame(g_0 <span class="op">+</span> <span class="op">\</span></span>
<span id="cb14-237"><a href="#cb14-237" aria-hidden="true" tabindex="-1"></a>                  [<span class="bu">dict</span>(x1<span class="op">=</span>x, x2<span class="op">=</span>y, clase<span class="op">=</span><span class="st">'P'</span>) <span class="cf">for</span> x, y <span class="kw">in</span> X_1] <span class="op">+\</span></span>
<span id="cb14-238"><a href="#cb14-238" aria-hidden="true" tabindex="-1"></a>                  [<span class="bu">dict</span>(x1<span class="op">=</span>x, x2<span class="op">=</span>y, clase<span class="op">=</span><span class="st">'N'</span>) <span class="cf">for</span> x, y <span class="kw">in</span> X_2] <span class="op">+\</span></span>
<span id="cb14-239"><a href="#cb14-239" aria-hidden="true" tabindex="-1"></a>                  [<span class="bu">dict</span>(x1<span class="op">=</span>point_g[<span class="dv">0</span>], x2<span class="op">=</span>point_g[<span class="dv">1</span>], tipo<span class="op">=</span><span class="st">'lx'</span>)] <span class="op">+\</span></span>
<span id="cb14-240"><a href="#cb14-240" aria-hidden="true" tabindex="-1"></a>                  [<span class="bu">dict</span>(x1<span class="op">=</span>point[<span class="dv">0</span>], x2<span class="op">=</span>point[<span class="dv">1</span>], tipo<span class="op">=</span><span class="st">'lx'</span>)]                  </span>
<span id="cb14-241"><a href="#cb14-241" aria-hidden="true" tabindex="-1"></a>                 )</span>
<span id="cb14-242"><a href="#cb14-242" aria-hidden="true" tabindex="-1"></a>ax <span class="op">=</span> sns.scatterplot(data<span class="op">=</span>df, x<span class="op">=</span><span class="st">'x1'</span>, y<span class="op">=</span><span class="st">'x2'</span>, hue<span class="op">=</span><span class="st">'clase'</span>, legend<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb14-243"><a href="#cb14-243" aria-hidden="true" tabindex="-1"></a>sns.lineplot(data<span class="op">=</span>df, x<span class="op">=</span><span class="st">'x1'</span>, y<span class="op">=</span><span class="st">'x2'</span>, ax<span class="op">=</span>ax,</span>
<span id="cb14-244"><a href="#cb14-244" aria-hidden="true" tabindex="-1"></a>             hue<span class="op">=</span><span class="st">'tipo'</span>,palette<span class="op">=</span>[<span class="st">'k'</span>] <span class="op">+</span> sns.color_palette()[<span class="dv">4</span>:<span class="dv">5</span>], legend<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb14-245"><a href="#cb14-245" aria-hidden="true" tabindex="-1"></a>_ <span class="op">=</span> ax.axis(<span class="st">'equal'</span>)</span>
<span id="cb14-246"><a href="#cb14-246" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb14-247"><a href="#cb14-247" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-248"><a href="#cb14-248" aria-hidden="true" tabindex="-1"></a>Considerando que el problema mostrado en la figura anterior está en $\mathbb R^2$, entonces $\mathbf x_g$ está dado por </span>
<span id="cb14-249"><a href="#cb14-249" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-250"><a href="#cb14-250" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb14-251"><a href="#cb14-251" aria-hidden="true" tabindex="-1"></a>\mathbf x_g = \frac{\mathbf x \cdot \mathbf x_0}{\mathbf x_0 \cdot \mathbf x_0} \mathbf x_0 - \ell_0 \frac{\mathbf w}{\mid\mid\mathbf w \mid\mid},</span>
<span id="cb14-252"><a href="#cb14-252" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb14-253"><a href="#cb14-253" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-254"><a href="#cb14-254" aria-hidden="true" tabindex="-1"></a>donde $\ell_0$ es la distancia del origen al hiperplano y $\mathbf x_0$ es cualquier vector que está en $\mathbf x_0 \cdot \mathbf w=0.$ Para dimensiones mayores el término $\frac{\mathbf x \cdot \mathbf x_0}{\mathbf x_0 \cdot \mathbf x_0}$ es la proyección al hiperplano $A$ tal que $A \mathbf w = 0.$</span>
<span id="cb14-255"><a href="#cb14-255" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-256"><a href="#cb14-256" aria-hidden="true" tabindex="-1"></a><span class="fu">### Múltiples Clases {#sec-multiples-clases}</span></span>
<span id="cb14-257"><a href="#cb14-257" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-258"><a href="#cb14-258" aria-hidden="true" tabindex="-1"></a>Una manera de tratar un problema de $K$ clases, es convertirlo en $K$ problemas de clasificación binarios, a este procedimiento se le conoce como _Uno vs Resto_. La idea es entrenar $K$ clasificadores donde la clase positiva corresponde a cada una de las clases y la clase de negativa se construye con todas las clases que no son la clase positiva en esa iteración. Finalmente, la clase predicha corresponde al clasificador que tiene el valor máximo en la función discriminante. </span>
<span id="cb14-259"><a href="#cb14-259" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-260"><a href="#cb14-260" aria-hidden="true" tabindex="-1"></a>La @fig-lineal-multiclase ejemplifica el comportamiento de esta técnica en un problema de tres clases y utilizando un clasificador con discrimitante lineal. En la figura se muestra las tres funciones discriminantes $g_k(\mathbf x)=0$, los parámetros escalados de esas funciones, i.e., $\ell_k \mathbf w_k$ y los datos. Por ejemplo se observa como la clase $1$ mostrada en azul, se separa de las otras dos clases con la función $g_1(\mathbf x)=0$, es decir, para $g_1(\mathbf x)=0$ la clase positiva es $1$ y la clase negativa corresponde a los elementos que corresponde a las clases $2$ y $3.$</span>
<span id="cb14-261"><a href="#cb14-261" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-264"><a href="#cb14-264" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb14-265"><a href="#cb14-265" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-fold: true</span></span>
<span id="cb14-266"><a href="#cb14-266" aria-hidden="true" tabindex="-1"></a><span class="co">#| warning: false</span></span>
<span id="cb14-267"><a href="#cb14-267" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: Problema multiclase</span></span>
<span id="cb14-268"><a href="#cb14-268" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-lineal-multiclase</span></span>
<span id="cb14-269"><a href="#cb14-269" aria-hidden="true" tabindex="-1"></a>seed <span class="op">=</span> <span class="dv">3</span></span>
<span id="cb14-270"><a href="#cb14-270" aria-hidden="true" tabindex="-1"></a>X_1 <span class="op">=</span> multivariate_normal(mean<span class="op">=</span>[<span class="dv">15</span>, <span class="dv">20</span>],</span>
<span id="cb14-271"><a href="#cb14-271" aria-hidden="true" tabindex="-1"></a>                          seed<span class="op">=</span>seed,</span>
<span id="cb14-272"><a href="#cb14-272" aria-hidden="true" tabindex="-1"></a>                          cov<span class="op">=</span>[[<span class="dv">3</span>, <span class="op">-</span><span class="dv">3</span>], [<span class="op">-</span><span class="dv">3</span>, <span class="dv">8</span>]]).rvs(<span class="dv">1000</span>)</span>
<span id="cb14-273"><a href="#cb14-273" aria-hidden="true" tabindex="-1"></a>X_2 <span class="op">=</span> multivariate_normal(mean<span class="op">=</span>[<span class="dv">5</span>, <span class="dv">5</span>],</span>
<span id="cb14-274"><a href="#cb14-274" aria-hidden="true" tabindex="-1"></a>                          seed<span class="op">=</span>seed,</span>
<span id="cb14-275"><a href="#cb14-275" aria-hidden="true" tabindex="-1"></a>                          cov<span class="op">=</span>[[<span class="dv">4</span>, <span class="dv">0</span>], [<span class="dv">0</span>, <span class="dv">2</span>]]).rvs(<span class="dv">1000</span>)</span>
<span id="cb14-276"><a href="#cb14-276" aria-hidden="true" tabindex="-1"></a>X_3 <span class="op">=</span> multivariate_normal(mean<span class="op">=</span>[<span class="op">-</span><span class="dv">5</span>, <span class="dv">20</span>],</span>
<span id="cb14-277"><a href="#cb14-277" aria-hidden="true" tabindex="-1"></a>                          seed<span class="op">=</span>seed,</span>
<span id="cb14-278"><a href="#cb14-278" aria-hidden="true" tabindex="-1"></a>                          cov<span class="op">=</span>[[<span class="dv">2</span>, <span class="dv">1</span>], [<span class="dv">1</span>, <span class="dv">2</span>]]).rvs(<span class="dv">1000</span>)</span>
<span id="cb14-279"><a href="#cb14-279" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-280"><a href="#cb14-280" aria-hidden="true" tabindex="-1"></a>T <span class="op">=</span> np.concatenate((X_1, X_2, X_3))</span>
<span id="cb14-281"><a href="#cb14-281" aria-hidden="true" tabindex="-1"></a>y_t <span class="op">=</span> np.array([<span class="st">'1'</span>] <span class="op">*</span> X_1.shape[<span class="dv">0</span>] <span class="op">+</span> [<span class="st">'2'</span>] <span class="op">*</span> X_2.shape[<span class="dv">0</span>] <span class="op">+</span> [<span class="st">'3'</span>] <span class="op">*</span> X_3.shape[<span class="dv">0</span>])</span>
<span id="cb14-282"><a href="#cb14-282" aria-hidden="true" tabindex="-1"></a>linear <span class="op">=</span> LinearSVC(dual<span class="op">=</span><span class="va">False</span>).fit(T, y_t)</span>
<span id="cb14-283"><a href="#cb14-283" aria-hidden="true" tabindex="-1"></a>g_0 <span class="op">=</span> []</span>
<span id="cb14-284"><a href="#cb14-284" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i, (w, w_0) <span class="kw">in</span> <span class="bu">enumerate</span>(<span class="bu">zip</span>(linear.coef_, linear.intercept_)):</span>
<span id="cb14-285"><a href="#cb14-285" aria-hidden="true" tabindex="-1"></a>    w_1, w_2 <span class="op">=</span> w</span>
<span id="cb14-286"><a href="#cb14-286" aria-hidden="true" tabindex="-1"></a>    g_0 <span class="op">+=</span> [<span class="bu">dict</span>(x1<span class="op">=</span>x, x2<span class="op">=</span>y, tipo<span class="op">=</span><span class="ss">f'g</span><span class="sc">{</span>i<span class="op">+</span><span class="dv">1</span><span class="sc">}</span><span class="ss">(x)=0'</span>)</span>
<span id="cb14-287"><a href="#cb14-287" aria-hidden="true" tabindex="-1"></a>            <span class="cf">for</span> x, y <span class="kw">in</span> <span class="bu">zip</span>(T[:, <span class="dv">0</span>], (<span class="op">-</span>w_0 <span class="op">-</span> w[<span class="dv">0</span>] <span class="op">*</span> T[:, <span class="dv">0</span>]) <span class="op">/</span> w[<span class="dv">1</span>])]</span>
<span id="cb14-288"><a href="#cb14-288" aria-hidden="true" tabindex="-1"></a>W <span class="op">=</span> [<span class="op">-</span>w0 <span class="op">*</span> w <span class="op">/</span> np.linalg.norm(w)<span class="op">**</span><span class="dv">2</span> </span>
<span id="cb14-289"><a href="#cb14-289" aria-hidden="true" tabindex="-1"></a>     <span class="cf">for</span> w, w0 <span class="kw">in</span> <span class="bu">zip</span>(linear.coef_, linear.intercept_)]    </span>
<span id="cb14-290"><a href="#cb14-290" aria-hidden="true" tabindex="-1"></a>df <span class="op">=</span> pd.DataFrame(g_0 <span class="op">+</span> <span class="op">\</span></span>
<span id="cb14-291"><a href="#cb14-291" aria-hidden="true" tabindex="-1"></a>                  [<span class="bu">dict</span>(x1<span class="op">=</span>x, x2<span class="op">=</span>y, clase<span class="op">=</span><span class="st">'1'</span>) <span class="cf">for</span> x, y <span class="kw">in</span> X_1] <span class="op">+\</span></span>
<span id="cb14-292"><a href="#cb14-292" aria-hidden="true" tabindex="-1"></a>                  [<span class="bu">dict</span>(x1<span class="op">=</span>x, x2<span class="op">=</span>y, clase<span class="op">=</span><span class="st">'2'</span>) <span class="cf">for</span> x, y <span class="kw">in</span> X_2] <span class="op">+\</span></span>
<span id="cb14-293"><a href="#cb14-293" aria-hidden="true" tabindex="-1"></a>                  [<span class="bu">dict</span>(x1<span class="op">=</span>x, x2<span class="op">=</span>y, clase<span class="op">=</span><span class="st">'3'</span>) <span class="cf">for</span> x, y <span class="kw">in</span> X_3] <span class="op">+\</span></span>
<span id="cb14-294"><a href="#cb14-294" aria-hidden="true" tabindex="-1"></a>                  [<span class="bu">dict</span>(x1<span class="op">=</span>w_1, x2<span class="op">=</span>w_2, clase<span class="op">=</span><span class="ss">f'lw</span><span class="sc">{</span>i<span class="op">+</span><span class="dv">1</span><span class="sc">}</span><span class="ss">'</span>)</span>
<span id="cb14-295"><a href="#cb14-295" aria-hidden="true" tabindex="-1"></a>                   <span class="cf">for</span> i, (w_1, w_2) <span class="kw">in</span> <span class="bu">enumerate</span>(W)]                  </span>
<span id="cb14-296"><a href="#cb14-296" aria-hidden="true" tabindex="-1"></a>                 )</span>
<span id="cb14-297"><a href="#cb14-297" aria-hidden="true" tabindex="-1"></a>ax <span class="op">=</span> sns.scatterplot(data<span class="op">=</span>df, x<span class="op">=</span><span class="st">'x1'</span>, y<span class="op">=</span><span class="st">'x2'</span>, hue<span class="op">=</span><span class="st">'clase'</span>, legend<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb14-298"><a href="#cb14-298" aria-hidden="true" tabindex="-1"></a>sns.lineplot(data<span class="op">=</span>df, x<span class="op">=</span><span class="st">'x1'</span>, y<span class="op">=</span><span class="st">'x2'</span>, ax<span class="op">=</span>ax, hue<span class="op">=</span><span class="st">'tipo'</span>,</span>
<span id="cb14-299"><a href="#cb14-299" aria-hidden="true" tabindex="-1"></a>             palette<span class="op">=</span>sns.color_palette()[<span class="dv">6</span>:<span class="dv">9</span>], legend<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb14-300"><a href="#cb14-300" aria-hidden="true" tabindex="-1"></a>_ <span class="op">=</span> ax.axis(<span class="st">'equal'</span>)</span>
<span id="cb14-301"><a href="#cb14-301" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb14-302"><a href="#cb14-302" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-303"><a href="#cb14-303" aria-hidden="true" tabindex="-1"></a><span class="fu">## Máquinas de Soporte Vectorial {#sec-svm}</span></span>
<span id="cb14-304"><a href="#cb14-304" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-305"><a href="#cb14-305" aria-hidden="true" tabindex="-1"></a>Es momento de describir algunos algoritmos para estimar los parámetros $\mathbf w$, y $w_0$ empezando por las máquinas de soporte vectorial. En este clasificador se asume un problema binario y las clases están representadas por $-1$ y $1$, es decir, $y \in <span class="sc">\{</span>-1, 1<span class="sc">\}</span>$. Entonces, las máquinas de soporte vectorial tratan de encontrar una función con las siguientes características. </span>
<span id="cb14-306"><a href="#cb14-306" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-307"><a href="#cb14-307" aria-hidden="true" tabindex="-1"></a>Sea $\mathbf x_i$ un ejemplo que corresponde a la clase $1$ entonces se busca $\mathbf w$ tal que</span>
<span id="cb14-308"><a href="#cb14-308" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-309"><a href="#cb14-309" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb14-310"><a href="#cb14-310" aria-hidden="true" tabindex="-1"></a>\mathbf w \cdot \mathbf x_i + w_0 \geq +1.</span>
<span id="cb14-311"><a href="#cb14-311" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb14-312"><a href="#cb14-312" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-313"><a href="#cb14-313" aria-hidden="true" tabindex="-1"></a>En el caso contrario, es decir, $\mathbf x_i$ un ejemplo de la clase $-1$, entonces </span>
<span id="cb14-314"><a href="#cb14-314" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-315"><a href="#cb14-315" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb14-316"><a href="#cb14-316" aria-hidden="true" tabindex="-1"></a>\mathbf w \cdot \mathbf x_i + w_0 \leq -1.</span>
<span id="cb14-317"><a href="#cb14-317" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb14-318"><a href="#cb14-318" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-319"><a href="#cb14-319" aria-hidden="true" tabindex="-1"></a>Estas ecuaciones se pueden escribir como </span>
<span id="cb14-320"><a href="#cb14-320" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-321"><a href="#cb14-321" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb14-322"><a href="#cb14-322" aria-hidden="true" tabindex="-1"></a>(\mathbf w \cdot \mathbf x_i + w_0) y_i \geq +1,</span>
<span id="cb14-323"><a href="#cb14-323" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb14-324"><a href="#cb14-324" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-325"><a href="#cb14-325" aria-hidden="true" tabindex="-1"></a>donde $(\mathbf x_i, y_i) \in \mathcal D.$ </span>
<span id="cb14-326"><a href="#cb14-326" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-327"><a href="#cb14-327" aria-hidden="true" tabindex="-1"></a>La función discriminante es $g(\mathbf x) = \mathbf w \cdot \mathbf x + w_0$ y la distancia (@eq-distancia-hiperplano) que existe entre cualquier punto $\mathbf x_i$ al discriminante está dada por </span>
<span id="cb14-328"><a href="#cb14-328" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-329"><a href="#cb14-329" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb14-330"><a href="#cb14-330" aria-hidden="true" tabindex="-1"></a>\frac{g(\mathbf x_i)}{\mid\mid \mathbf w \mid\mid}y_i.</span>
<span id="cb14-331"><a href="#cb14-331" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb14-332"><a href="#cb14-332" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-333"><a href="#cb14-333" aria-hidden="true" tabindex="-1"></a>Entonces, se puede ver que lo que se busca es encontrar $\mathbf w$ de tal manera que cualquier punto $\mathbf x_i$ esté lo mas alejada posible del discriminante, esto se logra minimizando $\mathbf w$, es decir, resolviendo el siguiente problema de optimización:</span>
<span id="cb14-334"><a href="#cb14-334" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-335"><a href="#cb14-335" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb14-336"><a href="#cb14-336" aria-hidden="true" tabindex="-1"></a>\min \frac{1}{2} \mid\mid\mathbf w \mid\mid</span>
<span id="cb14-337"><a href="#cb14-337" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb14-338"><a href="#cb14-338" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-339"><a href="#cb14-339" aria-hidden="true" tabindex="-1"></a>sujeto a $(\mathbf w \cdot \mathbf x_i + w_0) y_i \geq +1, \forall (\mathbf x_i, y_i) \in \mathcal D.$</span>
<span id="cb14-340"><a href="#cb14-340" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-341"><a href="#cb14-341" aria-hidden="true" tabindex="-1"></a><span class="fu">### Optimización</span></span>
<span id="cb14-342"><a href="#cb14-342" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-343"><a href="#cb14-343" aria-hidden="true" tabindex="-1"></a>Este es un problema de optimización que se puede resolver utilizando multiplicadores de Lagrange lo cual quedaría como </span>
<span id="cb14-344"><a href="#cb14-344" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-345"><a href="#cb14-345" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb14-346"><a href="#cb14-346" aria-hidden="true" tabindex="-1"></a>f_p = \frac{1}{2}\mid\mid\mathbf w \mid\mid - \sum_i^N \alpha_i ((\mathbf w \cdot \mathbf x_i + w_0) y_i - 1),</span>
<span id="cb14-347"><a href="#cb14-347" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb14-348"><a href="#cb14-348" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-349"><a href="#cb14-349" aria-hidden="true" tabindex="-1"></a>donde el mínimo corresponde a maximizar con respecto a $\alpha_i \geq 0$ y minimizar con respecto a $\mathbf w$ y $w_0.$ En esta formulación existe el problema para aquellos problemas donde no es posible encontrar un hiperplano que separa las dos clases. Para estos casos donde no es posible encontrar una separación perfecta se propone utilizar </span>
<span id="cb14-350"><a href="#cb14-350" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-351"><a href="#cb14-351" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb14-352"><a href="#cb14-352" aria-hidden="true" tabindex="-1"></a>(\mathbf w \cdot \mathbf x_i + w_0) y_i \geq 1 - \xi_i,</span>
<span id="cb14-353"><a href="#cb14-353" aria-hidden="true" tabindex="-1"></a>$$ </span>
<span id="cb14-354"><a href="#cb14-354" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-355"><a href="#cb14-355" aria-hidden="true" tabindex="-1"></a>donde $\xi$ captura los errores empezando por aquellos elementos que están del lado correcto del hiperplano, pero que no son mayores a $1$. La @fig-lineal-xi muestra un ejemplo donde existe un elemento negativo que se encuentra entre la función de decisión y el hiperplano de margen, i.e., el que corresponde a la restricción $\mathbf w \cdot \mathbf x_i + w_0 \geq 1$, es decir ese punto tiene un $0 &lt; \xi &lt; 1.$ También se observa un elemento positivo que está muy cerca a $g(\mathbf x) = 1.$</span>
<span id="cb14-356"><a href="#cb14-356" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-359"><a href="#cb14-359" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb14-360"><a href="#cb14-360" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-fold: true</span></span>
<span id="cb14-361"><a href="#cb14-361" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: Hiperplanos</span></span>
<span id="cb14-362"><a href="#cb14-362" aria-hidden="true" tabindex="-1"></a><span class="co">#| warning: false</span></span>
<span id="cb14-363"><a href="#cb14-363" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-lineal-xi</span></span>
<span id="cb14-364"><a href="#cb14-364" aria-hidden="true" tabindex="-1"></a>seed <span class="op">=</span> <span class="dv">2</span></span>
<span id="cb14-365"><a href="#cb14-365" aria-hidden="true" tabindex="-1"></a>X_1 <span class="op">=</span> multivariate_normal(mean<span class="op">=</span>[<span class="dv">15</span>, <span class="dv">20</span>], cov<span class="op">=</span>[[<span class="dv">3</span>, <span class="op">-</span><span class="dv">3</span>], [<span class="op">-</span><span class="dv">3</span>, <span class="dv">8</span>]], seed<span class="op">=</span>seed).rvs(<span class="dv">1000</span>)</span>
<span id="cb14-366"><a href="#cb14-366" aria-hidden="true" tabindex="-1"></a>X_2 <span class="op">=</span> multivariate_normal(mean<span class="op">=</span>[<span class="dv">8</span>, <span class="dv">8</span>], cov<span class="op">=</span>[[<span class="dv">4</span>, <span class="dv">0</span>], [<span class="dv">0</span>, <span class="dv">2</span>]], seed<span class="op">=</span>seed).rvs(<span class="dv">1000</span>)</span>
<span id="cb14-367"><a href="#cb14-367" aria-hidden="true" tabindex="-1"></a>T <span class="op">=</span> np.concatenate((X_1, X_2))</span>
<span id="cb14-368"><a href="#cb14-368" aria-hidden="true" tabindex="-1"></a>y_t <span class="op">=</span> np.array([<span class="st">'P'</span>] <span class="op">*</span> X_1.shape[<span class="dv">0</span>] <span class="op">+</span> [<span class="st">'N'</span>] <span class="op">*</span> X_2.shape[<span class="dv">0</span>])</span>
<span id="cb14-369"><a href="#cb14-369" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-370"><a href="#cb14-370" aria-hidden="true" tabindex="-1"></a>linear <span class="op">=</span> LinearSVC(dual<span class="op">=</span><span class="va">False</span>).fit(T, y_t)</span>
<span id="cb14-371"><a href="#cb14-371" aria-hidden="true" tabindex="-1"></a>w_1, w_2 <span class="op">=</span> linear.coef_[<span class="dv">0</span>]</span>
<span id="cb14-372"><a href="#cb14-372" aria-hidden="true" tabindex="-1"></a>w_0 <span class="op">=</span> linear.intercept_[<span class="dv">0</span>]</span>
<span id="cb14-373"><a href="#cb14-373" aria-hidden="true" tabindex="-1"></a>w <span class="op">=</span> np.array([w_1, w_2]) <span class="op">/</span> np.linalg.norm([w_1, w_2])</span>
<span id="cb14-374"><a href="#cb14-374" aria-hidden="true" tabindex="-1"></a>g_0 <span class="op">=</span> [<span class="bu">dict</span>(x1<span class="op">=</span>x, x2<span class="op">=</span>y, tipo<span class="op">=</span><span class="st">'g(x)=0'</span>)</span>
<span id="cb14-375"><a href="#cb14-375" aria-hidden="true" tabindex="-1"></a>       <span class="cf">for</span> x, y <span class="kw">in</span> <span class="bu">zip</span>(T[:, <span class="dv">0</span>], (<span class="op">-</span>w_0 <span class="op">-</span> w_1 <span class="op">*</span> T[:, <span class="dv">0</span>]) <span class="op">/</span> w_2)]</span>
<span id="cb14-376"><a href="#cb14-376" aria-hidden="true" tabindex="-1"></a>g_p <span class="op">=</span> [<span class="bu">dict</span>(x1<span class="op">=</span>p[<span class="st">'x1'</span>] <span class="op">+</span> w[<span class="dv">0</span>], x2<span class="op">=</span>p[<span class="st">'x2'</span>] <span class="op">+</span> w[<span class="dv">1</span>], tipo<span class="op">=</span><span class="st">'g(x)=1'</span>)</span>
<span id="cb14-377"><a href="#cb14-377" aria-hidden="true" tabindex="-1"></a>       <span class="cf">for</span> p <span class="kw">in</span> g_0]</span>
<span id="cb14-378"><a href="#cb14-378" aria-hidden="true" tabindex="-1"></a>g_n <span class="op">=</span> [<span class="bu">dict</span>(x1<span class="op">=</span>p[<span class="st">'x1'</span>] <span class="op">-</span> w[<span class="dv">0</span>], x2<span class="op">=</span>p[<span class="st">'x2'</span>] <span class="op">-</span> w[<span class="dv">1</span>], tipo<span class="op">=</span><span class="st">'g(x)=-1'</span>)</span>
<span id="cb14-379"><a href="#cb14-379" aria-hidden="true" tabindex="-1"></a>       <span class="cf">for</span> p <span class="kw">in</span> g_0]</span>
<span id="cb14-380"><a href="#cb14-380" aria-hidden="true" tabindex="-1"></a>df <span class="op">=</span> pd.DataFrame(g_0 <span class="op">+</span> g_p <span class="op">+</span> g_n <span class="op">+\</span></span>
<span id="cb14-381"><a href="#cb14-381" aria-hidden="true" tabindex="-1"></a>                  [<span class="bu">dict</span>(x1<span class="op">=</span>x, x2<span class="op">=</span>y, clase<span class="op">=</span><span class="st">'P'</span>) <span class="cf">for</span> x, y <span class="kw">in</span> X_1] <span class="op">+</span> <span class="op">\</span></span>
<span id="cb14-382"><a href="#cb14-382" aria-hidden="true" tabindex="-1"></a>                  [<span class="bu">dict</span>(x1<span class="op">=</span>x, x2<span class="op">=</span>y, clase<span class="op">=</span><span class="st">'N'</span>) <span class="cf">for</span> x, y <span class="kw">in</span> X_2]</span>
<span id="cb14-383"><a href="#cb14-383" aria-hidden="true" tabindex="-1"></a>                 )</span>
<span id="cb14-384"><a href="#cb14-384" aria-hidden="true" tabindex="-1"></a>ax <span class="op">=</span> sns.scatterplot(data<span class="op">=</span>df, x<span class="op">=</span><span class="st">'x1'</span>, y<span class="op">=</span><span class="st">'x2'</span>, hue<span class="op">=</span><span class="st">'clase'</span>, legend<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb14-385"><a href="#cb14-385" aria-hidden="true" tabindex="-1"></a>sns.lineplot(data<span class="op">=</span>df, x<span class="op">=</span><span class="st">'x1'</span>, y<span class="op">=</span><span class="st">'x2'</span>, ax<span class="op">=</span>ax,</span>
<span id="cb14-386"><a href="#cb14-386" aria-hidden="true" tabindex="-1"></a>             hue<span class="op">=</span><span class="st">'tipo'</span>, palette<span class="op">=</span>[<span class="st">'k'</span>] <span class="op">+</span> sns.color_palette()[<span class="dv">2</span>:<span class="dv">4</span>], legend<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb14-387"><a href="#cb14-387" aria-hidden="true" tabindex="-1"></a>_ <span class="op">=</span> ax.axis(<span class="st">'equal'</span>)</span>
<span id="cb14-388"><a href="#cb14-388" aria-hidden="true" tabindex="-1"></a><span class="in">```</span>  </span>
<span id="cb14-389"><a href="#cb14-389" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-390"><a href="#cb14-390" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-391"><a href="#cb14-391" aria-hidden="true" tabindex="-1"></a>Continuando con el problema de optimización, en las condiciones anteriores la función a optimizar es $\min \frac{1}{2} \mid\mid\mathbf w \mid\mid + C \sum_i^N \xi_i,$ utilizando multiplicadores de Lagrange queda como</span>
<span id="cb14-392"><a href="#cb14-392" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-393"><a href="#cb14-393" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb14-394"><a href="#cb14-394" aria-hidden="true" tabindex="-1"></a>f_p = \frac{1}{2}\mid\mid\mathbf w \mid\mid - \sum_i^N \alpha_i ((\mathbf w \cdot \mathbf x_i + w_0) y_i - 1 + \xi_i) - \sum_i^N \beta_i \xi_i.</span>
<span id="cb14-395"><a href="#cb14-395" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb14-396"><a href="#cb14-396" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-397"><a href="#cb14-397" aria-hidden="true" tabindex="-1"></a>Se observa que el parámetro $C$ controla la penalización que se hace a los elementos que se encuentran en el lado incorrecto del hiperplano o dentro del margen. La @fig-lineal-hip-c muestra el hiperplano generado utilizando $C=1$ y $C=0.01.$ Se observa como el elemento que está correctamente clasificado en $C=1$ pasa al lado incorrecto del hiperplano, ademas se ve como la función de decisión rota cuando el valor cambia. </span>
<span id="cb14-398"><a href="#cb14-398" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-399"><a href="#cb14-399" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-402"><a href="#cb14-402" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb14-403"><a href="#cb14-403" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-fold: true</span></span>
<span id="cb14-404"><a href="#cb14-404" aria-hidden="true" tabindex="-1"></a><span class="co">#| warning: false</span></span>
<span id="cb14-405"><a href="#cb14-405" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: Hiperplanos para diferentes valores de $C$. Se observa que en $C=0.01$ se clasifica incorrectamente un elemento positivo.</span></span>
<span id="cb14-406"><a href="#cb14-406" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-lineal-hip-c</span></span>
<span id="cb14-407"><a href="#cb14-407" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> k, (C, legend) <span class="kw">in</span> <span class="bu">enumerate</span>(<span class="bu">zip</span>([<span class="dv">1</span>, <span class="fl">0.01</span>], [<span class="va">False</span>, <span class="va">True</span>])):</span>
<span id="cb14-408"><a href="#cb14-408" aria-hidden="true" tabindex="-1"></a>     linear <span class="op">=</span> LinearSVC(dual<span class="op">=</span><span class="va">False</span>, C<span class="op">=</span>C).fit(T, y_t)</span>
<span id="cb14-409"><a href="#cb14-409" aria-hidden="true" tabindex="-1"></a>     w_1, w_2 <span class="op">=</span> linear.coef_[<span class="dv">0</span>]</span>
<span id="cb14-410"><a href="#cb14-410" aria-hidden="true" tabindex="-1"></a>     w_0 <span class="op">=</span> linear.intercept_[<span class="dv">0</span>]</span>
<span id="cb14-411"><a href="#cb14-411" aria-hidden="true" tabindex="-1"></a>     w <span class="op">=</span> np.array([w_1, w_2]) <span class="op">/</span> np.linalg.norm([w_1, w_2])</span>
<span id="cb14-412"><a href="#cb14-412" aria-hidden="true" tabindex="-1"></a>     g_0 <span class="op">=</span> [<span class="bu">dict</span>(x1<span class="op">=</span>x, x2<span class="op">=</span>y, tipo<span class="op">=</span><span class="st">'g(x)=0'</span>)</span>
<span id="cb14-413"><a href="#cb14-413" aria-hidden="true" tabindex="-1"></a>          <span class="cf">for</span> x, y <span class="kw">in</span> <span class="bu">zip</span>(T[:, <span class="dv">0</span>], (<span class="op">-</span>w_0 <span class="op">-</span> w_1 <span class="op">*</span> T[:, <span class="dv">0</span>]) <span class="op">/</span> w_2)]</span>
<span id="cb14-414"><a href="#cb14-414" aria-hidden="true" tabindex="-1"></a>     g_p <span class="op">=</span> [<span class="bu">dict</span>(x1<span class="op">=</span>p[<span class="st">'x1'</span>] <span class="op">+</span> w[<span class="dv">0</span>], x2<span class="op">=</span>p[<span class="st">'x2'</span>] <span class="op">+</span> w[<span class="dv">1</span>], tipo<span class="op">=</span><span class="st">'g(x)=1'</span>)</span>
<span id="cb14-415"><a href="#cb14-415" aria-hidden="true" tabindex="-1"></a>          <span class="cf">for</span> p <span class="kw">in</span> g_0]</span>
<span id="cb14-416"><a href="#cb14-416" aria-hidden="true" tabindex="-1"></a>     g_n <span class="op">=</span> [<span class="bu">dict</span>(x1<span class="op">=</span>p[<span class="st">'x1'</span>] <span class="op">-</span> w[<span class="dv">0</span>], x2<span class="op">=</span>p[<span class="st">'x2'</span>] <span class="op">-</span> w[<span class="dv">1</span>], tipo<span class="op">=</span><span class="st">'g(x)=-1'</span>)</span>
<span id="cb14-417"><a href="#cb14-417" aria-hidden="true" tabindex="-1"></a>          <span class="cf">for</span> p <span class="kw">in</span> g_0]</span>
<span id="cb14-418"><a href="#cb14-418" aria-hidden="true" tabindex="-1"></a>     df <span class="op">=</span> pd.DataFrame(g_0 <span class="op">+</span> g_p <span class="op">+</span> g_n <span class="op">+\</span></span>
<span id="cb14-419"><a href="#cb14-419" aria-hidden="true" tabindex="-1"></a>                    [<span class="bu">dict</span>(x1<span class="op">=</span>x, x2<span class="op">=</span>y, clase<span class="op">=</span><span class="st">'P'</span>) <span class="cf">for</span> x, y <span class="kw">in</span> X_1] <span class="op">+</span> <span class="op">\</span></span>
<span id="cb14-420"><a href="#cb14-420" aria-hidden="true" tabindex="-1"></a>                    [<span class="bu">dict</span>(x1<span class="op">=</span>x, x2<span class="op">=</span>y, clase<span class="op">=</span><span class="st">'N'</span>) <span class="cf">for</span> x, y <span class="kw">in</span> X_2]</span>
<span id="cb14-421"><a href="#cb14-421" aria-hidden="true" tabindex="-1"></a>                    )</span>
<span id="cb14-422"><a href="#cb14-422" aria-hidden="true" tabindex="-1"></a>     ax <span class="op">=</span> plt.subplot(<span class="dv">1</span>, <span class="dv">2</span>, k <span class="op">+</span> <span class="dv">1</span>)</span>
<span id="cb14-423"><a href="#cb14-423" aria-hidden="true" tabindex="-1"></a>     sns.scatterplot(data<span class="op">=</span>df, x<span class="op">=</span><span class="st">'x1'</span>, y<span class="op">=</span><span class="st">'x2'</span>, hue<span class="op">=</span><span class="st">'clase'</span>, legend<span class="op">=</span>legend, ax<span class="op">=</span>ax)</span>
<span id="cb14-424"><a href="#cb14-424" aria-hidden="true" tabindex="-1"></a>     sns.lineplot(data<span class="op">=</span>df, x<span class="op">=</span><span class="st">'x1'</span>, y<span class="op">=</span><span class="st">'x2'</span>, ax<span class="op">=</span>ax,</span>
<span id="cb14-425"><a href="#cb14-425" aria-hidden="true" tabindex="-1"></a>               hue<span class="op">=</span><span class="st">'tipo'</span>, palette<span class="op">=</span>[<span class="st">'k'</span>] <span class="op">+</span> sns.color_palette()[<span class="dv">2</span>:<span class="dv">4</span>], legend<span class="op">=</span>legend)</span>
<span id="cb14-426"><a href="#cb14-426" aria-hidden="true" tabindex="-1"></a>     ax.axis(<span class="st">'equal'</span>)</span>
<span id="cb14-427"><a href="#cb14-427" aria-hidden="true" tabindex="-1"></a>     ax.set_title(<span class="ss">f'C=</span><span class="sc">{</span>C<span class="sc">}</span><span class="ss">'</span>)</span>
<span id="cb14-428"><a href="#cb14-428" aria-hidden="true" tabindex="-1"></a>     <span class="cf">if</span> legend:</span>
<span id="cb14-429"><a href="#cb14-429" aria-hidden="true" tabindex="-1"></a>          sns.move_legend(ax, <span class="st">"upper left"</span>, bbox_to_anchor<span class="op">=</span>(<span class="dv">1</span>, <span class="fl">0.65</span>))</span>
<span id="cb14-430"><a href="#cb14-430" aria-hidden="true" tabindex="-1"></a>_ <span class="op">=</span> plt.tight_layout()</span>
<span id="cb14-431"><a href="#cb14-431" aria-hidden="true" tabindex="-1"></a><span class="in">```</span>  </span>
<span id="cb14-432"><a href="#cb14-432" aria-hidden="true" tabindex="-1"></a>&lt;/details&gt;</span>
<span id="cb14-433"><a href="#cb14-433" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-434"><a href="#cb14-434" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-435"><a href="#cb14-435" aria-hidden="true" tabindex="-1"></a>Este problema de optimización cumple con todas las características para poder encontrar su solución optimizando el problema dual. El problema dual corresponde a maximizar $f_p$ con respecto a $\alpha_i,$ sujeto a que las restricciones de que el gradiente de $f_p$ con respecto a $\mid\mid\mathbf w \mid\mid$, $w_0$ y $\xi_i$ sean cero. Utilizando estas características el problema dual corresponde a </span>
<span id="cb14-436"><a href="#cb14-436" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-437"><a href="#cb14-437" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-438"><a href="#cb14-438" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb14-439"><a href="#cb14-439" aria-hidden="true" tabindex="-1"></a>f_d = \sum_i^N \alpha_i - \frac{1}{2} \sum_i^N \sum_j^N \alpha_i \alpha_j y_i y_j \mathbf x_i \cdot \mathbf x_j,</span>
<span id="cb14-440"><a href="#cb14-440" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb14-441"><a href="#cb14-441" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-442"><a href="#cb14-442" aria-hidden="true" tabindex="-1"></a>sujeto a las restricciones $\sum_i^N \alpha_i y_i = 0$ y $0 \leq \alpha_i \leq C.$</span>
<span id="cb14-443"><a href="#cb14-443" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-444"><a href="#cb14-444" aria-hidden="true" tabindex="-1"></a>El problema de optimización dual tiene unas características que lo hacen deseable en ciertos casos, por ejemplo, el problema depende del número de ejemplos ($N$) en lugar de la dimensión. Entonces en problemas donde $d &gt; N$ es más conveniente utilizar el dual.</span>
<span id="cb14-445"><a href="#cb14-445" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-446"><a href="#cb14-446" aria-hidden="true" tabindex="-1"></a><span class="fu">### Kernel</span></span>
<span id="cb14-447"><a href="#cb14-447" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-448"><a href="#cb14-448" aria-hidden="true" tabindex="-1"></a>La otra característica del problema dual es que permite visualizar lo siguiente. Suponiendo que se usa una función $\phi: \mathbb R^d \leftarrow \mathbf R^{\hat d},$ de tal manera, que en el espacio $\phi$ se puede encontrar un hiperplano que separa las clases. Incorporando la función $\phi$ produce la siguiente función a optimizar </span>
<span id="cb14-449"><a href="#cb14-449" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-450"><a href="#cb14-450" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb14-451"><a href="#cb14-451" aria-hidden="true" tabindex="-1"></a>f_d = \sum_i^N \alpha_i - \frac{1}{2} \sum_i^N \sum_j^N \alpha_i \alpha_j y_i y_j \phi(\mathbf x_i) \cdot \phi(\mathbf x_j),</span>
<span id="cb14-452"><a href="#cb14-452" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb14-453"><a href="#cb14-453" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-454"><a href="#cb14-454" aria-hidden="true" tabindex="-1"></a>donde primero se transforman todos los datos al espacio generado por $\phi$ y después se calcula el producto punto. El producto punto se puede cambiar por una función **Kernel**, i.e., $K(\mathbf x_i, \mathbf x_j) = \phi(\mathbf x_i) \cdot \phi(\mathbf x_j)$ lo cual hace que innecesaria la transformación al espacio $\phi.$ Utilizando la función de kernel, el problema de optimización dual queda como:</span>
<span id="cb14-455"><a href="#cb14-455" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-456"><a href="#cb14-456" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb14-457"><a href="#cb14-457" aria-hidden="true" tabindex="-1"></a>f_d = \sum_i^N \alpha_i - \frac{1}{2} \sum_i^N \sum_j^N \alpha_i \alpha_j y_i y_j K(\mathbf x_i, \mathbf x_j).</span>
<span id="cb14-458"><a href="#cb14-458" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb14-459"><a href="#cb14-459" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-460"><a href="#cb14-460" aria-hidden="true" tabindex="-1"></a>La función discriminante está dada por $g(\mathbf x) = \sum_i^N \alpha_i y_i K(\mathbf x_i, \mathbf x),$ donde aquellos elementos donde $\alpha \neq 0$ se les conoce como los vectores de soporte. Estos elementos son los que se encuentran en el margen, dentro del margen y en el lado incorrecto de la función discriminante.</span>
<span id="cb14-461"><a href="#cb14-461" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-462"><a href="#cb14-462" aria-hidden="true" tabindex="-1"></a>La @fig-lineal-kernel muestra los datos del iris (proyectados con Análisis de Componentes Principales @sec-pca), las clases se encuentran en color azul, naranja y verde; en color rojo se muestran los vectores de soporte. La figura derecha muestra en color negro aquellos vectores de soporte que se encuentran en el lado incorrecto del hiperplano. Por otro lado se puede observar como los vectores de soporte separan las clases, del lado izquierdo se encuentran todos los elementos de la clase $0$, después se observan las clases $1$ y del lado derecho las clases $2$. Los vectores de soporte están en la frontera de las clases y los errores se encuentran entre las clases $1$ y $2$ que corresponden a las que no son linealmente separables. </span>
<span id="cb14-463"><a href="#cb14-463" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-466"><a href="#cb14-466" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb14-467"><a href="#cb14-467" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-fold: true</span></span>
<span id="cb14-468"><a href="#cb14-468" aria-hidden="true" tabindex="-1"></a><span class="co">#| warning: false</span></span>
<span id="cb14-469"><a href="#cb14-469" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: Visualización de los vectores de soporte usando PCA.</span></span>
<span id="cb14-470"><a href="#cb14-470" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-lineal-kernel</span></span>
<span id="cb14-471"><a href="#cb14-471" aria-hidden="true" tabindex="-1"></a>X, y <span class="op">=</span> load_iris(return_X_y<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb14-472"><a href="#cb14-472" aria-hidden="true" tabindex="-1"></a>linear <span class="op">=</span> SVC(kernel<span class="op">=</span><span class="st">'poly'</span>, degree<span class="op">=</span><span class="dv">2</span>, C<span class="op">=</span><span class="dv">10</span>).fit(X, y)</span>
<span id="cb14-473"><a href="#cb14-473" aria-hidden="true" tabindex="-1"></a>hy <span class="op">=</span> linear.predict(X)</span>
<span id="cb14-474"><a href="#cb14-474" aria-hidden="true" tabindex="-1"></a>D <span class="op">=</span> PCA(n_components<span class="op">=</span><span class="dv">2</span>).fit_transform(X)</span>
<span id="cb14-475"><a href="#cb14-475" aria-hidden="true" tabindex="-1"></a>mask <span class="op">=</span> np.zeros(D.shape[<span class="dv">0</span>])</span>
<span id="cb14-476"><a href="#cb14-476" aria-hidden="true" tabindex="-1"></a>mask[linear.support_] <span class="op">=</span> <span class="va">True</span></span>
<span id="cb14-477"><a href="#cb14-477" aria-hidden="true" tabindex="-1"></a>s <span class="op">=</span> <span class="st">'S'</span></span>
<span id="cb14-478"><a href="#cb14-478" aria-hidden="true" tabindex="-1"></a>df <span class="op">=</span> pd.DataFrame([<span class="bu">dict</span>(x1<span class="op">=</span>x1, x2<span class="op">=</span>x2, tipo<span class="op">=</span><span class="ss">f'</span><span class="sc">{</span>l <span class="cf">if</span> <span class="kw">not</span> c <span class="cf">else</span> s<span class="sc">}</span><span class="ss">'</span>, error<span class="op">=</span>err)</span>
<span id="cb14-479"><a href="#cb14-479" aria-hidden="true" tabindex="-1"></a>                   <span class="cf">for</span> (x1, x2), c, l, err <span class="kw">in</span> <span class="bu">zip</span>(D, mask, y, y <span class="op">!=</span> hy)])</span>
<span id="cb14-480"><a href="#cb14-480" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> k, legend <span class="kw">in</span> <span class="bu">enumerate</span>([<span class="va">False</span>, <span class="va">True</span>]):     </span>
<span id="cb14-481"><a href="#cb14-481" aria-hidden="true" tabindex="-1"></a>     <span class="cf">if</span> legend:</span>
<span id="cb14-482"><a href="#cb14-482" aria-hidden="true" tabindex="-1"></a>          df.loc[df.error, <span class="st">'tipo'</span>] <span class="op">=</span> <span class="st">'X'</span></span>
<span id="cb14-483"><a href="#cb14-483" aria-hidden="true" tabindex="-1"></a>     df.sort_values(by<span class="op">=</span><span class="st">'tipo'</span>, inplace<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb14-484"><a href="#cb14-484" aria-hidden="true" tabindex="-1"></a>     ax <span class="op">=</span> plt.subplot(<span class="dv">1</span>, <span class="dv">2</span>, k <span class="op">+</span> <span class="dv">1</span>)          </span>
<span id="cb14-485"><a href="#cb14-485" aria-hidden="true" tabindex="-1"></a>     sns.scatterplot(data<span class="op">=</span>df, x<span class="op">=</span><span class="st">'x1'</span>, y<span class="op">=</span><span class="st">'x2'</span>,</span>
<span id="cb14-486"><a href="#cb14-486" aria-hidden="true" tabindex="-1"></a>                     palette<span class="op">=</span>sns.color_palette()[:<span class="dv">4</span>] <span class="op">+</span> [<span class="st">'k'</span>],</span>
<span id="cb14-487"><a href="#cb14-487" aria-hidden="true" tabindex="-1"></a>                     hue<span class="op">=</span><span class="st">'tipo'</span>, legend<span class="op">=</span>legend, ax<span class="op">=</span>ax)</span>
<span id="cb14-488"><a href="#cb14-488" aria-hidden="true" tabindex="-1"></a>     <span class="cf">if</span> legend:</span>
<span id="cb14-489"><a href="#cb14-489" aria-hidden="true" tabindex="-1"></a>          sns.move_legend(ax, <span class="st">"upper left"</span>, bbox_to_anchor<span class="op">=</span>(<span class="dv">1</span>, <span class="fl">0.65</span>))</span>
<span id="cb14-490"><a href="#cb14-490" aria-hidden="true" tabindex="-1"></a>_ <span class="op">=</span> plt.tight_layout()</span>
<span id="cb14-491"><a href="#cb14-491" aria-hidden="true" tabindex="-1"></a><span class="in">```</span>  </span>
<span id="cb14-492"><a href="#cb14-492" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-493"><a href="#cb14-493" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-494"><a href="#cb14-494" aria-hidden="true" tabindex="-1"></a><span class="fu">## Regresión Logística {#sec-regresion-logistica}</span></span>
<span id="cb14-495"><a href="#cb14-495" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-496"><a href="#cb14-496" aria-hidden="true" tabindex="-1"></a>En clasificación binaria (@sec-binaria) se describió que la función discriminante se puede definir como la resta, i.e., $g_1(\mathbf x) - g_2(\mathbf x);$ equivalentemente se pudo haber seleccionado la división ($\frac{g_1(\mathbf x)}{g_2(\mathbf x)}$) para generar la función discriminante o el logaritmo de la división, i.e., $\log \frac{g_1(\mathbf x)}{g_2(\mathbf x)}.$ Esta última ecuación en el caso de $g_i(\mathbf x)=\mathbb P(\mathcal Y=i \mid \mathcal X=\mathbf x)$ corresponde a la función $\textsf{logit}$, tal y como se muestra a continuación.</span>
<span id="cb14-497"><a href="#cb14-497" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-498"><a href="#cb14-498" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb14-499"><a href="#cb14-499" aria-hidden="true" tabindex="-1"></a>\begin{split}</span>
<span id="cb14-500"><a href="#cb14-500" aria-hidden="true" tabindex="-1"></a>\log \frac{\mathbb P(\mathcal Y=1 \mid \mathcal X=\mathbf x)}{\mathbb P(\mathcal Y=2 \mid \mathcal X=\mathbf x)} &amp;= \frac{\mathbb P(\mathcal Y=1 \mid \mathcal X=\mathbf x)}{1 - \mathbb P(\mathcal Y=1 \mid \mathcal X=\mathbf x)}<span class="sc">\\</span></span>
<span id="cb14-501"><a href="#cb14-501" aria-hidden="true" tabindex="-1"></a>&amp;= \textsf{logit}(\mathbb P(\mathcal Y=1 \mid \mathcal X=\mathbf x)),</span>
<span id="cb14-502"><a href="#cb14-502" aria-hidden="true" tabindex="-1"></a>\end{split}</span>
<span id="cb14-503"><a href="#cb14-503" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb14-504"><a href="#cb14-504" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-505"><a href="#cb14-505" aria-hidden="true" tabindex="-1"></a>donde la inversa del $\textsf{logit}$ es la función sigmoide, $\textsf{sigmoid}(x) = \frac{1}{1 + \exp(-x)}$, es decir $\textsf{sigmoid}(\textsf{logit}(y)) = y$. </span>
<span id="cb14-506"><a href="#cb14-506" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-507"><a href="#cb14-507" aria-hidden="true" tabindex="-1"></a>Trabajando un poco con el $\textsf{logit}$ se puede observar que para el caso de dos clases está función queda como </span>
<span id="cb14-508"><a href="#cb14-508" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-509"><a href="#cb14-509" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb14-510"><a href="#cb14-510" aria-hidden="true" tabindex="-1"></a>\begin{split}</span>
<span id="cb14-511"><a href="#cb14-511" aria-hidden="true" tabindex="-1"></a>\textsf{logit}(\mathbb P(\mathcal Y=1 \mid \mathcal X=\mathbf x)) &amp;= \log\frac{\mathbb P(\mathcal Y=1 \mid \mathcal X=\mathbf x)}{\mathbb P(\mathcal Y=2 \mid \mathcal X=\mathbf x)}<span class="sc">\\</span></span>
<span id="cb14-512"><a href="#cb14-512" aria-hidden="true" tabindex="-1"></a>&amp;= \log\frac{\mathbb P(\mathcal X=\mathbf x \mid \mathcal Y=1)\mathbb P(\mathcal Y=1)}{\mathbb P(\mathcal X=\mathbf x \mid \mathcal Y=2)\mathbb P(\mathcal Y=2)}<span class="sc">\\</span></span>
<span id="cb14-513"><a href="#cb14-513" aria-hidden="true" tabindex="-1"></a>&amp;= \log\frac{\mathbb P(\mathcal X=\mathbf x \mid \mathcal Y=1)}{\mathbb P(\mathcal X=\mathbf x \mid \mathcal Y=2)} + \log\frac{\mathbb P(\mathcal Y=1)}{\mathbb P(\mathcal Y=2)}</span>
<span id="cb14-514"><a href="#cb14-514" aria-hidden="true" tabindex="-1"></a>\end{split},</span>
<span id="cb14-515"><a href="#cb14-515" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb14-516"><a href="#cb14-516" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-517"><a href="#cb14-517" aria-hidden="true" tabindex="-1"></a>asumiendo que la matriz de covarianza ($\Sigma$) es compartida entre las dos clases la ecuación anterior quedaría como:</span>
<span id="cb14-518"><a href="#cb14-518" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-519"><a href="#cb14-519" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb14-520"><a href="#cb14-520" aria-hidden="true" tabindex="-1"></a>\begin{split}</span>
<span id="cb14-521"><a href="#cb14-521" aria-hidden="true" tabindex="-1"></a>\textsf{logit}(\mathbb P(\mathcal Y=1 \mid \mathcal X=\mathbf x)) &amp;= \log \frac{(2\pi)^{-\frac{d}{2}} \mid\Sigma \mid^{-\frac{1}{2}}\exp{(-\frac{1}{2}(\mathbf x - \mathbf \mu_1)^\intercal \Sigma^{-1}(\mathbf x - \mathbf \mu_1))}}{(2\pi)^{-\frac{d}{2}} \mid\Sigma \mid^{-\frac{1}{2}}\exp{(-\frac{1}{2}(\mathbf x - \mathbf \mu_2)^\intercal \Sigma^{-1}(\mathbf x - \mathbf \mu_2))}}<span class="sc">\\</span> </span>
<span id="cb14-522"><a href="#cb14-522" aria-hidden="true" tabindex="-1"></a>&amp;+ \log\frac{\mathbb P(\mathcal Y=1)}{\mathbb P(\mathcal Y=2)}<span class="sc">\\</span></span>
<span id="cb14-523"><a href="#cb14-523" aria-hidden="true" tabindex="-1"></a>&amp;= \mathbf w \cdot \mathbf x + w_0</span>
<span id="cb14-524"><a href="#cb14-524" aria-hidden="true" tabindex="-1"></a>\end{split}</span>
<span id="cb14-525"><a href="#cb14-525" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb14-526"><a href="#cb14-526" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-527"><a href="#cb14-527" aria-hidden="true" tabindex="-1"></a>donde $\mathbf w=\Sigma^{-1}(\mathbf \mu_1 - \mathbf \mu_2)$ y $w_0=-\frac{1}{2}(\mathbf \mu_1 + \mathbf \mu_2)^\intercal \Sigma^{-1}(\mathbf \mu_1 + \mathbf \mu_2)+ \log\frac{\mathbb P(\mathcal Y=1)}{\mathbb P(\mathcal Y=2)}.$</span>
<span id="cb14-528"><a href="#cb14-528" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-529"><a href="#cb14-529" aria-hidden="true" tabindex="-1"></a>En el caso de regresión logística, se asume que $\textsf{logit}(\mathbb P(\mathcal Y=1 \mid \mathcal X=\mathbf x)) = \mathbf w \cdot \mathbf x + w_0$ y se realiza ninguna asunción sobre la distribución que tienen los datos. Equivalentemente, se puede asumir que $\log\frac{\mathbb P(\mathcal Y=1 \mid \mathcal X=\mathbf x)}{\mathbb P(\mathcal Y=2 \mid \mathcal X=\mathbf x)} = \mathbf w \cdot \mathbf x + w_0^0,$ realizando algunas substituciones se puede ver que $w_0 = w_0^0 + \log\frac{\mathbb P(\mathcal Y=1)}{\mathbb P(\mathcal Y=2)}.$</span>
<span id="cb14-530"><a href="#cb14-530" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-531"><a href="#cb14-531" aria-hidden="true" tabindex="-1"></a><span class="fu">### Optimización</span></span>
<span id="cb14-532"><a href="#cb14-532" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-533"><a href="#cb14-533" aria-hidden="true" tabindex="-1"></a>Se puede asumir que $\mathcal Y \mid \mathcal X$ sigue una distribución Bernoulli en el caso de dos clases, entonces el logaritmo de la verosimilitud (@sec-verosimilitud) quedaría como:</span>
<span id="cb14-534"><a href="#cb14-534" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-535"><a href="#cb14-535" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb14-536"><a href="#cb14-536" aria-hidden="true" tabindex="-1"></a>\ell(\mathbf w, w_0 \mid \mathcal D) = \prod_{(\mathbf x, y) \in \mathcal D} (C(\mathbf x))^{y} (1 -  C(\mathbf x)))^{1-y},</span>
<span id="cb14-537"><a href="#cb14-537" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb14-538"><a href="#cb14-538" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-539"><a href="#cb14-539" aria-hidden="true" tabindex="-1"></a>donde $C(\mathbf x)$ es la clase estimada por el clasificador. </span>
<span id="cb14-540"><a href="#cb14-540" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-541"><a href="#cb14-541" aria-hidden="true" tabindex="-1"></a>Siempre que se tiene que obtener el máximo de una función esta se puede transformar a un problema de minimización, por ejemplo, para el caso anterior definiendo como $E = -\log \ell$, utilizando esta transformación el problema sería minimizar la siguiente función:</span>
<span id="cb14-542"><a href="#cb14-542" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-543"><a href="#cb14-543" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb14-544"><a href="#cb14-544" aria-hidden="true" tabindex="-1"></a>E(\mathbf w, w_0 \mid \mathcal D) = - \sum_{(\mathbf x, y) \in \mathcal D} y \log C(x) + (1-y) \log (1 -  C(x)).</span>
<span id="cb14-545"><a href="#cb14-545" aria-hidden="true" tabindex="-1"></a>$${#eq-lineal-entropia-cruzada}</span>
<span id="cb14-546"><a href="#cb14-546" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-547"><a href="#cb14-547" aria-hidden="true" tabindex="-1"></a>Es importante notar que la ecuación anterior corresponde a Entropía cruzada (@sec-entropia-cruzada), donde $y=\mathbb P(\mathcal Y=y \mid \mathcal X=\mathbf x))$ y $C(\mathbf x)=\mathbb{\hat P}(\mathcal Y=y \mid \mathcal X=\mathbf x)$ y los términos $1-y$ y $1-C(\mathbf x)$ corresponde a la otra clase. </span>
<span id="cb14-548"><a href="#cb14-548" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-549"><a href="#cb14-549" aria-hidden="true" tabindex="-1"></a>Otra característica de $E(\mathbf w, w_0 \mid \mathcal D)$ es que no tiene una solución cerrada y por lo tanto es necesario utilizar un método de optimización (@sec-optimizacion) para encontrar los parámetros $\mathbf w$ y $w_0$. </span>
<span id="cb14-550"><a href="#cb14-550" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-551"><a href="#cb14-551" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-552"><a href="#cb14-552" aria-hidden="true" tabindex="-1"></a><span class="fu">## Comparación</span></span>
<span id="cb14-553"><a href="#cb14-553" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-554"><a href="#cb14-554" aria-hidden="true" tabindex="-1"></a>Es momento de comparar el comportamiento de los dos métodos de discriminantes lineales visto en la unidad, estos son, Máquinas de Soporte Vectorial (MSV) y Regresión Logística (RL). La @fig-lineal-comparacion muestra el hiperplano generado por MSV y RL, además se puede observar los valores de los pesos $\mathbf w$ para cada uno de los algoritmos. </span>
<span id="cb14-555"><a href="#cb14-555" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-558"><a href="#cb14-558" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb14-559"><a href="#cb14-559" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-fold: true</span></span>
<span id="cb14-560"><a href="#cb14-560" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: Comparación de dos métodos lineales</span></span>
<span id="cb14-561"><a href="#cb14-561" aria-hidden="true" tabindex="-1"></a><span class="co">#| warning: false</span></span>
<span id="cb14-562"><a href="#cb14-562" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-lineal-comparacion</span></span>
<span id="cb14-563"><a href="#cb14-563" aria-hidden="true" tabindex="-1"></a>svm <span class="op">=</span> LinearSVC(dual<span class="op">=</span><span class="va">False</span>).fit(T, y_t)</span>
<span id="cb14-564"><a href="#cb14-564" aria-hidden="true" tabindex="-1"></a>lr <span class="op">=</span> LogisticRegression().fit(T, y_t)</span>
<span id="cb14-565"><a href="#cb14-565" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-566"><a href="#cb14-566" aria-hidden="true" tabindex="-1"></a>g <span class="op">=</span> []</span>
<span id="cb14-567"><a href="#cb14-567" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> model, tipo <span class="kw">in</span> <span class="bu">zip</span>([svm, lr], [<span class="st">'MSV'</span>, <span class="st">'RL'</span>]):</span>
<span id="cb14-568"><a href="#cb14-568" aria-hidden="true" tabindex="-1"></a>     w_1, w_2 <span class="op">=</span> model.coef_[<span class="dv">0</span>]</span>
<span id="cb14-569"><a href="#cb14-569" aria-hidden="true" tabindex="-1"></a>     w_0 <span class="op">=</span> model.intercept_[<span class="dv">0</span>]</span>
<span id="cb14-570"><a href="#cb14-570" aria-hidden="true" tabindex="-1"></a>     g <span class="op">+=</span> [<span class="bu">dict</span>(x1<span class="op">=</span>x, x2<span class="op">=</span>y, tipo<span class="op">=</span>tipo)</span>
<span id="cb14-571"><a href="#cb14-571" aria-hidden="true" tabindex="-1"></a>           <span class="cf">for</span> x, y <span class="kw">in</span> <span class="bu">zip</span>(T[:, <span class="dv">0</span>], (<span class="op">-</span>w_0 <span class="op">-</span> w_1 <span class="op">*</span> T[:, <span class="dv">0</span>]) <span class="op">/</span> w_2)]</span>
<span id="cb14-572"><a href="#cb14-572" aria-hidden="true" tabindex="-1"></a>     g.append(<span class="bu">dict</span>(x1<span class="op">=</span>w_1, x2<span class="op">=</span>w_2, clase<span class="op">=</span>tipo))</span>
<span id="cb14-573"><a href="#cb14-573" aria-hidden="true" tabindex="-1"></a>df <span class="op">=</span> pd.DataFrame(g <span class="op">+</span> <span class="op">\</span></span>
<span id="cb14-574"><a href="#cb14-574" aria-hidden="true" tabindex="-1"></a>                  [<span class="bu">dict</span>(x1<span class="op">=</span>x, x2<span class="op">=</span>y, clase<span class="op">=</span><span class="st">'P'</span>) <span class="cf">for</span> x, y <span class="kw">in</span> X_1] <span class="op">+</span> <span class="op">\</span></span>
<span id="cb14-575"><a href="#cb14-575" aria-hidden="true" tabindex="-1"></a>                  [<span class="bu">dict</span>(x1<span class="op">=</span>x, x2<span class="op">=</span>y, clase<span class="op">=</span><span class="st">'N'</span>) <span class="cf">for</span> x, y <span class="kw">in</span> X_2]</span>
<span id="cb14-576"><a href="#cb14-576" aria-hidden="true" tabindex="-1"></a>                 )</span>
<span id="cb14-577"><a href="#cb14-577" aria-hidden="true" tabindex="-1"></a>ax <span class="op">=</span> sns.scatterplot(data<span class="op">=</span>df, x<span class="op">=</span><span class="st">'x1'</span>, y<span class="op">=</span><span class="st">'x2'</span>, hue<span class="op">=</span><span class="st">'clase'</span>, legend<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb14-578"><a href="#cb14-578" aria-hidden="true" tabindex="-1"></a>sns.lineplot(data<span class="op">=</span>df, x<span class="op">=</span><span class="st">'x1'</span>, y<span class="op">=</span><span class="st">'x2'</span>, </span>
<span id="cb14-579"><a href="#cb14-579" aria-hidden="true" tabindex="-1"></a>             ax<span class="op">=</span>ax, hue<span class="op">=</span><span class="st">'tipo'</span>, </span>
<span id="cb14-580"><a href="#cb14-580" aria-hidden="true" tabindex="-1"></a>             palette<span class="op">=</span>sns.color_palette()[:<span class="dv">2</span>], </span>
<span id="cb14-581"><a href="#cb14-581" aria-hidden="true" tabindex="-1"></a>             legend<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb14-582"><a href="#cb14-582" aria-hidden="true" tabindex="-1"></a>_ <span class="op">=</span> ax.axis(<span class="st">'equal'</span>)</span>
<span id="cb14-583"><a href="#cb14-583" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb14-584"><a href="#cb14-584" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-585"><a href="#cb14-585" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-586"><a href="#cb14-586" aria-hidden="true" tabindex="-1"></a>Complementando la comparación anterior con los datos del iris que se pueden obtener con las siguientes dos instrucciones. </span>
<span id="cb14-587"><a href="#cb14-587" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-590"><a href="#cb14-590" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb14-591"><a href="#cb14-591" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: true</span></span>
<span id="cb14-592"><a href="#cb14-592" aria-hidden="true" tabindex="-1"></a>D, y <span class="op">=</span> load_iris(return_X_y<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb14-593"><a href="#cb14-593" aria-hidden="true" tabindex="-1"></a>T, G, y_t, y_g <span class="op">=</span> train_test_split(D, y,</span>
<span id="cb14-594"><a href="#cb14-594" aria-hidden="true" tabindex="-1"></a>                                  test_size<span class="op">=</span><span class="fl">0.4</span>,</span>
<span id="cb14-595"><a href="#cb14-595" aria-hidden="true" tabindex="-1"></a>                                  random_state<span class="op">=</span><span class="dv">3</span>)</span>
<span id="cb14-596"><a href="#cb14-596" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb14-597"><a href="#cb14-597" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-598"><a href="#cb14-598" aria-hidden="true" tabindex="-1"></a>Los clasificadores a comparar son una máquina de soporte vectorial lineal, una máquina de soporte vectorial usando un kernel polinomial de grado $1$ y una regresión logística, tal y como se muestra en el siguiente código. </span>
<span id="cb14-599"><a href="#cb14-599" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-602"><a href="#cb14-602" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb14-603"><a href="#cb14-603" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: true</span></span>
<span id="cb14-604"><a href="#cb14-604" aria-hidden="true" tabindex="-1"></a><span class="co">#| warning: false</span></span>
<span id="cb14-605"><a href="#cb14-605" aria-hidden="true" tabindex="-1"></a>svm <span class="op">=</span> LinearSVC(dual<span class="op">=</span><span class="va">False</span>).fit(T, y_t)</span>
<span id="cb14-606"><a href="#cb14-606" aria-hidden="true" tabindex="-1"></a>svm_k <span class="op">=</span> SVC(kernel<span class="op">=</span><span class="st">'poly'</span>, degree<span class="op">=</span><span class="dv">1</span>).fit(T, y_t)</span>
<span id="cb14-607"><a href="#cb14-607" aria-hidden="true" tabindex="-1"></a>lr <span class="op">=</span> LogisticRegression().fit(T, y_t)</span>
<span id="cb14-608"><a href="#cb14-608" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb14-609"><a href="#cb14-609" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-610"><a href="#cb14-610" aria-hidden="true" tabindex="-1"></a>La @tbl-lineal-rendimiento muestra el rendimiento (en medidas macro) de estos algoritmos en el conjunto de prueba, se puede observar que estos algoritmos tienen rendimientos diferentes para esta selección del conjunto de entrenamiento y prueba. También en esta ocasión la regresión lineal es la que presenta el mejor rendimiento. Aunque es importante aclarar que este rendimiento es resultado del proceso aleatorio de selección del conjunto de entrenamiento y prueba.</span>
<span id="cb14-611"><a href="#cb14-611" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-612"><a href="#cb14-612" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-615"><a href="#cb14-615" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb14-616"><a href="#cb14-616" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: tbl-lineal-rendimiento</span></span>
<span id="cb14-617"><a href="#cb14-617" aria-hidden="true" tabindex="-1"></a><span class="co">#| tbl-cap: Rendimiento de clasificadores lineales</span></span>
<span id="cb14-618"><a href="#cb14-618" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: false</span></span>
<span id="cb14-619"><a href="#cb14-619" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-620"><a href="#cb14-620" aria-hidden="true" tabindex="-1"></a>headers <span class="op">=</span> <span class="st">'Clasificador | Precisión |&nbsp;Recall | $F_1$   |'</span></span>
<span id="cb14-621"><a href="#cb14-621" aria-hidden="true" tabindex="-1"></a>linea   <span class="op">=</span> <span class="st">'-------------|-----------|--------|---------|'</span></span>
<span id="cb14-622"><a href="#cb14-622" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-623"><a href="#cb14-623" aria-hidden="true" tabindex="-1"></a>hy_svm <span class="op">=</span> svm.predict(G)</span>
<span id="cb14-624"><a href="#cb14-624" aria-hidden="true" tabindex="-1"></a>hy_k <span class="op">=</span> svm_k.predict(G)</span>
<span id="cb14-625"><a href="#cb14-625" aria-hidden="true" tabindex="-1"></a>hy_lr <span class="op">=</span> lr.predict(G)</span>
<span id="cb14-626"><a href="#cb14-626" aria-hidden="true" tabindex="-1"></a>cdn <span class="op">=</span> <span class="st">''</span></span>
<span id="cb14-627"><a href="#cb14-627" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> hyx, system <span class="kw">in</span> <span class="bu">zip</span>([hy_svm, hy_k, hy_lr],</span>
<span id="cb14-628"><a href="#cb14-628" aria-hidden="true" tabindex="-1"></a>                       [<span class="st">'MSV - Lineal'</span>, <span class="st">'MSV - Kernel'</span>, <span class="st">'RL'</span>]):</span>
<span id="cb14-629"><a href="#cb14-629" aria-hidden="true" tabindex="-1"></a>  perfs <span class="op">=</span> [<span class="ss">f'$</span><span class="sc">{</span>func(y_g, hyx, average<span class="op">=</span><span class="st">"macro"</span>)<span class="sc">:0.4f}</span><span class="ss">$'</span></span>
<span id="cb14-630"><a href="#cb14-630" aria-hidden="true" tabindex="-1"></a>           <span class="cf">for</span> func <span class="kw">in</span> [precision_score, recall_score, f1_score]]</span>
<span id="cb14-631"><a href="#cb14-631" aria-hidden="true" tabindex="-1"></a>  cdn <span class="op">+=</span> <span class="ss">f'</span><span class="sc">{</span>system<span class="sc">}</span><span class="ss">|'</span> <span class="op">+</span> <span class="st">'|'</span>.join(perfs) <span class="op">+</span> <span class="st">'|</span><span class="ch">\n</span><span class="st">'</span></span>
<span id="cb14-632"><a href="#cb14-632" aria-hidden="true" tabindex="-1"></a>Markdown(<span class="ss">f'</span><span class="sc">{</span>headers<span class="sc">}</span><span class="ch">\n</span><span class="sc">{</span>linea<span class="sc">}</span><span class="ch">\n</span><span class="sc">{</span>cdn<span class="sc">}</span><span class="ss">'</span>)  </span>
<span id="cb14-633"><a href="#cb14-633" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
</code><button title="Copiar al portapapeles" class="code-copy-button" data-in-quarto-modal=""><i class="bi"></i></button></pre></div>
</div></div></div></div></div>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
      &nbsp;
    </div>   
    <div class="nav-footer-center">
      &nbsp;
    </div>
    <div class="nav-footer-right">
<p><a href="http://creativecommons.org/licenses/by-sa/4.0/"><img src="https://i.creativecommons.org/l/by-sa/4.0/80x15.png" class="img-fluid"></a> <br> Esta obra está bajo una <a href="http://creativecommons.org/licenses/by-sa/4.0/">Licencia Creative Commons Atribución-CompartirIgual 4.0 Internacional</a></p>
</div>
  </div>
</footer>




</body></html>