<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="es" xml:lang="es"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.6.42">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>10&nbsp; Optimización – Aprendizaje Computacional</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<link href="../capitulos/11RedesNeuronales.html" rel="next">
<link href="../capitulos/09Lineal.html" rel="prev">
<script src="../site_libs/quarto-html/quarto.js"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting-2f5df379a58b258e96c21c0638c20c03.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap-a34d670291f06f286357e447776a572a.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "Sin resultados",
    "search-matching-documents-text": "documentos encontrados",
    "search-copy-link-title": "Copiar el enlace en la búsqueda",
    "search-hide-matches-text": "Ocultar resultados adicionales",
    "search-more-match-text": "resultado adicional en este documento",
    "search-more-matches-text": "resultados adicionales en este documento",
    "search-clear-button-title": "Borrar",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancelar",
    "search-submit-button-title": "Enviar",
    "search-label": "Buscar"
  }
}</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="nav-sidebar floating">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Alternar barra lateral" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../capitulos/10Optimizacion.html"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Optimización</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Alternar barra lateral" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Buscar" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="../">Aprendizaje Computacional</a> 
        <div class="sidebar-tools-main">
    <a href="https://github.com/INGEOTEC/AprendizajeComputacional" title="Ejecutar el código" class="quarto-navigation-tool px-1" aria-label="Ejecutar el código"><i class="bi bi-github"></i></a>
    <a href="../Aprendizaje-Computacional.pdf" title="Descargar PDF" class="quarto-navigation-tool px-1" aria-label="Descargar PDF"><i class="bi bi-file-pdf"></i></a>
</div>
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Buscar"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Prefacio</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../capitulos/01Introduccion.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Introducción</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../capitulos/02Teoria_Decision.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Teoría de Decisión Bayesiana</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../capitulos/03Parametricos.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Métodos Paramétricos</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../capitulos/04Rendimiento.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Rendimiento</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../capitulos/05ReduccionDim.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Reducción de Dimensión</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../capitulos/06Agrupamiento.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Agrupamiento</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../capitulos/07NoParametricos.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Métodos No Paramétricos</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../capitulos/08Arboles.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Árboles de Decisión</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../capitulos/09Lineal.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Discriminantes Lineales</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../capitulos/10Optimizacion.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Optimización</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../capitulos/11RedesNeuronales.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Redes Neuronales</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../capitulos/12Ensambles.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">Ensambles</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../capitulos/13Comparacion.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">Comparación de Algoritmos</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../capitulos/17Referencias.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Referencias</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true">
 <span class="menu-text">Apéndices</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true" aria-label="Alternar sección">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../capitulos/14Estadistica.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">A</span>&nbsp; <span class="chapter-title">Estadística</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../capitulos/16ConjuntosDatos.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">B</span>&nbsp; <span class="chapter-title">Conjunto de Datos</span></span></a>
  </div>
</li>
      </ul>
  </li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Tabla de contenidos</h2>
   
  <ul>
  <li><a href="#paquetes-usados" id="toc-paquetes-usados" class="nav-link active" data-scroll-target="#paquetes-usados"><span class="header-section-number">10.1</span> Paquetes usados</a></li>
  <li><a href="#sec-intro-10" id="toc-sec-intro-10" class="nav-link" data-scroll-target="#sec-intro-10"><span class="header-section-number">10.2</span> Introducción</a></li>
  <li><a href="#sec-descenso-gradiente" id="toc-sec-descenso-gradiente" class="nav-link" data-scroll-target="#sec-descenso-gradiente"><span class="header-section-number">10.3</span> Descenso por Gradiente</a>
  <ul class="collapse">
  <li><a href="#sec-regresion-lineal-manual" id="toc-sec-regresion-lineal-manual" class="nav-link" data-scroll-target="#sec-regresion-lineal-manual"><span class="header-section-number">10.3.1</span> Ejemplo - Regresión Lineal</a></li>
  </ul></li>
  <li><a href="#sec-diferenciacion-automatica" id="toc-sec-diferenciacion-automatica" class="nav-link" data-scroll-target="#sec-diferenciacion-automatica"><span class="header-section-number">10.4</span> Diferenciación Automática</a>
  <ul class="collapse">
  <li><a href="#una-variable" id="toc-una-variable" class="nav-link" data-scroll-target="#una-variable"><span class="header-section-number">10.4.1</span> Una Variable</a></li>
  <li><a href="#dos-variables" id="toc-dos-variables" class="nav-link" data-scroll-target="#dos-variables"><span class="header-section-number">10.4.2</span> Dos Variables</a></li>
  <li><a href="#visualización" id="toc-visualización" class="nav-link" data-scroll-target="#visualización"><span class="header-section-number">10.4.3</span> Visualización</a></li>
  <li><a href="#regresión-lineal" id="toc-regresión-lineal" class="nav-link" data-scroll-target="#regresión-lineal"><span class="header-section-number">10.4.4</span> Regresión Lineal</a></li>
  </ul></li>
  <li><a href="#sec-regresion-logistica-optimizacion" id="toc-sec-regresion-logistica-optimizacion" class="nav-link" data-scroll-target="#sec-regresion-logistica-optimizacion"><span class="header-section-number">10.5</span> Regresión Logística</a></li>
  <li><a href="#sec-actualizacion-parametros" id="toc-sec-actualizacion-parametros" class="nav-link" data-scroll-target="#sec-actualizacion-parametros"><span class="header-section-number">10.6</span> Actualización de Parámetros</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<div class="quarto-title-block"><div><h1 class="title"><span id="sec-optimizacion" class="quarto-section-identifier"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Optimización</span></span></h1><button type="button" class="btn code-tools-button dropdown-toggle" id="quarto-code-tools-menu" data-bs-toggle="dropdown" aria-expanded="false"><i class="bi"></i> Código</button><ul class="dropdown-menu dropdown-menu-end" aria-labelelledby="quarto-code-tools-menu"><li><a id="quarto-show-all-code" class="dropdown-item" href="javascript:void(0)" role="button">Mostrar todo el código</a></li><li><a id="quarto-hide-all-code" class="dropdown-item" href="javascript:void(0)" role="button">Ocultar todo el código</a></li><li><hr class="dropdown-divider"></li><li><a id="quarto-view-source" class="dropdown-item" href="javascript:void(0)" role="button">Ver el código fuente</a></li></ul></div></div>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<p>El <strong>objetivo</strong> de la unidad es conocer y aplicar el método de <strong>Descenso de Gradiente</strong> y <strong>Propagación hacia Atrás</strong> par estimar los parámetros de modelos de clasificación y regresión.</p>
<section id="paquetes-usados" class="level2" data-number="10.1">
<h2 data-number="10.1" class="anchored" data-anchor-id="paquetes-usados"><span class="header-section-number">10.1</span> Paquetes usados</h2>
<div id="31b5e548" class="cell" data-execution_count="1">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.linear_model <span class="im">import</span> LogisticRegression</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.preprocessing <span class="im">import</span> StandardScaler, minmax_scale</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> scipy.stats <span class="im">import</span> multivariate_normal</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> jax <span class="im">import</span> grad, jit, value_and_grad, random</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> jax.scipy.optimize <span class="im">import</span> minimize</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> jax.lax <span class="im">as</span> lax</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> jax.numpy <span class="im">as</span> jnp</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> matplotlib <span class="im">import</span> pylab <span class="im">as</span> plt</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> seaborn <span class="im">as</span> sns</span></code><button title="Copiar al portapapeles" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<hr>
<div class="quarto-video"><iframe data-external="1" src="https://www.youtube.com/embed/oxUDZFgdo-U" width="560" height="315" title="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div>
<hr>
</section>
<section id="sec-intro-10" class="level2" data-number="10.2">
<h2 data-number="10.2" class="anchored" data-anchor-id="sec-intro-10"><span class="header-section-number">10.2</span> Introducción</h2>
<p>Existen diferentes modelos de clasificación y regresión donde no es posible encontrar una solución analítica para estimar los parámetros, por ejemplo en Regresión Logística (<a href="09Lineal.html#sec-regresion-logistica" class="quarto-xref"><span>Sección 9.5</span></a>). Es en este escenario donde se voltea a métodos de optimización iterativos para calcular los parámetros.</p>
<p>En esta unidad se describe posiblemente el método de optimización más conocido que es <strong>Descenso de Gradiente</strong>. Este método como su nombre lo indica utiliza el gradiente como su ingrediente principal; se describirá como se puede calcular el gradiente utilizando un método gráfico y como este método naturalmente realiza <strong>Propagación hacia Atrás</strong>.</p>
</section>
<section id="sec-descenso-gradiente" class="level2" data-number="10.3">
<h2 data-number="10.3" class="anchored" data-anchor-id="sec-descenso-gradiente"><span class="header-section-number">10.3</span> Descenso por Gradiente</h2>
<p>En un modelo de clasificación y regresión interesa encontrar un vector de parámetros, <span class="math inline">\(\mathbf w^{*},\)</span> que minimicen una función de error, <span class="math inline">\(E,\)</span> de la siguiente manera:</p>
<p><span class="math display">\[
\mathbf w^{*} = \textsf{argmin}_{\mathbf w} E(\mathbf w \mid \mathcal D).
\]</span></p>
<p>En el caso de que <span class="math inline">\(E(\mathbf w \mid \mathcal D)\)</span> sea una función diferenciable, el gradiente está dado por:</p>
<p><span class="math display">\[
\nabla_{\mathbf w} E(\mathbf w \mid \mathcal D) = [\frac{\partial E}{\partial \mathbf w_1}, \frac{\partial E}{\partial \mathbf w_2}, \ldots]^\intercal.
\]</span></p>
<p>La idea general es tomar la dirección opuesta al gradiente para encontrar el mínimo de la función. Entonces el cambio de parámetro está dado por</p>
<p><span class="math display">\[
\begin{split}
\Delta \mathbf w &amp;= - \eta \nabla_{\mathbf w} E\\
       \mathbf w^{t+1} &amp;= \mathbf w^{t-1} + \Delta \mathbf w \\
       &amp;= \mathbf w^{t-1} - \eta \nabla_{\mathbf w} E
\end{split}
\]</span></p>
<section id="sec-regresion-lineal-manual" class="level3" data-number="10.3.1">
<h3 data-number="10.3.1" class="anchored" data-anchor-id="sec-regresion-lineal-manual"><span class="header-section-number">10.3.1</span> Ejemplo - Regresión Lineal</h3>
<p>Suponiendo que se quieren estimar los parámetros de la siguiente ecuación lineal: <span class="math inline">\(f(x) = a x + b,\)</span> para lo cual se tiene un conjunto de entrenamiento en el intervalo <span class="math inline">\(x=[-10, 10],\)</span> generado con los parámetros <span class="math inline">\(a=2.3\)</span> y <span class="math inline">\(b=-3.\)</span> Importante no olvidar que los parámetros <span class="math inline">\(a=2.3\)</span> y <span class="math inline">\(b=-3\)</span> son desconocidos y se quieren estimar usando <strong>Descenso por Gradiente</strong> y también es importante mencionar que para este problema en particular es posible tener una solución analítica para estimar los parámetros.</p>
<p>El primer paso es definir la función de error <span class="math inline">\(E(a, b \mid \mathcal D),\)</span> en problemas de regresión una función de error viable es: <span class="math inline">\(E(a, b \mid \mathcal D) = \sum_{(x, y) \in \mathcal D} (y - f(x))^2.\)</span></p>
<p>La regla para actualizar los valores iniciales es: <span class="math inline">\(w = w - \eta \nabla_w E\)</span>; por lo que se procede a calcular <span class="math inline">\(\nabla_w E\)</span> donde <span class="math inline">\(w\)</span> corresponde a los parámetros <span class="math inline">\(a\)</span> y <span class="math inline">\(b\)</span>.</p>
<p><span class="math display">\[
\begin{split}
    \frac{\partial E}{\partial w} &amp;= \frac{\partial}{\partial w} \sum (y - f(x))^2 \\
    &amp;= 2 \sum (y - f(x)) \frac{\partial}{\partial w} (y - f(x)) \\
    &amp;= - 2 \sum (y - f(x)) \frac{\partial}{\partial w} f(x)
\end{split}
\]</span></p>
<p>donde <span class="math inline">\(\frac{\partial}{\partial a} f(x) = x\)</span> y <span class="math inline">\(\frac{\partial}{\partial b} f(x) = 1.0\)</span> Las ecuaciones para actualizar <span class="math inline">\(a\)</span> y <span class="math inline">\(b\)</span> serían:</p>
<p><span class="math display">\[
\begin{split}
    e(y, x) &amp;= y - f(x) \\
    a &amp;= a + 2 \eta \sum_{(x, y) \in \mathcal D} e(y, x) x \\  
    b &amp;= b + 2 \eta \sum_{(x, y) \in \mathcal D} e(y, x)
\end{split}
\]</span></p>
<p>Con el objetivo de visualizar descenso por gradiente y completar el ejemplo anterior, el siguiente código implementa el proceso de optimización mencionado. Lo primero es generar el conjunto de entrenamiento.</p>
<div id="252840d8" class="cell" data-execution_count="3">
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> np.linspace(<span class="op">-</span><span class="dv">10</span>, <span class="dv">10</span>, <span class="dv">50</span>)</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> <span class="fl">2.3</span> <span class="op">*</span> x <span class="op">-</span> <span class="dv">3</span></span></code><button title="Copiar al portapapeles" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>El proceso inicia con valores aleatorios de <span class="math inline">\(a\)</span> y <span class="math inline">\(b\)</span>, estos valores podrían ser <span class="math inline">\(5.3\)</span> y <span class="math inline">\(-5.1,\)</span> además se utilizará una <span class="math inline">\(\eta=0.0001\)</span>, se guardarán todos los puntos visitados en la lista <code>D</code>. Las variables y valores iniciales quedarían como:</p>
<div id="0af9ae13" class="cell" data-execution_count="4">
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a>a <span class="op">=</span> <span class="fl">5.3</span></span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>b <span class="op">=</span> <span class="op">-</span><span class="fl">5.1</span></span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a>delta <span class="op">=</span> np.inf</span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a>eta <span class="op">=</span> <span class="fl">0.0001</span></span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a>D <span class="op">=</span> [(a, b)]</span></code><button title="Copiar al portapapeles" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>El siguiente ciclo realiza la iteración del proceso de optimización y se detienen cuando los valores estimados varían poco entre dos iteraciones consecutivas, en particular cuando en promedio el cambio en las constantes sea menor a <span class="math inline">\(0.0001\)</span>.</p>
<div id="9728554d" class="cell" data-execution_count="5">
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="cf">while</span> delta <span class="op">&gt;</span> <span class="fl">0.0001</span>:</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a>    hy <span class="op">=</span> a <span class="op">*</span> x <span class="op">+</span> b</span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a>    e <span class="op">=</span> (y <span class="op">-</span> hy)</span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a>    a <span class="op">=</span> a <span class="op">+</span> <span class="dv">2</span> <span class="op">*</span> eta <span class="op">*</span> (e <span class="op">*</span> x).<span class="bu">sum</span>()</span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a>    b <span class="op">=</span> b <span class="op">+</span> <span class="dv">2</span> <span class="op">*</span> eta <span class="op">*</span> e.<span class="bu">sum</span>()</span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a>    D.append((a, b))</span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a>    delta <span class="op">=</span> np.fabs(np.array(D[<span class="op">-</span><span class="dv">1</span>]) <span class="op">-</span> np.array(D[<span class="op">-</span><span class="dv">2</span>])).mean()</span></code><button title="Copiar al portapapeles" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>En la <a href="#fig-optimizacion-evolucion-desc" class="quarto-xref">Figura&nbsp;<span>10.1</span></a> se muestra el camino que siguieron los parámetros hasta llegar a los parámetros que generaron el problema. Los parámetros que generaron el problema se encuentran marcados en negro.</p>
<div id="cell-fig-optimizacion-evolucion-desc" class="cell" data-execution_count="6">
<details class="code-fold">
<summary>Código</summary>
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a>df <span class="op">=</span> pd.DataFrame(np.array(D), columns<span class="op">=</span>[<span class="st">'a'</span>, <span class="st">'b'</span>])</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a>_ <span class="op">=</span> np.atleast_2d(np.log(np.arange(<span class="dv">1</span>, df.shape[<span class="dv">0</span>] <span class="op">+</span> <span class="dv">1</span>))).T</span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a>df[<span class="st">'iteración %'</span>] <span class="op">=</span> minmax_scale(_).flatten()</span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a>sns.relplot(data<span class="op">=</span>df, kind<span class="op">=</span><span class="st">'scatter'</span>, x<span class="op">=</span><span class="st">'a'</span>, y<span class="op">=</span><span class="st">'b'</span>, hue<span class="op">=</span><span class="st">'iteración %'</span>)</span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a>_ <span class="op">=</span> plt.tight_layout()</span></code><button title="Copiar al portapapeles" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div id="fig-optimizacion-evolucion-desc" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-optimizacion-evolucion-desc-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="10Optimizacion_files/figure-html/fig-optimizacion-evolucion-desc-output-1.png" width="555" height="470" class="figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-optimizacion-evolucion-desc-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figura&nbsp;10.1: Camino de los parámetros en el proceso de optimización.
</figcaption>
</figure>
</div>
</div>
</div>
</section>
</section>
<section id="sec-diferenciacion-automatica" class="level2" data-number="10.4">
<h2 data-number="10.4" class="anchored" data-anchor-id="sec-diferenciacion-automatica"><span class="header-section-number">10.4</span> Diferenciación Automática</h2>
<p>No en todos los casos es posible encontrar una ecuación cerrada para actualizar los parámetros; también el trabajo manual que se requiere para encontrar la solución analítica es considerable y depende de la complejidad de la función objetivo que se quiere derivar. Observando el procedimiento para actualizar los parámetros, i.e., <span class="math inline">\(\mathbf w^{t+1} = w^{t-1} - \eta \nabla_{\mathbf w} E\)</span>, se concluye que se requiere evaluar <span class="math inline">\(\nabla_{\mathbf w} E\)</span> en un punto en particular, entonces, para solucionar el problema, es suficiente contar con un procedimiento que permita conocer el valor de <span class="math inline">\(\nabla_{\mathbf w} E\)</span> en cualquier punto deseado, y en particular no es necesario conocer la solución analítica de <span class="math inline">\(\nabla_{\mathbf w} E.\)</span> Al procedimiento que encuentra el valor de la derivada de cualquier función en un punto de manera automática se le conoce como diferenciación automática.</p>
<section id="una-variable" class="level3" data-number="10.4.1">
<h3 data-number="10.4.1" class="anchored" data-anchor-id="una-variable"><span class="header-section-number">10.4.1</span> Una Variable</h3>
<p>Se puede entender el proceso de diferenciación automática siguiendo un par de ejemplos. Sea <span class="math inline">\(f(x) = x^2,\)</span> en este caso <span class="math inline">\(f'(x) = 2x.\)</span> Ahora considerando que se quiere evaluar <span class="math inline">\(f'(2.5)\)</span>, se sabe que <span class="math inline">\(f'(2.5)=5,\)</span> pero también se puede generar un programa donde en la primera fase se calcule <span class="math inline">\(f(2.5)=(2.5)^2\)</span> y al momento de calcular <span class="math inline">\(f\)</span> se guarde en un espacio asociado a <span class="math inline">\(f\)</span> el valor de <span class="math inline">\(f'(2.5).\)</span> es decir <span class="math inline">\(5\)</span>. Se puede observar que este procedimiento soluciona este problema simple para cualquier función de una variable.</p>
<p>La librería <a href="https://jax.readthedocs.io">JAX</a> permite hacer diferenciación automática en Python. Siguiendo el ejemplo anterior, se genera la función <span class="math inline">\(f(x) = x^2\)</span> con el siguiente código</p>
<div id="e51c2725" class="cell" data-execution_count="7">
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> sq(x):</span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> x<span class="op">**</span><span class="dv">2</span></span></code><button title="Copiar al portapapeles" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Esta función se puede evaluar como <code>sq(2.5)</code>; lo interesante es que <code>sq</code> se puede componer con la función <code>grad</code> para calcular la derivada de la siguiente manera.</p>
<div id="5afe37e5" class="cell" data-execution_count="8">
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a>res <span class="op">=</span> grad(sq)(<span class="fl">2.5</span>)</span></code><button title="Copiar al portapapeles" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Ejecutando el código anterior se obtiene <span class="math inline">\(5\)</span>, <code>grad</code> internamente guarda <span class="math inline">\(5\)</span>, <code>grad</code> asociado a <code>sq</code>.</p>
<p>Ahora en el caso de una composición, por ejemplo, sea <span class="math inline">\(g(x)=\sin(x)\)</span> y se desea conocer <span class="math inline">\(\frac{d}{dx} g(f(x)),\)</span> siguiendo el mismo principio primero se evalúa <span class="math inline">\(f(2.5)\)</span> y se guarda asociado a <span class="math inline">\(f\)</span> el valor de <span class="math inline">\(5\)</span> que corresponde a <span class="math inline">\(f',\)</span> después se evalúa <span class="math inline">\(g(5)\)</span> y se guarda asociado a <span class="math inline">\(g\)</span> el valor <span class="math inline">\(\cos(2.5^2).\)</span> Ahora para conocer el valor de <span class="math inline">\(\frac{d}{dx} g(f(2.5))\)</span> se camina en el sentido inverso multiplicando todos los valores guardados, es decir, se multiplica <span class="math inline">\(5 \times \cos(2.5^2).\)</span> Implementando este ejemplo en Python quedaría como:</p>
<div id="8fd8aafb" class="cell" data-execution_count="10">
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> sin_sq(x):</span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> jnp.sin(sq(x))</span></code><button title="Copiar al portapapeles" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>donde la función <code>sin_sq</code> tiene la composición de <span class="math inline">\(g \circ f.\)</span> Ahora la derivada de esta composición en 2.5 se obtiene como <code>grad(sin_sq)(2.5)</code> obteniendo un valor de <span class="math inline">\(4.9972\)</span>.</p>
</section>
<section id="dos-variables" class="level3" data-number="10.4.2">
<h3 data-number="10.4.2" class="anchored" data-anchor-id="dos-variables"><span class="header-section-number">10.4.2</span> Dos Variables</h3>
<p>El ejemplo anterior se puede extender para cualquier composición de funciones, pero ese ejemplo no deja claro lo que pasaría con una función de dos o más variables, como por ejemplo, <span class="math inline">\(h(x, y) = x \times y.\)</span> En este caso, el objetivo es calcular tanto <span class="math inline">\(\frac{d}{dx} h\)</span> como <span class="math inline">\(\frac{d}{dy} h.\)</span></p>
<p>El procedimiento es similar, con el ingrediente extra de que se tiene que recordar la función y la posición del argumento, es decir, para <span class="math inline">\(h(2, 5)\)</span> se asocia el valor <span class="math inline">\(\frac{d}{dx} h\)</span> con el primer argumento de la función <span class="math inline">\(h\)</span> y <span class="math inline">\(\frac{d}{dx} h\)</span> con el segundo argumento de la función <span class="math inline">\(h\)</span>. Para <span class="math inline">\(h'(2, 5)\)</span> se asocia el valor de <span class="math inline">\(5\)</span> con el primer argumento de <span class="math inline">\(h\)</span> y <span class="math inline">\(2\)</span> con el segundo argumento de <span class="math inline">\(h.\)</span> Se puede observar que los valores guardados corresponden a <span class="math inline">\(\frac{d}{dx}h(2, 5)=y=5\)</span> y <span class="math inline">\(\frac{d}{dy}h(2, 5)=x=2.\)</span> Escribiendo este ejemplo en Python quedaría como</p>
<div id="8133eb32" class="cell" data-execution_count="12">
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> prod(x):</span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> x[<span class="dv">0</span>] <span class="op">*</span> x[<span class="dv">1</span>]</span></code><button title="Copiar al portapapeles" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>calculando la derivada como <code>grad(prod)(jnp.array([2., 5.]))</code> se obtiene <code>[5., 2.]</code> que corresponde a la derivada parcial en cada argumento.</p>
</section>
<section id="visualización" class="level3" data-number="10.4.3">
<h3 data-number="10.4.3" class="anchored" data-anchor-id="visualización"><span class="header-section-number">10.4.3</span> Visualización</h3>
<p>Una manera de visualizar este proceso de diferenciación automática es poniéndolo en un árbol de expresión, en la <a href="#fig-optimizacion-arbol-expresion" class="quarto-xref">Figura&nbsp;<span>10.2</span></a> se muestra la ecuación <span class="math inline">\(ax^2+bx+c\)</span>. Dentro de cada nodo se observa el valor que se tiene que guardar para cualquier valor de <span class="math inline">\(a,b,c\)</span> y <span class="math inline">\(x.\)</span></p>
<div id="fig-optimizacion-arbol-expresion" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-optimizacion-arbol-expresion-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="../docs/assets/images/eval.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-optimizacion-arbol-expresion-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figura&nbsp;10.2: Árbol de expresión
</figcaption>
</figure>
</div>
</section>
<section id="regresión-lineal" class="level3" data-number="10.4.4">
<h3 data-number="10.4.4" class="anchored" data-anchor-id="regresión-lineal"><span class="header-section-number">10.4.4</span> Regresión Lineal</h3>
<p>En esta sección se realiza utilizando diferenciación automática el ejemplo de regresión lineal (<a href="#sec-regresion-lineal-manual" class="quarto-xref"><span>Sección 10.3.1</span></a>). Lo primero que se tiene que hacer es definir la función para la cual se quieren optimizar los parámetros. Esta función es <span class="math inline">\(ax + b\)</span> la cual se define con el siguiente código. El primer argumento de la función son los parámetros y después viene los argumentos de la función que en este caso son los valores de <span class="math inline">\(x.\)</span></p>
<div id="4b18974c" class="cell" data-execution_count="13">
<div class="sourceCode cell-code" id="cb10"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> func(params, x):</span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a>    a, b <span class="op">=</span> params</span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> a <span class="op">*</span> x <span class="op">+</span> b</span></code><button title="Copiar al portapapeles" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>El siguiente paso es definir la función de error, en este caso la función de error definida previamente es <span class="math inline">\(\sum_{i=1}^{N} (y_i - \hat y_i)^2\)</span> tal y como se muestra en la siguiente función.</p>
<div id="5b688ba9" class="cell" data-execution_count="14">
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> error(params, x, y):</span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a>    hy <span class="op">=</span> func(params, x)</span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> lax.fori_loop(<span class="dv">0</span>, x.shape[<span class="dv">0</span>],</span>
<span id="cb11-4"><a href="#cb11-4" aria-hidden="true" tabindex="-1"></a>                         <span class="kw">lambda</span> i, acc: acc <span class="op">+</span> (y[i] <span class="op">-</span> hy[i])<span class="op">**</span><span class="dv">2</span>,</span>
<span id="cb11-5"><a href="#cb11-5" aria-hidden="true" tabindex="-1"></a>                         <span class="dv">0</span>)</span></code><button title="Copiar al portapapeles" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Finalmente, se actualizan los parámetros siguiendo un procedimiento equivalente al mostrado anteriormente. La primera linea define el valor de <span class="math inline">\(\eta,\)</span> el valor de la variable <code>delta</code> indica el término del ciclo, la tercera línea define los parámetros iniciales (<code>params</code>), la siguiente guarda los todos los puntos visitados (cuarta línea) y la quinta línea deriva la función de <code>error</code> para calcular el gradiente. La primera instrucción dentro del ciclo actualiza los parámetros.</p>
<div id="d8cc2697" class="cell" data-execution_count="15">
<div class="sourceCode cell-code" id="cb12"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a>eta <span class="op">=</span> <span class="fl">0.0001</span></span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a>delta <span class="op">=</span> np.inf</span>
<span id="cb12-3"><a href="#cb12-3" aria-hidden="true" tabindex="-1"></a>params <span class="op">=</span> jnp.array([<span class="fl">5.3</span>, <span class="op">-</span><span class="fl">5.1</span>])</span>
<span id="cb12-4"><a href="#cb12-4" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> jnp.array(x)</span>
<span id="cb12-5"><a href="#cb12-5" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> jnp.array(y)</span>
<span id="cb12-6"><a href="#cb12-6" aria-hidden="true" tabindex="-1"></a>D <span class="op">=</span> [params]</span>
<span id="cb12-7"><a href="#cb12-7" aria-hidden="true" tabindex="-1"></a>error_grad <span class="op">=</span> grad(error)</span>
<span id="cb12-8"><a href="#cb12-8" aria-hidden="true" tabindex="-1"></a><span class="cf">while</span> delta <span class="op">&gt;</span> <span class="fl">0.0001</span>:</span>
<span id="cb12-9"><a href="#cb12-9" aria-hidden="true" tabindex="-1"></a>    params <span class="op">-=</span> eta <span class="op">*</span> error_grad(params, x, y)</span>
<span id="cb12-10"><a href="#cb12-10" aria-hidden="true" tabindex="-1"></a>    D.append(params)</span>
<span id="cb12-11"><a href="#cb12-11" aria-hidden="true" tabindex="-1"></a>    delta <span class="op">=</span> jnp.fabs(D[<span class="op">-</span><span class="dv">1</span>] <span class="op">-</span> D[<span class="op">-</span><span class="dv">2</span>]).mean()</span></code><button title="Copiar al portapapeles" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
</section>
<section id="sec-regresion-logistica-optimizacion" class="level2" data-number="10.5">
<h2 data-number="10.5" class="anchored" data-anchor-id="sec-regresion-logistica-optimizacion"><span class="header-section-number">10.5</span> Regresión Logística</h2>
<p>En esta sección, se presenta la implementación de Regresión Logística (<a href="09Lineal.html#sec-regresion-logistica" class="quarto-xref"><span>Sección 9.5</span></a>) utilizando diferenciación automática.</p>
<p>El método se probará en los datos generados por dos normales en <span class="math inline">\(\mathbb R^2\)</span> que se generan con las siguientes instrucciones.</p>
<div id="d1ccb447" class="cell" data-execution_count="16">
<div class="sourceCode cell-code" id="cb13"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a>X_1 <span class="op">=</span> multivariate_normal(mean<span class="op">=</span>[<span class="dv">15</span>, <span class="dv">20</span>], cov<span class="op">=</span>[[<span class="dv">3</span>, <span class="op">-</span><span class="dv">3</span>], [<span class="op">-</span><span class="dv">3</span>, <span class="dv">8</span>]]).rvs(<span class="dv">1000</span>)</span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a>X_2 <span class="op">=</span> multivariate_normal(mean<span class="op">=</span>[<span class="dv">8</span>, <span class="dv">8</span>], cov<span class="op">=</span>[[<span class="dv">4</span>, <span class="dv">0</span>], [<span class="dv">0</span>, <span class="dv">2</span>]]).rvs(<span class="dv">1000</span>)</span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a>T <span class="op">=</span> np.concatenate((X_1, X_2))</span>
<span id="cb13-4"><a href="#cb13-4" aria-hidden="true" tabindex="-1"></a>normalize <span class="op">=</span> StandardScaler().fit(T)</span>
<span id="cb13-5"><a href="#cb13-5" aria-hidden="true" tabindex="-1"></a>T <span class="op">=</span> jnp.array(normalize.transform(T))</span>
<span id="cb13-6"><a href="#cb13-6" aria-hidden="true" tabindex="-1"></a>y_t <span class="op">=</span> jnp.array([<span class="dv">1</span>] <span class="op">*</span> X_1.shape[<span class="dv">0</span>] <span class="op">+</span> [<span class="dv">0</span>] <span class="op">*</span> X_2.shape[<span class="dv">0</span>])</span></code><button title="Copiar al portapapeles" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Siguiendo los pasos utilizados en el ejemplo anterior, la primera función a implementar es el model, es decir la función sigmoide, i.e., <span class="math inline">\(\textsf{sigmoid}(\mathbf x) = \frac{1}{1 + \exp(- (\mathbf w \cdot \mathbf x + w_0))}.\)</span></p>
<div id="2a02dffa" class="cell" data-execution_count="17">
<div class="sourceCode cell-code" id="cb14"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> modelo(params, x):</span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a>    _ <span class="op">=</span> jnp.exp(<span class="op">-</span>(jnp.dot(x, params[:<span class="dv">2</span>]) <span class="op">+</span> params[<span class="op">-</span><span class="dv">1</span>]))</span>
<span id="cb14-3"><a href="#cb14-3" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="dv">1</span> <span class="op">/</span> (<span class="dv">1</span> <span class="op">+</span> _)</span></code><button title="Copiar al portapapeles" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>El siguiente paso es implementar la función de error que guía el proceso de optimización, en el caso de regresión logística de dos clases la función de error corresponde a la media de la Entropía Cruzada (<a href="09Lineal.html#eq-lineal-entropia-cruzada" class="quarto-xref">Ecuación&nbsp;<span>9.2</span></a>). Esta función se implementa en dos pasos, primero se calcula la entropía cruzada usando el siguiente procedimiento. Donde la primera línea verifica selecciona si se trata de la clase positiva (<span class="math inline">\(1\)</span>) o de la clase negativa <span class="math inline">\(0\)</span> y calcula el <span class="math inline">\(\log \hat y\)</span> o <span class="math inline">\(\log (1 - \hat y)\)</span> respectivamente.</p>
<div id="7c1323e2" class="cell" data-execution_count="18">
<div class="sourceCode cell-code" id="cb15"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> entropia_cruzada(y, hy):</span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a>    _ <span class="op">=</span> lax.cond(y <span class="op">==</span> <span class="dv">1</span>, <span class="kw">lambda</span> w: jnp.log(w), <span class="kw">lambda</span> w: jnp.log(<span class="dv">1</span> <span class="op">-</span> w), hy)</span>
<span id="cb15-3"><a href="#cb15-3" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> lax.cond(_ <span class="op">==</span> <span class="op">-</span>jnp.inf, <span class="kw">lambda</span> w: jnp.log(<span class="fl">1e-6</span>), <span class="kw">lambda</span> w: w, _)</span></code><button title="Copiar al portapapeles" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>El segundo paso es calcular la media de la entropía cruzada de cada elemento de <span class="math inline">\(\mathcal D,\)</span> esto se puede realizar con la siguiente función.</p>
<div id="36a220fd" class="cell" data-execution_count="19">
<div class="sourceCode cell-code" id="cb16"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> suma_entropia_cruzada(params, x, y):</span>
<span id="cb16-2"><a href="#cb16-2" aria-hidden="true" tabindex="-1"></a>    hy <span class="op">=</span> modelo(params, x)</span>
<span id="cb16-3"><a href="#cb16-3" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="op">-</span> lax.fori_loop(<span class="dv">0</span>, y.shape[<span class="dv">0</span>],</span>
<span id="cb16-4"><a href="#cb16-4" aria-hidden="true" tabindex="-1"></a>                           <span class="kw">lambda</span> i, x: x <span class="op">+</span> entropia_cruzada(y[i], hy[i]),</span>
<span id="cb16-5"><a href="#cb16-5" aria-hidden="true" tabindex="-1"></a>                           <span class="dv">1</span>) <span class="op">/</span> y.shape[<span class="dv">0</span>]</span></code><button title="Copiar al portapapeles" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>El paso final es actualizar los parámetros, en esta ocasión se utilizará la función <code>value_and_grad</code> que regresa la evaluación de la función así como la derivada, esto para poder desplegar como el error disminuye con el paso de las iteraciones. El código es similar al mostrado anteriormente. La diferencias son la forma en generar los parámetros iniciales, primeras tres líneas, el parámetro <span class="math inline">\(\eta\)</span>, que se itera por <span class="math inline">\(n=5000\)</span> iteraciones y la función <code>value_and_grad</code>.</p>
<div id="c0eed4fb" class="cell" data-execution_count="20">
<div class="sourceCode cell-code" id="cb17"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a>key <span class="op">=</span> random.PRNGKey(<span class="dv">0</span>)</span>
<span id="cb17-2"><a href="#cb17-2" aria-hidden="true" tabindex="-1"></a>key, subkey <span class="op">=</span> random.split(key)</span>
<span id="cb17-3"><a href="#cb17-3" aria-hidden="true" tabindex="-1"></a>params <span class="op">=</span> random.normal(subkey, (<span class="dv">3</span>,)) <span class="op">*</span> jnp.sqrt(<span class="dv">2</span> <span class="op">/</span> <span class="dv">2</span>)</span>
<span id="cb17-4"><a href="#cb17-4" aria-hidden="true" tabindex="-1"></a>eta <span class="op">=</span> <span class="fl">0.1</span></span>
<span id="cb17-5"><a href="#cb17-5" aria-hidden="true" tabindex="-1"></a>error <span class="op">=</span> []</span>
<span id="cb17-6"><a href="#cb17-6" aria-hidden="true" tabindex="-1"></a>error_grad  <span class="op">=</span> value_and_grad(suma_entropia_cruzada)</span>
<span id="cb17-7"><a href="#cb17-7" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">5000</span>):</span>
<span id="cb17-8"><a href="#cb17-8" aria-hidden="true" tabindex="-1"></a>    _, g <span class="op">=</span> error_grad(params, T, y_t)</span>
<span id="cb17-9"><a href="#cb17-9" aria-hidden="true" tabindex="-1"></a>    error.append(_)</span>
<span id="cb17-10"><a href="#cb17-10" aria-hidden="true" tabindex="-1"></a>    params <span class="op">-=</span> eta <span class="op">*</span> g</span></code><button title="Copiar al portapapeles" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>La <a href="#fig-optimizacion-descenso-ent-x" class="quarto-xref">Figura&nbsp;<span>10.3</span></a> muestra como se reduce la media de la entropía cruzada con respecto al número de iteraciones, este error sigue bajando aunque la velocidad disminuye drasticamente.</p>
<div id="cell-fig-optimizacion-descenso-ent-x" class="cell" data-execution_count="21">
<details class="code-fold">
<summary>Código</summary>
<div class="sourceCode cell-code" id="cb18"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a>df <span class="op">=</span> pd.DataFrame({<span class="st">'Iteración'</span>: np.arange(<span class="dv">5000</span>), <span class="st">'error'</span>: np.array(error)})</span>
<span id="cb18-2"><a href="#cb18-2" aria-hidden="true" tabindex="-1"></a>sns.relplot(data<span class="op">=</span>df, x<span class="op">=</span><span class="st">'Iteración'</span>, y<span class="op">=</span><span class="st">'error'</span>, kind<span class="op">=</span><span class="st">'line'</span>)</span>
<span id="cb18-3"><a href="#cb18-3" aria-hidden="true" tabindex="-1"></a>_ <span class="op">=</span> plt.tight_layout()</span></code><button title="Copiar al portapapeles" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div id="fig-optimizacion-descenso-ent-x" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-optimizacion-descenso-ent-x-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="10Optimizacion_files/figure-html/fig-optimizacion-descenso-ent-x-output-1.png" width="470" height="470" class="figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-optimizacion-descenso-ent-x-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figura&nbsp;10.3: Descenso de Entropía Cruzada
</figcaption>
</figure>
</div>
</div>
</div>

<p>Optimizando los parámetros de la regresión logística utilizando el procedimiento de diferenciación automática se obtiene [3.9203, 4.3832, -0.4768].</p>
<p>En Discriminantes Lineales (<a href="09Lineal.html" class="quarto-xref"><span>Capítulo 9</span></a>) se presento el procedimiento para entrenar un clasificador logístico usando la siguiente instrucción.</p>
<div id="0d98e324" class="cell" data-execution_count="23">
<div class="sourceCode cell-code" id="cb19"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a>lr <span class="op">=</span> LogisticRegression().fit(T, y_t)</span></code><button title="Copiar al portapapeles" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Los parámetros son [3.2929, 4.4508, -0.3220]. El error en el conjunto se entrenamiento es se puede calcular de la siguiente manera. En comparación el error obtenido por diferenciación automática es <span class="math inline">\(0.0026\)</span>, por supuesto este puede variar cambiando el número de iteraciones.</p>
<div id="e36f2971" class="cell" data-execution_count="25">
<div class="sourceCode cell-code" id="cb20"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a>_ <span class="op">=</span> jnp.array([lr.coef_[<span class="dv">0</span>, <span class="dv">0</span>], lr.coef_[<span class="dv">0</span>, <span class="dv">1</span>], lr.intercept_[<span class="dv">0</span>]])</span>
<span id="cb20-2"><a href="#cb20-2" aria-hidden="true" tabindex="-1"></a>suma_entropia_cruzada(_, T, y_t)</span></code><button title="Copiar al portapapeles" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="25">
<pre><code>Array(0.00329172, dtype=float32)</code></pre>
</div>
</div>
</section>
<section id="sec-actualizacion-parametros" class="level2" data-number="10.6">
<h2 data-number="10.6" class="anchored" data-anchor-id="sec-actualizacion-parametros"><span class="header-section-number">10.6</span> Actualización de Parámetros</h2>
<p>Por supuesto la regla <span class="math inline">\(\mathbf w^{t+1} = \mathbf w^{t-1} - \eta \nabla_{\mathbf w} E\)</span> para actualizar los parámetros <span class="math inline">\(\mathbf w\)</span> no es única y existe una gama de métodos que se pueden seleccionar dependiendo de las características del problema. En particular regresión logística es una problema de optimización convexo, en este tipo de problemas un algoritmos para encontrar los parámetros es el <code>BFGS</code>. Por ejemplo el siguiente código utiliza este algoritmo para encontrar los parámetros de la regresión logística.</p>
<div id="61b08767" class="cell" data-execution_count="26">
<div class="sourceCode cell-code" id="cb22"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb22-1"><a href="#cb22-1" aria-hidden="true" tabindex="-1"></a>p <span class="op">=</span> random.normal(subkey, (<span class="dv">3</span>,)) <span class="op">*</span> jnp.sqrt(<span class="dv">2</span> <span class="op">/</span> <span class="dv">2</span>)</span>
<span id="cb22-2"><a href="#cb22-2" aria-hidden="true" tabindex="-1"></a>res <span class="op">=</span> minimize(suma_entropia_cruzada, p, args<span class="op">=</span>(T, y_t), method<span class="op">=</span><span class="st">'BFGS'</span>)</span></code><button title="Copiar al portapapeles" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Se observa como se usa la misma función de error (<code>suma_entropia_cruzada</code>) y los mismos parámetros iniciales. Los parámetros que se encuentran con este método son [13.5714, 20.6805, 1.9514] y tiene un error <span class="math inline">\(-0.0005\)</span>.</p>
<p>La <a href="#fig-optimizacion-comparacion" class="quarto-xref">Figura&nbsp;<span>10.4</span></a> muestra la función discriminantes obtenida por cada uno de los métodos, el método de diferenciación automática (JAX), el método de la librearía <code>sklearn</code> y el algoritmo <code>BFGS</code>. Se puede observar que el plano de <code>sklearn</code> y <code>BFGS</code> son más similares, esto no es sorpresa porque los dos métodos implementan el mismo método de optimización, es decir, <code>BFGS</code>. Por supuesto la implementaciones tiene algunas diferencias, que pueden ir desde el número de iteraciones y la forma de generar los parámetros iniciales. Si se escalan los parámetros <span class="math inline">\(\mathbf w\)</span> para que tengan una longitud de <span class="math inline">\(1\)</span> se observaría que <code>sklearn</code> y <code>BFGS</code> se tocan.</p>
<div id="cell-fig-optimizacion-comparacion" class="cell" data-execution_count="28">
<details class="code-fold">
<summary>Código</summary>
<div class="sourceCode cell-code" id="cb23"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb23-1"><a href="#cb23-1" aria-hidden="true" tabindex="-1"></a>W <span class="op">=</span> [jnp.array([lr.coef_[<span class="dv">0</span>, <span class="dv">0</span>], lr.coef_[<span class="dv">0</span>, <span class="dv">1</span>], lr.intercept_[<span class="dv">0</span>]]), params, res.x]</span>
<span id="cb23-2"><a href="#cb23-2" aria-hidden="true" tabindex="-1"></a>g <span class="op">=</span> []</span>
<span id="cb23-3"><a href="#cb23-3" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> w, tipo <span class="kw">in</span> <span class="bu">zip</span>(W, [<span class="st">'sklearn'</span>, <span class="st">'JAX'</span>, <span class="st">'BFGS'</span>]):</span>
<span id="cb23-4"><a href="#cb23-4" aria-hidden="true" tabindex="-1"></a>     w_1, w_2, w_0 <span class="op">=</span> w</span>
<span id="cb23-5"><a href="#cb23-5" aria-hidden="true" tabindex="-1"></a>     g <span class="op">+=</span> [<span class="bu">dict</span>(x1<span class="op">=</span><span class="bu">float</span>(x), x2<span class="op">=</span><span class="bu">float</span>(y), tipo<span class="op">=</span>tipo)</span>
<span id="cb23-6"><a href="#cb23-6" aria-hidden="true" tabindex="-1"></a>           <span class="cf">for</span> x, y <span class="kw">in</span> <span class="bu">zip</span>(T[:, <span class="dv">0</span>], (<span class="op">-</span>w_0 <span class="op">-</span> w_1 <span class="op">*</span> T[:, <span class="dv">0</span>]) <span class="op">/</span> w_2)]</span>
<span id="cb23-7"><a href="#cb23-7" aria-hidden="true" tabindex="-1"></a>     <span class="co">#&nbsp;w_1, w_2, w_0 = w / np.linalg.norm(w) * 3</span></span>
<span id="cb23-8"><a href="#cb23-8" aria-hidden="true" tabindex="-1"></a>     g.append(<span class="bu">dict</span>(x1<span class="op">=</span><span class="bu">float</span>(w_1), x2<span class="op">=</span><span class="bu">float</span>(w_2), clase<span class="op">=</span>tipo))</span>
<span id="cb23-9"><a href="#cb23-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-10"><a href="#cb23-10" aria-hidden="true" tabindex="-1"></a>df <span class="op">=</span> pd.DataFrame(g <span class="op">+</span> <span class="op">\</span></span>
<span id="cb23-11"><a href="#cb23-11" aria-hidden="true" tabindex="-1"></a>                  [<span class="bu">dict</span>(x1<span class="op">=</span>x, x2<span class="op">=</span>y, clase<span class="op">=</span><span class="st">'P'</span>) <span class="cf">for</span> x, y <span class="kw">in</span> normalize.transform(X_1)] <span class="op">+</span> <span class="op">\</span></span>
<span id="cb23-12"><a href="#cb23-12" aria-hidden="true" tabindex="-1"></a>                  [<span class="bu">dict</span>(x1<span class="op">=</span>x, x2<span class="op">=</span>y, clase<span class="op">=</span><span class="st">'N'</span>) <span class="cf">for</span> x, y <span class="kw">in</span> normalize.transform(X_2)]</span>
<span id="cb23-13"><a href="#cb23-13" aria-hidden="true" tabindex="-1"></a>                 )</span>
<span id="cb23-14"><a href="#cb23-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-15"><a href="#cb23-15" aria-hidden="true" tabindex="-1"></a>ax <span class="op">=</span> sns.scatterplot(data<span class="op">=</span>df, x<span class="op">=</span><span class="st">'x1'</span>, y<span class="op">=</span><span class="st">'x2'</span>, hue<span class="op">=</span><span class="st">'clase'</span>, legend<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb23-16"><a href="#cb23-16" aria-hidden="true" tabindex="-1"></a>sns.lineplot(data<span class="op">=</span>df, x<span class="op">=</span><span class="st">'x1'</span>, y<span class="op">=</span><span class="st">'x2'</span>, </span>
<span id="cb23-17"><a href="#cb23-17" aria-hidden="true" tabindex="-1"></a>             ax<span class="op">=</span>ax, </span>
<span id="cb23-18"><a href="#cb23-18" aria-hidden="true" tabindex="-1"></a>             hue<span class="op">=</span><span class="st">'tipo'</span>, </span>
<span id="cb23-19"><a href="#cb23-19" aria-hidden="true" tabindex="-1"></a>             palette<span class="op">=</span>sns.color_palette()[:<span class="dv">3</span>],</span>
<span id="cb23-20"><a href="#cb23-20" aria-hidden="true" tabindex="-1"></a>             legend<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb23-21"><a href="#cb23-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-22"><a href="#cb23-22" aria-hidden="true" tabindex="-1"></a>_ <span class="op">=</span> ax.axis(<span class="st">'equal'</span>)</span></code><button title="Copiar al portapapeles" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div id="fig-optimizacion-comparacion" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-optimizacion-comparacion-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="10Optimizacion_files/figure-html/fig-optimizacion-comparacion-output-1.png" width="582" height="427" class="figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-optimizacion-comparacion-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figura&nbsp;10.4: Comparación de modelos lineales con diferentes optimizadores
</figcaption>
</figure>
</div>
</div>
</div>


<!-- -->

</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copiado");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copiado");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
  const viewSource = window.document.getElementById('quarto-view-source') ||
                     window.document.getElementById('quarto-code-tools-source');
  if (viewSource) {
    const sourceUrl = viewSource.getAttribute("data-quarto-source-url");
    viewSource.addEventListener("click", function(e) {
      if (sourceUrl) {
        // rstudio viewer pane
        if (/\bcapabilities=\b/.test(window.location)) {
          window.open(sourceUrl);
        } else {
          window.location.href = sourceUrl;
        }
      } else {
        const modal = new bootstrap.Modal(document.getElementById('quarto-embedded-source-code-modal'));
        modal.show();
      }
      return false;
    });
  }
  function toggleCodeHandler(show) {
    return function(e) {
      const detailsSrc = window.document.querySelectorAll(".cell > details > .sourceCode");
      for (let i=0; i<detailsSrc.length; i++) {
        const details = detailsSrc[i].parentElement;
        if (show) {
          details.open = true;
        } else {
          details.removeAttribute("open");
        }
      }
      const cellCodeDivs = window.document.querySelectorAll(".cell > .sourceCode");
      const fromCls = show ? "hidden" : "unhidden";
      const toCls = show ? "unhidden" : "hidden";
      for (let i=0; i<cellCodeDivs.length; i++) {
        const codeDiv = cellCodeDivs[i];
        if (codeDiv.classList.contains(fromCls)) {
          codeDiv.classList.remove(fromCls);
          codeDiv.classList.add(toCls);
        } 
      }
      return false;
    }
  }
  const hideAllCode = window.document.getElementById("quarto-hide-all-code");
  if (hideAllCode) {
    hideAllCode.addEventListener("click", toggleCodeHandler(false));
  }
  const showAllCode = window.document.getElementById("quarto-show-all-code");
  if (showAllCode) {
    showAllCode.addEventListener("click", toggleCodeHandler(true));
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp("https:\/\/ingeotec\.github\.io\/AprendizajeComputacional");
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="../capitulos/09Lineal.html" class="pagination-link" aria-label="Discriminantes Lineales">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Discriminantes Lineales</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="../capitulos/11RedesNeuronales.html" class="pagination-link" aria-label="Redes Neuronales">
        <span class="nav-page-text"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Redes Neuronales</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav><div class="modal fade" id="quarto-embedded-source-code-modal" tabindex="-1" aria-labelledby="quarto-embedded-source-code-modal-label" aria-hidden="true"><div class="modal-dialog modal-dialog-scrollable"><div class="modal-content"><div class="modal-header"><h5 class="modal-title" id="quarto-embedded-source-code-modal-label">Ejecutar el código</h5><button class="btn-close" data-bs-dismiss="modal"></button></div><div class="modal-body"><div class="">
<div class="sourceCode" id="cb24" data-shortcodes="false"><pre class="sourceCode markdown code-with-copy"><code class="sourceCode markdown"><span id="cb24-1"><a href="#cb24-1" aria-hidden="true" tabindex="-1"></a><span class="fu"># Optimización {#sec-optimizacion}</span></span>
<span id="cb24-2"><a href="#cb24-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-3"><a href="#cb24-3" aria-hidden="true" tabindex="-1"></a>El **objetivo** de la unidad es conocer y aplicar el método de **Descenso de Gradiente** y **Propagación hacia Atrás** par estimar los parámetros de modelos de clasificación y regresión.</span>
<span id="cb24-4"><a href="#cb24-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-5"><a href="#cb24-5" aria-hidden="true" tabindex="-1"></a><span class="fu">## Paquetes usados</span></span>
<span id="cb24-6"><a href="#cb24-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-9"><a href="#cb24-9" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb24-10"><a href="#cb24-10" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: true</span></span>
<span id="cb24-11"><a href="#cb24-11" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.linear_model <span class="im">import</span> LogisticRegression</span>
<span id="cb24-12"><a href="#cb24-12" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.preprocessing <span class="im">import</span> StandardScaler, minmax_scale</span>
<span id="cb24-13"><a href="#cb24-13" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> scipy.stats <span class="im">import</span> multivariate_normal</span>
<span id="cb24-14"><a href="#cb24-14" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> jax <span class="im">import</span> grad, jit, value_and_grad, random</span>
<span id="cb24-15"><a href="#cb24-15" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> jax.scipy.optimize <span class="im">import</span> minimize</span>
<span id="cb24-16"><a href="#cb24-16" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> jax.lax <span class="im">as</span> lax</span>
<span id="cb24-17"><a href="#cb24-17" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> jax.numpy <span class="im">as</span> jnp</span>
<span id="cb24-18"><a href="#cb24-18" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> matplotlib <span class="im">import</span> pylab <span class="im">as</span> plt</span>
<span id="cb24-19"><a href="#cb24-19" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb24-20"><a href="#cb24-20" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb24-21"><a href="#cb24-21" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> seaborn <span class="im">as</span> sns</span>
<span id="cb24-22"><a href="#cb24-22" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb24-23"><a href="#cb24-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-26"><a href="#cb24-26" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb24-27"><a href="#cb24-27" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: false</span></span>
<span id="cb24-28"><a href="#cb24-28" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> IPython.display <span class="im">import</span> Markdown</span>
<span id="cb24-29"><a href="#cb24-29" aria-hidden="true" tabindex="-1"></a>sns.set_style(<span class="st">'whitegrid'</span>)</span>
<span id="cb24-30"><a href="#cb24-30" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb24-31"><a href="#cb24-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-32"><a href="#cb24-32" aria-hidden="true" tabindex="-1"></a>::: {.content-visible when-format="html"}</span>
<span id="cb24-33"><a href="#cb24-33" aria-hidden="true" tabindex="-1"></a>---</span>
<span id="cb24-34"><a href="#cb24-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-35"><a href="#cb24-35" aria-hidden="true" tabindex="-1"></a>{{&lt; video https://www.youtube.com/embed/oxUDZFgdo-U width="560" height="315" &gt;}}</span>
<span id="cb24-36"><a href="#cb24-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-37"><a href="#cb24-37" aria-hidden="true" tabindex="-1"></a>---</span>
<span id="cb24-38"><a href="#cb24-38" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb24-39"><a href="#cb24-39" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-40"><a href="#cb24-40" aria-hidden="true" tabindex="-1"></a><span class="fu">## Introducción {#sec-intro-10}</span></span>
<span id="cb24-41"><a href="#cb24-41" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-42"><a href="#cb24-42" aria-hidden="true" tabindex="-1"></a>Existen diferentes modelos de clasificación y regresión donde no es posible encontrar una solución analítica para estimar los parámetros, por ejemplo en Regresión Logística (@sec-regresion-logistica). Es en este escenario donde se voltea a métodos de optimización iterativos para calcular los parámetros. </span>
<span id="cb24-43"><a href="#cb24-43" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-44"><a href="#cb24-44" aria-hidden="true" tabindex="-1"></a>En esta unidad se describe posiblemente el método de optimización más conocido que es **Descenso de Gradiente**. Este método como su nombre lo indica utiliza el gradiente como su ingrediente principal; se describirá como se puede calcular el gradiente utilizando un método gráfico y como este método naturalmente realiza **Propagación hacia Atrás**.</span>
<span id="cb24-45"><a href="#cb24-45" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-46"><a href="#cb24-46" aria-hidden="true" tabindex="-1"></a><span class="fu">## Descenso por Gradiente {#sec-descenso-gradiente}</span></span>
<span id="cb24-47"><a href="#cb24-47" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-48"><a href="#cb24-48" aria-hidden="true" tabindex="-1"></a>En un modelo de clasificación y regresión interesa encontrar un vector de parámetros, $\mathbf w^{*},$ que minimicen una función de error, $E,$ de la siguiente manera:</span>
<span id="cb24-49"><a href="#cb24-49" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-50"><a href="#cb24-50" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb24-51"><a href="#cb24-51" aria-hidden="true" tabindex="-1"></a>\mathbf w^{*} = \textsf{argmin}_{\mathbf w} E(\mathbf w \mid \mathcal D).</span>
<span id="cb24-52"><a href="#cb24-52" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb24-53"><a href="#cb24-53" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-54"><a href="#cb24-54" aria-hidden="true" tabindex="-1"></a>En el caso de que $E(\mathbf w \mid \mathcal D)$ sea una función diferenciable, el gradiente está dado por:</span>
<span id="cb24-55"><a href="#cb24-55" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-56"><a href="#cb24-56" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb24-57"><a href="#cb24-57" aria-hidden="true" tabindex="-1"></a>\nabla_{\mathbf w} E(\mathbf w \mid \mathcal D) = <span class="co">[</span><span class="ot">\frac{\partial E}{\partial \mathbf w_1}, \frac{\partial E}{\partial \mathbf w_2}, \ldots</span><span class="co">]</span>^\intercal.</span>
<span id="cb24-58"><a href="#cb24-58" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb24-59"><a href="#cb24-59" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-60"><a href="#cb24-60" aria-hidden="true" tabindex="-1"></a>La idea general es tomar la dirección opuesta al gradiente para encontrar el mínimo de la función. Entonces el cambio de parámetro está dado por </span>
<span id="cb24-61"><a href="#cb24-61" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-62"><a href="#cb24-62" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb24-63"><a href="#cb24-63" aria-hidden="true" tabindex="-1"></a>\begin{split}</span>
<span id="cb24-64"><a href="#cb24-64" aria-hidden="true" tabindex="-1"></a>\Delta \mathbf w &amp;= - \eta \nabla_{\mathbf w} E<span class="sc">\\</span></span>
<span id="cb24-65"><a href="#cb24-65" aria-hidden="true" tabindex="-1"></a>       \mathbf w^{t+1} &amp;= \mathbf w^{t-1} + \Delta \mathbf w <span class="sc">\\</span></span>
<span id="cb24-66"><a href="#cb24-66" aria-hidden="true" tabindex="-1"></a>       &amp;= \mathbf w^{t-1} - \eta \nabla_{\mathbf w} E</span>
<span id="cb24-67"><a href="#cb24-67" aria-hidden="true" tabindex="-1"></a>\end{split}</span>
<span id="cb24-68"><a href="#cb24-68" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb24-69"><a href="#cb24-69" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-70"><a href="#cb24-70" aria-hidden="true" tabindex="-1"></a><span class="fu">### Ejemplo - Regresión Lineal {#sec-regresion-lineal-manual}</span></span>
<span id="cb24-71"><a href="#cb24-71" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-72"><a href="#cb24-72" aria-hidden="true" tabindex="-1"></a>Suponiendo que se quieren estimar los parámetros de la siguiente ecuación lineal: $f(x) = a x + b,$ para lo cual se tiene un conjunto de entrenamiento en el intervalo $x=<span class="co">[</span><span class="ot">-10, 10</span><span class="co">]</span>,$ generado con los parámetros $a=2.3$ y $b=-3.$ Importante no olvidar que los parámetros $a=2.3$ y $b=-3$ son desconocidos y se quieren estimar usando **Descenso por Gradiente** y también es importante mencionar que para este problema en particular es posible tener una solución analítica para estimar los parámetros. </span>
<span id="cb24-73"><a href="#cb24-73" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-74"><a href="#cb24-74" aria-hidden="true" tabindex="-1"></a>El primer paso es definir la función de error $E(a, b \mid \mathcal D),$ en problemas de regresión una función de error viable es: $E(a, b \mid \mathcal D) = \sum_{(x, y) \in \mathcal D} (y - f(x))^2.$</span>
<span id="cb24-75"><a href="#cb24-75" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-76"><a href="#cb24-76" aria-hidden="true" tabindex="-1"></a>La regla para actualizar los valores iniciales es: $w = w - \eta \nabla_w E$; por lo que se procede a calcular $\nabla_w E$ donde $w$ corresponde a los parámetros $a$ y $b$. </span>
<span id="cb24-77"><a href="#cb24-77" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-78"><a href="#cb24-78" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb24-79"><a href="#cb24-79" aria-hidden="true" tabindex="-1"></a>\begin{split}</span>
<span id="cb24-80"><a href="#cb24-80" aria-hidden="true" tabindex="-1"></a>    \frac{\partial E}{\partial w} &amp;= \frac{\partial}{\partial w} \sum (y - f(x))^2 <span class="sc">\\</span></span>
<span id="cb24-81"><a href="#cb24-81" aria-hidden="true" tabindex="-1"></a>    &amp;= 2 \sum (y - f(x)) \frac{\partial}{\partial w} (y - f(x)) <span class="sc">\\</span></span>
<span id="cb24-82"><a href="#cb24-82" aria-hidden="true" tabindex="-1"></a>    &amp;= - 2 \sum (y - f(x)) \frac{\partial}{\partial w} f(x)</span>
<span id="cb24-83"><a href="#cb24-83" aria-hidden="true" tabindex="-1"></a>\end{split}</span>
<span id="cb24-84"><a href="#cb24-84" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb24-85"><a href="#cb24-85" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-86"><a href="#cb24-86" aria-hidden="true" tabindex="-1"></a>donde $\frac{\partial}{\partial a} f(x) = x$ y $\frac{\partial}{\partial b} f(x) = 1.0$ Las ecuaciones para actualizar $a$ y $b$ serían:</span>
<span id="cb24-87"><a href="#cb24-87" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-88"><a href="#cb24-88" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb24-89"><a href="#cb24-89" aria-hidden="true" tabindex="-1"></a>\begin{split}</span>
<span id="cb24-90"><a href="#cb24-90" aria-hidden="true" tabindex="-1"></a>    e(y, x) &amp;= y - f(x) <span class="sc">\\</span></span>
<span id="cb24-91"><a href="#cb24-91" aria-hidden="true" tabindex="-1"></a>    a &amp;= a + 2 \eta \sum_{(x, y) \in \mathcal D} e(y, x) x <span class="sc">\\</span>  </span>
<span id="cb24-92"><a href="#cb24-92" aria-hidden="true" tabindex="-1"></a>    b &amp;= b + 2 \eta \sum_{(x, y) \in \mathcal D} e(y, x)</span>
<span id="cb24-93"><a href="#cb24-93" aria-hidden="true" tabindex="-1"></a>\end{split}</span>
<span id="cb24-94"><a href="#cb24-94" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb24-95"><a href="#cb24-95" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-96"><a href="#cb24-96" aria-hidden="true" tabindex="-1"></a>Con el objetivo de visualizar descenso por gradiente y completar el ejemplo anterior, el siguiente código implementa el proceso de optimización mencionado. Lo primero es generar el conjunto de entrenamiento.</span>
<span id="cb24-97"><a href="#cb24-97" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-100"><a href="#cb24-100" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb24-101"><a href="#cb24-101" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: true</span></span>
<span id="cb24-102"><a href="#cb24-102" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> np.linspace(<span class="op">-</span><span class="dv">10</span>, <span class="dv">10</span>, <span class="dv">50</span>)</span>
<span id="cb24-103"><a href="#cb24-103" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> <span class="fl">2.3</span> <span class="op">*</span> x <span class="op">-</span> <span class="dv">3</span></span>
<span id="cb24-104"><a href="#cb24-104" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb24-105"><a href="#cb24-105" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-106"><a href="#cb24-106" aria-hidden="true" tabindex="-1"></a>El proceso inicia con valores aleatorios de $a$ y $b$, estos valores podrían ser $5.3$ y $-5.1,$ además se utilizará una $\eta=0.0001$, se guardarán todos los puntos visitados en la lista <span class="in">`D`</span>. Las variables y valores iniciales quedarían como:</span>
<span id="cb24-107"><a href="#cb24-107" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-110"><a href="#cb24-110" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb24-111"><a href="#cb24-111" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: true</span></span>
<span id="cb24-112"><a href="#cb24-112" aria-hidden="true" tabindex="-1"></a>a <span class="op">=</span> <span class="fl">5.3</span></span>
<span id="cb24-113"><a href="#cb24-113" aria-hidden="true" tabindex="-1"></a>b <span class="op">=</span> <span class="op">-</span><span class="fl">5.1</span></span>
<span id="cb24-114"><a href="#cb24-114" aria-hidden="true" tabindex="-1"></a>delta <span class="op">=</span> np.inf</span>
<span id="cb24-115"><a href="#cb24-115" aria-hidden="true" tabindex="-1"></a>eta <span class="op">=</span> <span class="fl">0.0001</span></span>
<span id="cb24-116"><a href="#cb24-116" aria-hidden="true" tabindex="-1"></a>D <span class="op">=</span> [(a, b)]</span>
<span id="cb24-117"><a href="#cb24-117" aria-hidden="true" tabindex="-1"></a><span class="in">```</span> </span>
<span id="cb24-118"><a href="#cb24-118" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-119"><a href="#cb24-119" aria-hidden="true" tabindex="-1"></a>El siguiente ciclo realiza la iteración del proceso de optimización y se detienen cuando los valores estimados varían poco entre dos iteraciones consecutivas, en particular cuando en promedio el cambio en las constantes sea menor a $0.0001$. </span>
<span id="cb24-120"><a href="#cb24-120" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-123"><a href="#cb24-123" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb24-124"><a href="#cb24-124" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: true</span></span>
<span id="cb24-125"><a href="#cb24-125" aria-hidden="true" tabindex="-1"></a><span class="cf">while</span> delta <span class="op">&gt;</span> <span class="fl">0.0001</span>:</span>
<span id="cb24-126"><a href="#cb24-126" aria-hidden="true" tabindex="-1"></a>    hy <span class="op">=</span> a <span class="op">*</span> x <span class="op">+</span> b</span>
<span id="cb24-127"><a href="#cb24-127" aria-hidden="true" tabindex="-1"></a>    e <span class="op">=</span> (y <span class="op">-</span> hy)</span>
<span id="cb24-128"><a href="#cb24-128" aria-hidden="true" tabindex="-1"></a>    a <span class="op">=</span> a <span class="op">+</span> <span class="dv">2</span> <span class="op">*</span> eta <span class="op">*</span> (e <span class="op">*</span> x).<span class="bu">sum</span>()</span>
<span id="cb24-129"><a href="#cb24-129" aria-hidden="true" tabindex="-1"></a>    b <span class="op">=</span> b <span class="op">+</span> <span class="dv">2</span> <span class="op">*</span> eta <span class="op">*</span> e.<span class="bu">sum</span>()</span>
<span id="cb24-130"><a href="#cb24-130" aria-hidden="true" tabindex="-1"></a>    D.append((a, b))</span>
<span id="cb24-131"><a href="#cb24-131" aria-hidden="true" tabindex="-1"></a>    delta <span class="op">=</span> np.fabs(np.array(D[<span class="op">-</span><span class="dv">1</span>]) <span class="op">-</span> np.array(D[<span class="op">-</span><span class="dv">2</span>])).mean()</span>
<span id="cb24-132"><a href="#cb24-132" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb24-133"><a href="#cb24-133" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-134"><a href="#cb24-134" aria-hidden="true" tabindex="-1"></a>En la @fig-optimizacion-evolucion-desc se muestra el camino que siguieron los parámetros hasta llegar a los parámetros que generaron el problema. Los parámetros que generaron el problema se encuentran marcados en negro. </span>
<span id="cb24-135"><a href="#cb24-135" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-138"><a href="#cb24-138" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb24-139"><a href="#cb24-139" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-fold: true</span></span>
<span id="cb24-140"><a href="#cb24-140" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: Camino de los parámetros en el proceso de optimización.</span></span>
<span id="cb24-141"><a href="#cb24-141" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-optimizacion-evolucion-desc</span></span>
<span id="cb24-142"><a href="#cb24-142" aria-hidden="true" tabindex="-1"></a>df <span class="op">=</span> pd.DataFrame(np.array(D), columns<span class="op">=</span>[<span class="st">'a'</span>, <span class="st">'b'</span>])</span>
<span id="cb24-143"><a href="#cb24-143" aria-hidden="true" tabindex="-1"></a>_ <span class="op">=</span> np.atleast_2d(np.log(np.arange(<span class="dv">1</span>, df.shape[<span class="dv">0</span>] <span class="op">+</span> <span class="dv">1</span>))).T</span>
<span id="cb24-144"><a href="#cb24-144" aria-hidden="true" tabindex="-1"></a>df[<span class="st">'iteración %'</span>] <span class="op">=</span> minmax_scale(_).flatten()</span>
<span id="cb24-145"><a href="#cb24-145" aria-hidden="true" tabindex="-1"></a>sns.relplot(data<span class="op">=</span>df, kind<span class="op">=</span><span class="st">'scatter'</span>, x<span class="op">=</span><span class="st">'a'</span>, y<span class="op">=</span><span class="st">'b'</span>, hue<span class="op">=</span><span class="st">'iteración %'</span>)</span>
<span id="cb24-146"><a href="#cb24-146" aria-hidden="true" tabindex="-1"></a>_ <span class="op">=</span> plt.tight_layout()</span>
<span id="cb24-147"><a href="#cb24-147" aria-hidden="true" tabindex="-1"></a><span class="in">```</span>  </span>
<span id="cb24-148"><a href="#cb24-148" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-149"><a href="#cb24-149" aria-hidden="true" tabindex="-1"></a><span class="fu">## Diferenciación Automática {#sec-diferenciacion-automatica}</span></span>
<span id="cb24-150"><a href="#cb24-150" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-151"><a href="#cb24-151" aria-hidden="true" tabindex="-1"></a>No en todos los casos es posible encontrar una ecuación cerrada para actualizar los parámetros; también el trabajo manual que se requiere para encontrar la solución analítica es considerable y depende de la complejidad de la función objetivo que se quiere derivar. Observando el procedimiento para actualizar los parámetros, i.e., $\mathbf w^{t+1} = w^{t-1} - \eta \nabla_{\mathbf w} E$, se concluye que se requiere evaluar $\nabla_{\mathbf w} E$ en un punto en particular, entonces, para solucionar el problema, es suficiente contar con un procedimiento que permita conocer el valor de $\nabla_{\mathbf w} E$ en cualquier punto deseado, y en particular no es necesario conocer la solución analítica de $\nabla_{\mathbf w} E.$ Al procedimiento que encuentra el valor de la derivada de cualquier función en un punto de manera automática se le conoce como diferenciación automática.</span>
<span id="cb24-152"><a href="#cb24-152" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-153"><a href="#cb24-153" aria-hidden="true" tabindex="-1"></a><span class="fu">### Una Variable</span></span>
<span id="cb24-154"><a href="#cb24-154" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-155"><a href="#cb24-155" aria-hidden="true" tabindex="-1"></a>Se puede entender el proceso de diferenciación automática siguiendo un par de ejemplos. Sea $f(x) = x^2,$ en este caso $f'(x) = 2x.$ Ahora considerando que se quiere evaluar $f'(2.5)$, se sabe que $f'(2.5)=5,$  pero también se puede generar un programa donde en la primera fase se calcule $f(2.5)=(2.5)^2$ y al momento de calcular $f$ se guarde en un espacio asociado a $f$ el valor de $f'(2.5).$ es decir $5$. Se puede observar que este procedimiento soluciona este problema simple para cualquier función de una variable. </span>
<span id="cb24-156"><a href="#cb24-156" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-157"><a href="#cb24-157" aria-hidden="true" tabindex="-1"></a>La librería <span class="co">[</span><span class="ot">JAX</span><span class="co">](https://jax.readthedocs.io)</span> permite hacer diferenciación automática en Python. Siguiendo el ejemplo anterior, se genera la función $f(x) = x^2$ con el siguiente código</span>
<span id="cb24-158"><a href="#cb24-158" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-161"><a href="#cb24-161" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb24-162"><a href="#cb24-162" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: true</span></span>
<span id="cb24-163"><a href="#cb24-163" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> sq(x):</span>
<span id="cb24-164"><a href="#cb24-164" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> x<span class="op">**</span><span class="dv">2</span></span>
<span id="cb24-165"><a href="#cb24-165" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb24-166"><a href="#cb24-166" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-167"><a href="#cb24-167" aria-hidden="true" tabindex="-1"></a>Esta función se puede evaluar como <span class="in">`sq(2.5)`</span>; lo interesante es que <span class="in">`sq`</span> se puede componer con la función <span class="in">`grad`</span> para calcular la derivada de la siguiente manera. </span>
<span id="cb24-168"><a href="#cb24-168" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-171"><a href="#cb24-171" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb24-172"><a href="#cb24-172" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: true</span></span>
<span id="cb24-173"><a href="#cb24-173" aria-hidden="true" tabindex="-1"></a>res <span class="op">=</span> grad(sq)(<span class="fl">2.5</span>)</span>
<span id="cb24-174"><a href="#cb24-174" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb24-175"><a href="#cb24-175" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-176"><a href="#cb24-176" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-179"><a href="#cb24-179" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb24-180"><a href="#cb24-180" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: false</span></span>
<span id="cb24-181"><a href="#cb24-181" aria-hidden="true" tabindex="-1"></a>res_f <span class="op">=</span> Markdown(<span class="ss">f'$</span><span class="sc">{</span><span class="bu">int</span>(res)<span class="sc">}</span><span class="ss">$'</span>)</span>
<span id="cb24-182"><a href="#cb24-182" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb24-183"><a href="#cb24-183" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-184"><a href="#cb24-184" aria-hidden="true" tabindex="-1"></a>Ejecutando el código anterior se obtiene <span class="in">`{python} res_f`</span>, <span class="in">`grad`</span> internamente guarda <span class="in">`{python} res_f`</span>, <span class="in">`grad`</span> asociado a <span class="in">`sq`</span>. </span>
<span id="cb24-185"><a href="#cb24-185" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-186"><a href="#cb24-186" aria-hidden="true" tabindex="-1"></a>Ahora en el caso de una composición, por ejemplo, sea $g(x)=\sin(x)$ y se desea conocer $\frac{d}{dx} g(f(x)),$ siguiendo el mismo principio primero se evalúa $f(2.5)$ y se guarda asociado a $f$ el valor de $5$ que corresponde a $f',$ después se evalúa $g(5)$ y se guarda asociado a $g$ el valor $\cos(2.5^2).$ Ahora para conocer el valor de $\frac{d}{dx} g(f(2.5))$ se camina en el sentido inverso multiplicando todos los valores guardados, es decir, se multiplica $5 \times \cos(2.5^2).$ Implementando este ejemplo en Python quedaría como:</span>
<span id="cb24-187"><a href="#cb24-187" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-190"><a href="#cb24-190" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb24-191"><a href="#cb24-191" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: true</span></span>
<span id="cb24-192"><a href="#cb24-192" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> sin_sq(x):</span>
<span id="cb24-193"><a href="#cb24-193" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> jnp.sin(sq(x))</span>
<span id="cb24-194"><a href="#cb24-194" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb24-195"><a href="#cb24-195" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-198"><a href="#cb24-198" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb24-199"><a href="#cb24-199" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: false</span></span>
<span id="cb24-200"><a href="#cb24-200" aria-hidden="true" tabindex="-1"></a>grad_f <span class="op">=</span> Markdown(<span class="ss">f'$</span><span class="sc">{</span><span class="bu">float</span>(grad(sin_sq)(<span class="fl">2.5</span>))<span class="sc">:0.4f}</span><span class="ss">$'</span>)</span>
<span id="cb24-201"><a href="#cb24-201" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb24-202"><a href="#cb24-202" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-203"><a href="#cb24-203" aria-hidden="true" tabindex="-1"></a>donde la función <span class="in">`sin_sq`</span> tiene la composición de $g \circ f.$ Ahora la derivada de esta composición en 2.5 se obtiene como <span class="in">`grad(sin_sq)(2.5)`</span> obteniendo un valor de <span class="in">`{python} grad_f`</span>.</span>
<span id="cb24-204"><a href="#cb24-204" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-205"><a href="#cb24-205" aria-hidden="true" tabindex="-1"></a><span class="fu">### Dos Variables</span></span>
<span id="cb24-206"><a href="#cb24-206" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-207"><a href="#cb24-207" aria-hidden="true" tabindex="-1"></a>El ejemplo anterior se puede extender para cualquier composición de funciones, pero ese ejemplo no deja claro lo que pasaría con una función de dos o más variables, como por ejemplo, $h(x, y) = x \times y.$ En este caso, el objetivo es calcular tanto $\frac{d}{dx} h$ como $\frac{d}{dy} h.$</span>
<span id="cb24-208"><a href="#cb24-208" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-209"><a href="#cb24-209" aria-hidden="true" tabindex="-1"></a>El procedimiento es similar, con el ingrediente extra de que se tiene que recordar la función y la posición del argumento, es decir, para $h(2, 5)$ se asocia el valor $\frac{d}{dx} h$ con el primer argumento de la función $h$ y $\frac{d}{dx} h$ con el segundo argumento de la función $h$. Para $h'(2, 5)$ se asocia el valor de $5$ </span>
<span id="cb24-210"><a href="#cb24-210" aria-hidden="true" tabindex="-1"></a>con el primer argumento de $h$ y $2$ con el segundo argumento de $h.$</span>
<span id="cb24-211"><a href="#cb24-211" aria-hidden="true" tabindex="-1"></a>Se puede observar que los valores guardados corresponden a $\frac{d}{dx}h(2, 5)=y=5$ y $\frac{d}{dy}h(2, 5)=x=2.$ Escribiendo este ejemplo en Python quedaría como </span>
<span id="cb24-212"><a href="#cb24-212" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-215"><a href="#cb24-215" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb24-216"><a href="#cb24-216" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: true</span></span>
<span id="cb24-217"><a href="#cb24-217" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> prod(x):</span>
<span id="cb24-218"><a href="#cb24-218" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> x[<span class="dv">0</span>] <span class="op">*</span> x[<span class="dv">1</span>]</span>
<span id="cb24-219"><a href="#cb24-219" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb24-220"><a href="#cb24-220" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-221"><a href="#cb24-221" aria-hidden="true" tabindex="-1"></a>calculando la derivada como <span class="in">`grad(prod)(jnp.array([2., 5.]))`</span> se obtiene  <span class="in">`[5., 2.]`</span> que corresponde a la derivada parcial en cada argumento. </span>
<span id="cb24-222"><a href="#cb24-222" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-223"><a href="#cb24-223" aria-hidden="true" tabindex="-1"></a><span class="fu">### Visualización</span></span>
<span id="cb24-224"><a href="#cb24-224" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-225"><a href="#cb24-225" aria-hidden="true" tabindex="-1"></a>Una manera de visualizar este proceso de diferenciación automática es poniéndolo en un árbol de expresión, en la @fig-optimizacion-arbol-expresion se muestra la ecuación $ax^2+bx+c$. Dentro de cada nodo se observa el valor que se tiene que guardar para cualquier valor de $a,b,c$ y $x.$ </span>
<span id="cb24-226"><a href="#cb24-226" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-227"><a href="#cb24-227" aria-hidden="true" tabindex="-1"></a><span class="al">![Árbol de expresión](/docs/assets/images/eval.png)</span>{#fig-optimizacion-arbol-expresion}</span>
<span id="cb24-228"><a href="#cb24-228" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-229"><a href="#cb24-229" aria-hidden="true" tabindex="-1"></a><span class="fu">### Regresión Lineal</span></span>
<span id="cb24-230"><a href="#cb24-230" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-231"><a href="#cb24-231" aria-hidden="true" tabindex="-1"></a>En esta sección se realiza utilizando diferenciación automática el ejemplo de regresión lineal (@sec-regresion-lineal-manual). Lo primero que se tiene que hacer es definir la función para la cual se quieren optimizar los parámetros. Esta función es $ax + b$ la cual se define con el siguiente código. El primer argumento de la función son los parámetros y después viene los argumentos de la función que en este caso son los valores de $x.$</span>
<span id="cb24-232"><a href="#cb24-232" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-235"><a href="#cb24-235" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb24-236"><a href="#cb24-236" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: true</span></span>
<span id="cb24-237"><a href="#cb24-237" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> func(params, x):</span>
<span id="cb24-238"><a href="#cb24-238" aria-hidden="true" tabindex="-1"></a>    a, b <span class="op">=</span> params</span>
<span id="cb24-239"><a href="#cb24-239" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> a <span class="op">*</span> x <span class="op">+</span> b</span>
<span id="cb24-240"><a href="#cb24-240" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb24-241"><a href="#cb24-241" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-242"><a href="#cb24-242" aria-hidden="true" tabindex="-1"></a>El siguiente paso es definir la función de error, en este caso la función de error definida previamente es $\sum_{i=1}^{N} (y_i - \hat y_i)^2$ tal y como se muestra en la siguiente función. </span>
<span id="cb24-243"><a href="#cb24-243" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-246"><a href="#cb24-246" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb24-247"><a href="#cb24-247" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: true</span></span>
<span id="cb24-248"><a href="#cb24-248" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> error(params, x, y):</span>
<span id="cb24-249"><a href="#cb24-249" aria-hidden="true" tabindex="-1"></a>    hy <span class="op">=</span> func(params, x)</span>
<span id="cb24-250"><a href="#cb24-250" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> lax.fori_loop(<span class="dv">0</span>, x.shape[<span class="dv">0</span>],</span>
<span id="cb24-251"><a href="#cb24-251" aria-hidden="true" tabindex="-1"></a>                         <span class="kw">lambda</span> i, acc: acc <span class="op">+</span> (y[i] <span class="op">-</span> hy[i])<span class="op">**</span><span class="dv">2</span>,</span>
<span id="cb24-252"><a href="#cb24-252" aria-hidden="true" tabindex="-1"></a>                         <span class="dv">0</span>)</span>
<span id="cb24-253"><a href="#cb24-253" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb24-254"><a href="#cb24-254" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-255"><a href="#cb24-255" aria-hidden="true" tabindex="-1"></a>Finalmente, se actualizan los parámetros siguiendo un procedimiento equivalente al mostrado anteriormente. La primera linea define el valor de $\eta,$ el valor de la variable <span class="in">`delta`</span> indica el término del ciclo, la tercera línea define los parámetros iniciales (<span class="in">`params`</span>), la siguiente guarda los todos los puntos visitados (cuarta línea) y la quinta línea deriva la función de <span class="in">`error`</span> para calcular el gradiente. La primera instrucción dentro del ciclo actualiza los parámetros.  </span>
<span id="cb24-256"><a href="#cb24-256" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-259"><a href="#cb24-259" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb24-260"><a href="#cb24-260" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: true</span></span>
<span id="cb24-261"><a href="#cb24-261" aria-hidden="true" tabindex="-1"></a>eta <span class="op">=</span> <span class="fl">0.0001</span></span>
<span id="cb24-262"><a href="#cb24-262" aria-hidden="true" tabindex="-1"></a>delta <span class="op">=</span> np.inf</span>
<span id="cb24-263"><a href="#cb24-263" aria-hidden="true" tabindex="-1"></a>params <span class="op">=</span> jnp.array([<span class="fl">5.3</span>, <span class="op">-</span><span class="fl">5.1</span>])</span>
<span id="cb24-264"><a href="#cb24-264" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> jnp.array(x)</span>
<span id="cb24-265"><a href="#cb24-265" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> jnp.array(y)</span>
<span id="cb24-266"><a href="#cb24-266" aria-hidden="true" tabindex="-1"></a>D <span class="op">=</span> [params]</span>
<span id="cb24-267"><a href="#cb24-267" aria-hidden="true" tabindex="-1"></a>error_grad <span class="op">=</span> grad(error)</span>
<span id="cb24-268"><a href="#cb24-268" aria-hidden="true" tabindex="-1"></a><span class="cf">while</span> delta <span class="op">&gt;</span> <span class="fl">0.0001</span>:</span>
<span id="cb24-269"><a href="#cb24-269" aria-hidden="true" tabindex="-1"></a>    params <span class="op">-=</span> eta <span class="op">*</span> error_grad(params, x, y)</span>
<span id="cb24-270"><a href="#cb24-270" aria-hidden="true" tabindex="-1"></a>    D.append(params)</span>
<span id="cb24-271"><a href="#cb24-271" aria-hidden="true" tabindex="-1"></a>    delta <span class="op">=</span> jnp.fabs(D[<span class="op">-</span><span class="dv">1</span>] <span class="op">-</span> D[<span class="op">-</span><span class="dv">2</span>]).mean()</span>
<span id="cb24-272"><a href="#cb24-272" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb24-273"><a href="#cb24-273" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-274"><a href="#cb24-274" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-275"><a href="#cb24-275" aria-hidden="true" tabindex="-1"></a><span class="fu">## Regresión Logística {#sec-regresion-logistica-optimizacion}</span></span>
<span id="cb24-276"><a href="#cb24-276" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-277"><a href="#cb24-277" aria-hidden="true" tabindex="-1"></a>En esta sección, se presenta la implementación de Regresión Logística (@sec-regresion-logistica) utilizando diferenciación automática. </span>
<span id="cb24-278"><a href="#cb24-278" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-279"><a href="#cb24-279" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-280"><a href="#cb24-280" aria-hidden="true" tabindex="-1"></a>El método se probará en los datos generados por dos normales en $\mathbb R^2$ que se generan con las siguientes instrucciones. </span>
<span id="cb24-281"><a href="#cb24-281" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-284"><a href="#cb24-284" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb24-285"><a href="#cb24-285" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: true</span></span>
<span id="cb24-286"><a href="#cb24-286" aria-hidden="true" tabindex="-1"></a>X_1 <span class="op">=</span> multivariate_normal(mean<span class="op">=</span>[<span class="dv">15</span>, <span class="dv">20</span>], cov<span class="op">=</span>[[<span class="dv">3</span>, <span class="op">-</span><span class="dv">3</span>], [<span class="op">-</span><span class="dv">3</span>, <span class="dv">8</span>]]).rvs(<span class="dv">1000</span>)</span>
<span id="cb24-287"><a href="#cb24-287" aria-hidden="true" tabindex="-1"></a>X_2 <span class="op">=</span> multivariate_normal(mean<span class="op">=</span>[<span class="dv">8</span>, <span class="dv">8</span>], cov<span class="op">=</span>[[<span class="dv">4</span>, <span class="dv">0</span>], [<span class="dv">0</span>, <span class="dv">2</span>]]).rvs(<span class="dv">1000</span>)</span>
<span id="cb24-288"><a href="#cb24-288" aria-hidden="true" tabindex="-1"></a>T <span class="op">=</span> np.concatenate((X_1, X_2))</span>
<span id="cb24-289"><a href="#cb24-289" aria-hidden="true" tabindex="-1"></a>normalize <span class="op">=</span> StandardScaler().fit(T)</span>
<span id="cb24-290"><a href="#cb24-290" aria-hidden="true" tabindex="-1"></a>T <span class="op">=</span> jnp.array(normalize.transform(T))</span>
<span id="cb24-291"><a href="#cb24-291" aria-hidden="true" tabindex="-1"></a>y_t <span class="op">=</span> jnp.array([<span class="dv">1</span>] <span class="op">*</span> X_1.shape[<span class="dv">0</span>] <span class="op">+</span> [<span class="dv">0</span>] <span class="op">*</span> X_2.shape[<span class="dv">0</span>])</span>
<span id="cb24-292"><a href="#cb24-292" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb24-293"><a href="#cb24-293" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-294"><a href="#cb24-294" aria-hidden="true" tabindex="-1"></a>Siguiendo los pasos utilizados en el ejemplo anterior, la primera función a implementar es el model, es decir la función sigmoide, i.e., $\textsf{sigmoid}(\mathbf x) = \frac{1}{1 + \exp(- (\mathbf w \cdot \mathbf x + w_0))}.$</span>
<span id="cb24-295"><a href="#cb24-295" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-298"><a href="#cb24-298" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb24-299"><a href="#cb24-299" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: true</span></span>
<span id="cb24-300"><a href="#cb24-300" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> modelo(params, x):</span>
<span id="cb24-301"><a href="#cb24-301" aria-hidden="true" tabindex="-1"></a>    _ <span class="op">=</span> jnp.exp(<span class="op">-</span>(jnp.dot(x, params[:<span class="dv">2</span>]) <span class="op">+</span> params[<span class="op">-</span><span class="dv">1</span>]))</span>
<span id="cb24-302"><a href="#cb24-302" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="dv">1</span> <span class="op">/</span> (<span class="dv">1</span> <span class="op">+</span> _)</span>
<span id="cb24-303"><a href="#cb24-303" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb24-304"><a href="#cb24-304" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-305"><a href="#cb24-305" aria-hidden="true" tabindex="-1"></a>El siguiente paso es implementar la función de error que guía el proceso de optimización, en el caso de regresión logística de dos clases la función de error corresponde a la media de la Entropía Cruzada (@eq-lineal-entropia-cruzada). Esta función se implementa en dos pasos, primero se calcula la entropía cruzada usando el siguiente procedimiento. Donde la primera línea verifica selecciona si se trata de la clase positiva ($1$) o de la clase negativa $0$ y calcula el $\log \hat y$ o $\log (1 - \hat y)$ respectivamente.</span>
<span id="cb24-306"><a href="#cb24-306" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-309"><a href="#cb24-309" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb24-310"><a href="#cb24-310" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: true</span></span>
<span id="cb24-311"><a href="#cb24-311" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> entropia_cruzada(y, hy):</span>
<span id="cb24-312"><a href="#cb24-312" aria-hidden="true" tabindex="-1"></a>    _ <span class="op">=</span> lax.cond(y <span class="op">==</span> <span class="dv">1</span>, <span class="kw">lambda</span> w: jnp.log(w), <span class="kw">lambda</span> w: jnp.log(<span class="dv">1</span> <span class="op">-</span> w), hy)</span>
<span id="cb24-313"><a href="#cb24-313" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> lax.cond(_ <span class="op">==</span> <span class="op">-</span>jnp.inf, <span class="kw">lambda</span> w: jnp.log(<span class="fl">1e-6</span>), <span class="kw">lambda</span> w: w, _)</span>
<span id="cb24-314"><a href="#cb24-314" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb24-315"><a href="#cb24-315" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-316"><a href="#cb24-316" aria-hidden="true" tabindex="-1"></a>El segundo paso es calcular la media de la entropía cruzada de cada elemento de $\mathcal D,$ esto se puede realizar con la siguiente función. </span>
<span id="cb24-317"><a href="#cb24-317" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-320"><a href="#cb24-320" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb24-321"><a href="#cb24-321" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: true</span></span>
<span id="cb24-322"><a href="#cb24-322" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> suma_entropia_cruzada(params, x, y):</span>
<span id="cb24-323"><a href="#cb24-323" aria-hidden="true" tabindex="-1"></a>    hy <span class="op">=</span> modelo(params, x)</span>
<span id="cb24-324"><a href="#cb24-324" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="op">-</span> lax.fori_loop(<span class="dv">0</span>, y.shape[<span class="dv">0</span>],</span>
<span id="cb24-325"><a href="#cb24-325" aria-hidden="true" tabindex="-1"></a>                           <span class="kw">lambda</span> i, x: x <span class="op">+</span> entropia_cruzada(y[i], hy[i]),</span>
<span id="cb24-326"><a href="#cb24-326" aria-hidden="true" tabindex="-1"></a>                           <span class="dv">1</span>) <span class="op">/</span> y.shape[<span class="dv">0</span>]</span>
<span id="cb24-327"><a href="#cb24-327" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb24-328"><a href="#cb24-328" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-329"><a href="#cb24-329" aria-hidden="true" tabindex="-1"></a>El paso final es actualizar los parámetros, en esta ocasión se utilizará la función <span class="in">`value_and_grad`</span> que regresa la evaluación de la función así como la derivada, esto para poder desplegar como el error disminuye con el paso de las iteraciones. El código es similar al mostrado anteriormente. La diferencias son la forma en generar los parámetros iniciales, primeras tres líneas, el parámetro $\eta$, que se itera por $n=5000$ iteraciones y la función <span class="in">`value_and_grad`</span>.</span>
<span id="cb24-330"><a href="#cb24-330" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-333"><a href="#cb24-333" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb24-334"><a href="#cb24-334" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: true</span></span>
<span id="cb24-335"><a href="#cb24-335" aria-hidden="true" tabindex="-1"></a>key <span class="op">=</span> random.PRNGKey(<span class="dv">0</span>)</span>
<span id="cb24-336"><a href="#cb24-336" aria-hidden="true" tabindex="-1"></a>key, subkey <span class="op">=</span> random.split(key)</span>
<span id="cb24-337"><a href="#cb24-337" aria-hidden="true" tabindex="-1"></a>params <span class="op">=</span> random.normal(subkey, (<span class="dv">3</span>,)) <span class="op">*</span> jnp.sqrt(<span class="dv">2</span> <span class="op">/</span> <span class="dv">2</span>)</span>
<span id="cb24-338"><a href="#cb24-338" aria-hidden="true" tabindex="-1"></a>eta <span class="op">=</span> <span class="fl">0.1</span></span>
<span id="cb24-339"><a href="#cb24-339" aria-hidden="true" tabindex="-1"></a>error <span class="op">=</span> []</span>
<span id="cb24-340"><a href="#cb24-340" aria-hidden="true" tabindex="-1"></a>error_grad  <span class="op">=</span> value_and_grad(suma_entropia_cruzada)</span>
<span id="cb24-341"><a href="#cb24-341" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">5000</span>):</span>
<span id="cb24-342"><a href="#cb24-342" aria-hidden="true" tabindex="-1"></a>    _, g <span class="op">=</span> error_grad(params, T, y_t)</span>
<span id="cb24-343"><a href="#cb24-343" aria-hidden="true" tabindex="-1"></a>    error.append(_)</span>
<span id="cb24-344"><a href="#cb24-344" aria-hidden="true" tabindex="-1"></a>    params <span class="op">-=</span> eta <span class="op">*</span> g</span>
<span id="cb24-345"><a href="#cb24-345" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb24-346"><a href="#cb24-346" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-347"><a href="#cb24-347" aria-hidden="true" tabindex="-1"></a>La @fig-optimizacion-descenso-ent-x muestra como se reduce la media de la entropía cruzada con respecto al número de iteraciones, este error sigue bajando aunque la velocidad disminuye drasticamente. </span>
<span id="cb24-348"><a href="#cb24-348" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-351"><a href="#cb24-351" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb24-352"><a href="#cb24-352" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-fold: true</span></span>
<span id="cb24-353"><a href="#cb24-353" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: Descenso de Entropía Cruzada</span></span>
<span id="cb24-354"><a href="#cb24-354" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-optimizacion-descenso-ent-x</span></span>
<span id="cb24-355"><a href="#cb24-355" aria-hidden="true" tabindex="-1"></a>df <span class="op">=</span> pd.DataFrame({<span class="st">'Iteración'</span>: np.arange(<span class="dv">5000</span>), <span class="st">'error'</span>: np.array(error)})</span>
<span id="cb24-356"><a href="#cb24-356" aria-hidden="true" tabindex="-1"></a>sns.relplot(data<span class="op">=</span>df, x<span class="op">=</span><span class="st">'Iteración'</span>, y<span class="op">=</span><span class="st">'error'</span>, kind<span class="op">=</span><span class="st">'line'</span>)</span>
<span id="cb24-357"><a href="#cb24-357" aria-hidden="true" tabindex="-1"></a>_ <span class="op">=</span> plt.tight_layout()</span>
<span id="cb24-358"><a href="#cb24-358" aria-hidden="true" tabindex="-1"></a><span class="in">```</span>  </span>
<span id="cb24-359"><a href="#cb24-359" aria-hidden="true" tabindex="-1"></a>&lt;/details&gt;</span>
<span id="cb24-360"><a href="#cb24-360" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-363"><a href="#cb24-363" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb24-364"><a href="#cb24-364" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: false</span></span>
<span id="cb24-365"><a href="#cb24-365" aria-hidden="true" tabindex="-1"></a>_ <span class="op">=</span> <span class="st">', '</span>.join([<span class="ss">f'</span><span class="sc">{</span>x<span class="sc">:0.4f}</span><span class="ss">'</span> <span class="cf">for</span> x <span class="kw">in</span> params])</span>
<span id="cb24-366"><a href="#cb24-366" aria-hidden="true" tabindex="-1"></a>params_f <span class="op">=</span> Markdown(<span class="ss">f'[</span><span class="sc">{</span>_<span class="sc">}</span><span class="ss">]'</span>)</span>
<span id="cb24-367"><a href="#cb24-367" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb24-368"><a href="#cb24-368" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-369"><a href="#cb24-369" aria-hidden="true" tabindex="-1"></a>Optimizando los parámetros de la regresión logística utilizando el procedimiento de diferenciación automática se obtiene <span class="in">`{python} params_f`</span>. </span>
<span id="cb24-370"><a href="#cb24-370" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-371"><a href="#cb24-371" aria-hidden="true" tabindex="-1"></a>En Discriminantes Lineales (@sec-discriminantes-lineales) se presento el procedimiento para entrenar un clasificador logístico usando la siguiente instrucción. </span>
<span id="cb24-372"><a href="#cb24-372" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-375"><a href="#cb24-375" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb24-376"><a href="#cb24-376" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: true</span></span>
<span id="cb24-377"><a href="#cb24-377" aria-hidden="true" tabindex="-1"></a>lr <span class="op">=</span> LogisticRegression().fit(T, y_t)</span>
<span id="cb24-378"><a href="#cb24-378" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb24-379"><a href="#cb24-379" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-382"><a href="#cb24-382" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb24-383"><a href="#cb24-383" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: false</span></span>
<span id="cb24-384"><a href="#cb24-384" aria-hidden="true" tabindex="-1"></a>pp <span class="op">=</span> np.r_[lr.coef_[<span class="dv">0</span>, <span class="dv">0</span>], lr.coef_[<span class="dv">0</span>, <span class="dv">1</span>], lr.intercept_[<span class="dv">0</span>]]</span>
<span id="cb24-385"><a href="#cb24-385" aria-hidden="true" tabindex="-1"></a>_ <span class="op">=</span> <span class="st">', '</span>.join([<span class="ss">f'</span><span class="sc">{</span>x<span class="sc">:0.4f}</span><span class="ss">'</span> <span class="cf">for</span> x <span class="kw">in</span> pp])</span>
<span id="cb24-386"><a href="#cb24-386" aria-hidden="true" tabindex="-1"></a>params_lr_f <span class="op">=</span> Markdown(<span class="ss">f'[</span><span class="sc">{</span>_<span class="sc">}</span><span class="ss">]'</span>)</span>
<span id="cb24-387"><a href="#cb24-387" aria-hidden="true" tabindex="-1"></a>error_f <span class="op">=</span> Markdown(<span class="ss">f'$</span><span class="sc">{</span>error[<span class="op">-</span><span class="dv">1</span>]<span class="sc">:0.4f}</span><span class="ss">$'</span>)</span>
<span id="cb24-388"><a href="#cb24-388" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb24-389"><a href="#cb24-389" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-390"><a href="#cb24-390" aria-hidden="true" tabindex="-1"></a>Los parámetros son <span class="in">`{python} params_lr_f`</span>. El error en el conjunto se entrenamiento es  se puede calcular de la siguiente manera. En comparación el error obtenido por diferenciación automática es <span class="in">`{python} error_f`</span>, por supuesto este puede variar cambiando el número de iteraciones. </span>
<span id="cb24-391"><a href="#cb24-391" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-394"><a href="#cb24-394" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb24-395"><a href="#cb24-395" aria-hidden="true" tabindex="-1"></a>_ <span class="op">=</span> jnp.array([lr.coef_[<span class="dv">0</span>, <span class="dv">0</span>], lr.coef_[<span class="dv">0</span>, <span class="dv">1</span>], lr.intercept_[<span class="dv">0</span>]])</span>
<span id="cb24-396"><a href="#cb24-396" aria-hidden="true" tabindex="-1"></a>suma_entropia_cruzada(_, T, y_t)</span>
<span id="cb24-397"><a href="#cb24-397" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb24-398"><a href="#cb24-398" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-399"><a href="#cb24-399" aria-hidden="true" tabindex="-1"></a><span class="fu">## Actualización de Parámetros {#sec-actualizacion-parametros}</span></span>
<span id="cb24-400"><a href="#cb24-400" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-401"><a href="#cb24-401" aria-hidden="true" tabindex="-1"></a>Por supuesto la regla $\mathbf w^{t+1} = \mathbf w^{t-1} - \eta \nabla_{\mathbf w} E$ para actualizar los parámetros $\mathbf w$ no es única y existe una gama de métodos que se pueden seleccionar dependiendo de las características del problema. En particular regresión logística es una problema de optimización convexo, en este tipo de problemas un algoritmos para encontrar los parámetros es el <span class="in">`BFGS`</span>. Por ejemplo el siguiente código utiliza este algoritmo para encontrar los parámetros de la regresión logística. </span>
<span id="cb24-402"><a href="#cb24-402" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-405"><a href="#cb24-405" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb24-406"><a href="#cb24-406" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: true</span></span>
<span id="cb24-407"><a href="#cb24-407" aria-hidden="true" tabindex="-1"></a>p <span class="op">=</span> random.normal(subkey, (<span class="dv">3</span>,)) <span class="op">*</span> jnp.sqrt(<span class="dv">2</span> <span class="op">/</span> <span class="dv">2</span>)</span>
<span id="cb24-408"><a href="#cb24-408" aria-hidden="true" tabindex="-1"></a>res <span class="op">=</span> minimize(suma_entropia_cruzada, p, args<span class="op">=</span>(T, y_t), method<span class="op">=</span><span class="st">'BFGS'</span>)</span>
<span id="cb24-409"><a href="#cb24-409" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb24-410"><a href="#cb24-410" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-413"><a href="#cb24-413" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb24-414"><a href="#cb24-414" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: false</span></span>
<span id="cb24-415"><a href="#cb24-415" aria-hidden="true" tabindex="-1"></a>_ <span class="op">=</span> <span class="st">', '</span>.join([<span class="ss">f'</span><span class="sc">{</span>x<span class="sc">:0.4f}</span><span class="ss">'</span> <span class="cf">for</span> x <span class="kw">in</span> res.x])</span>
<span id="cb24-416"><a href="#cb24-416" aria-hidden="true" tabindex="-1"></a>params_bfgs_f <span class="op">=</span> Markdown(<span class="ss">f'[</span><span class="sc">{</span>_<span class="sc">}</span><span class="ss">]'</span>)</span>
<span id="cb24-417"><a href="#cb24-417" aria-hidden="true" tabindex="-1"></a>_ <span class="op">=</span> jnp.array(res.x)</span>
<span id="cb24-418"><a href="#cb24-418" aria-hidden="true" tabindex="-1"></a>error_bfgs <span class="op">=</span> suma_entropia_cruzada(_, T, y_t)</span>
<span id="cb24-419"><a href="#cb24-419" aria-hidden="true" tabindex="-1"></a>error_bfgs <span class="op">=</span> Markdown(<span class="ss">f'$</span><span class="sc">{</span><span class="bu">float</span>(error_bfgs)<span class="sc">:0.4f}</span><span class="ss">$'</span>)</span>
<span id="cb24-420"><a href="#cb24-420" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb24-421"><a href="#cb24-421" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-422"><a href="#cb24-422" aria-hidden="true" tabindex="-1"></a>Se observa como se usa la misma función de error (<span class="in">`suma_entropia_cruzada`</span>) y los mismos parámetros iniciales. Los parámetros que se encuentran con este método son <span class="in">`{python} params_bfgs_f`</span> y tiene un error <span class="in">`{python} error_bfgs`</span>.</span>
<span id="cb24-423"><a href="#cb24-423" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-424"><a href="#cb24-424" aria-hidden="true" tabindex="-1"></a>La @fig-optimizacion-comparacion muestra la función discriminantes obtenida por cada uno de los métodos, el método de diferenciación automática (JAX), el método de la librearía <span class="in">`sklearn`</span> y el algoritmo <span class="in">`BFGS`</span>. Se puede observar que el plano de <span class="in">`sklearn`</span> y <span class="in">`BFGS`</span> son más similares, esto no es sorpresa porque los dos métodos implementan el mismo método de optimización, es decir, <span class="in">`BFGS`</span>. Por supuesto la implementaciones tiene algunas diferencias, que pueden ir desde el número de iteraciones y la forma de generar los parámetros iniciales. Si se escalan los parámetros $\mathbf w$ para que tengan una longitud de $1$ se observaría que <span class="in">`sklearn`</span> y <span class="in">`BFGS`</span> se tocan.</span>
<span id="cb24-425"><a href="#cb24-425" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-426"><a href="#cb24-426" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-427"><a href="#cb24-427" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-430"><a href="#cb24-430" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb24-431"><a href="#cb24-431" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-fold: true</span></span>
<span id="cb24-432"><a href="#cb24-432" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: Comparación de modelos lineales con diferentes optimizadores</span></span>
<span id="cb24-433"><a href="#cb24-433" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-optimizacion-comparacion</span></span>
<span id="cb24-434"><a href="#cb24-434" aria-hidden="true" tabindex="-1"></a>W <span class="op">=</span> [jnp.array([lr.coef_[<span class="dv">0</span>, <span class="dv">0</span>], lr.coef_[<span class="dv">0</span>, <span class="dv">1</span>], lr.intercept_[<span class="dv">0</span>]]), params, res.x]</span>
<span id="cb24-435"><a href="#cb24-435" aria-hidden="true" tabindex="-1"></a>g <span class="op">=</span> []</span>
<span id="cb24-436"><a href="#cb24-436" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> w, tipo <span class="kw">in</span> <span class="bu">zip</span>(W, [<span class="st">'sklearn'</span>, <span class="st">'JAX'</span>, <span class="st">'BFGS'</span>]):</span>
<span id="cb24-437"><a href="#cb24-437" aria-hidden="true" tabindex="-1"></a>     w_1, w_2, w_0 <span class="op">=</span> w</span>
<span id="cb24-438"><a href="#cb24-438" aria-hidden="true" tabindex="-1"></a>     g <span class="op">+=</span> [<span class="bu">dict</span>(x1<span class="op">=</span><span class="bu">float</span>(x), x2<span class="op">=</span><span class="bu">float</span>(y), tipo<span class="op">=</span>tipo)</span>
<span id="cb24-439"><a href="#cb24-439" aria-hidden="true" tabindex="-1"></a>           <span class="cf">for</span> x, y <span class="kw">in</span> <span class="bu">zip</span>(T[:, <span class="dv">0</span>], (<span class="op">-</span>w_0 <span class="op">-</span> w_1 <span class="op">*</span> T[:, <span class="dv">0</span>]) <span class="op">/</span> w_2)]</span>
<span id="cb24-440"><a href="#cb24-440" aria-hidden="true" tabindex="-1"></a>     <span class="co">#&nbsp;w_1, w_2, w_0 = w / np.linalg.norm(w) * 3</span></span>
<span id="cb24-441"><a href="#cb24-441" aria-hidden="true" tabindex="-1"></a>     g.append(<span class="bu">dict</span>(x1<span class="op">=</span><span class="bu">float</span>(w_1), x2<span class="op">=</span><span class="bu">float</span>(w_2), clase<span class="op">=</span>tipo))</span>
<span id="cb24-442"><a href="#cb24-442" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-443"><a href="#cb24-443" aria-hidden="true" tabindex="-1"></a>df <span class="op">=</span> pd.DataFrame(g <span class="op">+</span> <span class="op">\</span></span>
<span id="cb24-444"><a href="#cb24-444" aria-hidden="true" tabindex="-1"></a>                  [<span class="bu">dict</span>(x1<span class="op">=</span>x, x2<span class="op">=</span>y, clase<span class="op">=</span><span class="st">'P'</span>) <span class="cf">for</span> x, y <span class="kw">in</span> normalize.transform(X_1)] <span class="op">+</span> <span class="op">\</span></span>
<span id="cb24-445"><a href="#cb24-445" aria-hidden="true" tabindex="-1"></a>                  [<span class="bu">dict</span>(x1<span class="op">=</span>x, x2<span class="op">=</span>y, clase<span class="op">=</span><span class="st">'N'</span>) <span class="cf">for</span> x, y <span class="kw">in</span> normalize.transform(X_2)]</span>
<span id="cb24-446"><a href="#cb24-446" aria-hidden="true" tabindex="-1"></a>                 )</span>
<span id="cb24-447"><a href="#cb24-447" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-448"><a href="#cb24-448" aria-hidden="true" tabindex="-1"></a>ax <span class="op">=</span> sns.scatterplot(data<span class="op">=</span>df, x<span class="op">=</span><span class="st">'x1'</span>, y<span class="op">=</span><span class="st">'x2'</span>, hue<span class="op">=</span><span class="st">'clase'</span>, legend<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb24-449"><a href="#cb24-449" aria-hidden="true" tabindex="-1"></a>sns.lineplot(data<span class="op">=</span>df, x<span class="op">=</span><span class="st">'x1'</span>, y<span class="op">=</span><span class="st">'x2'</span>, </span>
<span id="cb24-450"><a href="#cb24-450" aria-hidden="true" tabindex="-1"></a>             ax<span class="op">=</span>ax, </span>
<span id="cb24-451"><a href="#cb24-451" aria-hidden="true" tabindex="-1"></a>             hue<span class="op">=</span><span class="st">'tipo'</span>, </span>
<span id="cb24-452"><a href="#cb24-452" aria-hidden="true" tabindex="-1"></a>             palette<span class="op">=</span>sns.color_palette()[:<span class="dv">3</span>],</span>
<span id="cb24-453"><a href="#cb24-453" aria-hidden="true" tabindex="-1"></a>             legend<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb24-454"><a href="#cb24-454" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-455"><a href="#cb24-455" aria-hidden="true" tabindex="-1"></a>_ <span class="op">=</span> ax.axis(<span class="st">'equal'</span>)</span>
<span id="cb24-456"><a href="#cb24-456" aria-hidden="true" tabindex="-1"></a><span class="in">```</span>  </span>
</code><button title="Copiar al portapapeles" class="code-copy-button" data-in-quarto-modal=""><i class="bi"></i></button></pre></div>
</div></div></div></div></div>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
      &nbsp;
    </div>   
    <div class="nav-footer-center">
      &nbsp;
    </div>
    <div class="nav-footer-right">
<p><a href="http://creativecommons.org/licenses/by-sa/4.0/"><img src="https://i.creativecommons.org/l/by-sa/4.0/80x15.png" class="img-fluid"></a> <br> Esta obra está bajo una <a href="http://creativecommons.org/licenses/by-sa/4.0/">Licencia Creative Commons Atribución-CompartirIgual 4.0 Internacional</a></p>
</div>
  </div>
</footer>




</body></html>