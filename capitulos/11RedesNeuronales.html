<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="es" xml:lang="es"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.4.320">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Aprendizaje Computacional - 11&nbsp; Redes Neuronales</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<link href="../capitulos/12Ensambles.html" rel="next">
<link href="../capitulos/10Optimizacion.html" rel="prev">
<script src="../site_libs/quarto-html/quarto.js"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 20,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "language": {
    "search-no-results-text": "Sin resultados",
    "search-matching-documents-text": "documentos encontrados",
    "search-copy-link-title": "Copiar el enlace en la búsqueda",
    "search-hide-matches-text": "Ocultar resultados adicionales",
    "search-more-match-text": "resultado adicional en este documento",
    "search-more-matches-text": "resultados adicionales en este documento",
    "search-clear-button-title": "Borrar",
    "search-detached-cancel-button-title": "Cancelar",
    "search-submit-button-title": "Enviar",
    "search-label": "Buscar"
  }
}</script>

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

</head>

<body class="nav-sidebar floating">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Alternar barra lateral" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../capitulos/11RedesNeuronales.html"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Redes Neuronales</span></a></li></ol></nav>
        <a class="flex-grow-1" role="button" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Alternar barra lateral" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="../">Aprendizaje Computacional</a> 
        <div class="sidebar-tools-main">
    <a href="https://github.com/INGEOTEC/AprendizajeComputacional" title="Source Code" class="quarto-navigation-tool px-1" aria-label="Source Code"><i class="bi bi-github"></i></a>
    <a href="../Aprendizaje-Computacional.pdf" title="Download PDF" class="quarto-navigation-tool px-1" aria-label="Download PDF"><i class="bi bi-file-pdf"></i></a>
</div>
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Buscar"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Prefacio</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../capitulos/01Tipos.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Tipos de Aprendizaje Computacional</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../capitulos/02Teoria_Decision.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Teoría de Decisión Bayesiana</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../capitulos/03Parametricos.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Métodos Paramétricos</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../capitulos/04Rendimiento.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Rendimiento</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../capitulos/05ReduccionDim.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Reducción de Dimensión</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../capitulos/06Agrupamiento.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Agrupamiento</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../capitulos/07NoParametricos.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Métodos No Paramétricos</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../capitulos/08Arboles.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Árboles de Decisión</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../capitulos/09Lineal.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Discriminantes Lineales</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../capitulos/10Optimizacion.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Optimización</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../capitulos/11RedesNeuronales.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Redes Neuronales</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../capitulos/12Ensambles.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">Ensambles</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../capitulos/13Comparacion.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">Comparación de Algoritmos</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../capitulos/17Referencias.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Referencias</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="true">
 <span class="menu-text">Apéndices</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="true" aria-label="Alternar sección">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../capitulos/14Estadistica.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">A</span>&nbsp; <span class="chapter-title">Estadística</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../capitulos/15Codigo.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">B</span>&nbsp; <span class="chapter-title">Código</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../capitulos/16ConjuntosDatos.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">C</span>&nbsp; <span class="chapter-title">Conjunto de Datos</span></span></a>
  </div>
</li>
      </ul>
  </li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Tabla de contenidos</h2>
   
  <ul>
  <li><a href="#paquetes-usados" id="toc-paquetes-usados" class="nav-link active" data-scroll-target="#paquetes-usados"><span class="header-section-number">11.1</span> Paquetes usados</a></li>
  <li><a href="#introducción" id="toc-introducción" class="nav-link" data-scroll-target="#introducción"><span class="header-section-number">11.2</span> Introducción</a></li>
  <li><a href="#sec-regresion-logistica-multinomial" id="toc-sec-regresion-logistica-multinomial" class="nav-link" data-scroll-target="#sec-regresion-logistica-multinomial"><span class="header-section-number">11.3</span> Regresión Logística Multinomial</a>
  <ul class="collapse">
  <li><a href="#optimización" id="toc-optimización" class="nav-link" data-scroll-target="#optimización"><span class="header-section-number">11.3.1</span> Optimización</a></li>
  <li><a href="#sec-adam" id="toc-sec-adam" class="nav-link" data-scroll-target="#sec-adam"><span class="header-section-number">11.3.2</span> Optimización Método Adam</a></li>
  <li><a href="#comparación-entre-optimizadores" id="toc-comparación-entre-optimizadores" class="nav-link" data-scroll-target="#comparación-entre-optimizadores"><span class="header-section-number">11.3.3</span> Comparación entre Optimizadores</a></li>
  </ul></li>
  <li><a href="#sec-preceptron" id="toc-sec-preceptron" class="nav-link" data-scroll-target="#sec-preceptron"><span class="header-section-number">11.4</span> Perceptrón</a>
  <ul class="collapse">
  <li><a href="#composición-de-perceptrones-lineales" id="toc-composición-de-perceptrones-lineales" class="nav-link" data-scroll-target="#composición-de-perceptrones-lineales"><span class="header-section-number">11.4.1</span> Composición de Perceptrones Lineales</a></li>
  </ul></li>
  <li><a href="#perceptrón-multicapa" id="toc-perceptrón-multicapa" class="nav-link" data-scroll-target="#perceptrón-multicapa"><span class="header-section-number">11.5</span> Perceptrón Multicapa</a>
  <ul class="collapse">
  <li><a href="#desvanecimiento-del-gradiente" id="toc-desvanecimiento-del-gradiente" class="nav-link" data-scroll-target="#desvanecimiento-del-gradiente"><span class="header-section-number">11.5.1</span> Desvanecimiento del Gradiente</a></li>
  </ul></li>
  <li><a href="#ejemplo-dígitos" id="toc-ejemplo-dígitos" class="nav-link" data-scroll-target="#ejemplo-dígitos"><span class="header-section-number">11.6</span> Ejemplo: Dígitos</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<div class="quarto-title-block"><div><h1 class="title"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Redes Neuronales</span></h1><button type="button" class="btn code-tools-button dropdown-toggle" id="quarto-code-tools-menu" data-bs-toggle="dropdown" aria-expanded="false"><i class="bi"></i> Código</button><ul class="dropdown-menu dropdown-menu-end" aria-labelelledby="quarto-code-tools-menu"><li><a id="quarto-show-all-code" class="dropdown-item" href="javascript:void(0)" role="button">Mostrar todo el código</a></li><li><a id="quarto-hide-all-code" class="dropdown-item" href="javascript:void(0)" role="button">Ocultar todo el código</a></li><li><hr class="dropdown-divider"></li><li><a id="quarto-view-source" class="dropdown-item" href="javascript:void(0)" role="button">Ver el código fuente</a></li></ul></div></div>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>

<p>El <strong>objetivo</strong> de la unidad es conocer, diseñar y aplicar redes neuronales artificiales para problemas de regresión y clasificación.</p>
<section id="paquetes-usados" class="level2" data-number="11.1">
<h2 data-number="11.1" class="anchored" data-anchor-id="paquetes-usados"><span class="header-section-number">11.1</span> Paquetes usados</h2>
<div id="bd8958e8" class="cell" data-execution_count="1">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> jax</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> jax.lax <span class="im">as</span> lax</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> jax.numpy <span class="im">as</span> jnp</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> optax</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.datasets <span class="im">import</span> load_iris, load_digits</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.preprocessing <span class="im">import</span> StandardScaler</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> train_test_split</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> seaborn <span class="im">as</span> sns</span></code><button title="Copiar al portapapeles" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<hr>
<div class="quarto-video"><iframe data-external="1" src="https://www.youtube.com/embed/5C9wNeDQ5eU" width="560" height="315" title="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div>
<hr>
</section>
<section id="introducción" class="level2" data-number="11.2">
<h2 data-number="11.2" class="anchored" data-anchor-id="introducción"><span class="header-section-number">11.2</span> Introducción</h2>
<p>Las redes neuronales son sin duda uno de los algoritmos de aprendizaje supervisado que mas han tomado auge en los últimos tiempos. Para iniciar la descripción de redes neuronales se toma de base el algoritmo de Regresión Logística (<a href="10Optimizacion.html#sec-regresion-logistica-optimizacion" class="quarto-xref"><span>Sección&nbsp;10.5</span></a>) pero en el caso de múltiples clases, es decir, Regresión Logística Multinomial.</p>
</section>
<section id="sec-regresion-logistica-multinomial" class="level2" data-number="11.3">
<h2 data-number="11.3" class="anchored" data-anchor-id="sec-regresion-logistica-multinomial"><span class="header-section-number">11.3</span> Regresión Logística Multinomial</h2>
<p>La idea de regresión logística es modelar <span class="math inline">\(\mathbb P(\mathcal Y=y \mid \mathcal X=\mathbf x)=\textsf{Ber}(y \mid \textsf{sigmoid}(\mathbf w \cdot \mathbf x + w_0)),\)</span> es decir, que la clase <span class="math inline">\(y\)</span> esta modelada como una distribución Bernoulli con parámetro <span class="math inline">\(\textsf{sigmoid}(\mathbf w \cdot \mathbf x + w_0).\)</span> Siguiendo está definición que es equivalente a la mostrada <a href="../AprendizajeComputacional/capitulos/10Optimizacion/#sec:regresion-logistica-optimizacion">anteriormente</a>, se puede modelar a un problema de multiples clases como <span class="math inline">\(\mathbb P(\mathcal Y=y \mid \mathcal X=\mathbf x)=\textsf{Cat}(y \mid \textsf{softmax}(W \mathbf x + \mathbf w_0)),\)</span> es decir, que la clase proviene de una distribución Categórica con parámetros <span class="math inline">\(\textsf{softmax}(W \mathbf x + \mathbf w_0),\)</span> donde <span class="math inline">\(W \in \mathbb R^{K \times d},\)</span> <span class="math inline">\(\mathbf x \in \mathbb R^d\)</span> y <span class="math inline">\(\mathbf w_0 \in \mathbb R^d.\)</span></p>
<p>La función <span class="math inline">\(\textsf{softmax}(\mathbf v)\)</span>, donde <span class="math inline">\(\mathbf v = W \mathbf x + \mathbf w_0\)</span> está definida como:</p>
<p><span class="math display">\[
\mathbf v_i = \frac{\exp \mathbf v_i}{\sum_{j=1}^K \exp \mathbf v_j}.
\]</span></p>
<p>La función <code>jax.nn.softmax</code> implementa <span class="math inline">\(\textsf{softmax}\)</span>; en el siguiente ejemplo se calcula para el vector <code>[2, 1, 3]</code></p>
<div id="dac8738c" class="cell" data-execution_count="3">
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a>jax.nn.softmax(np.array([<span class="dv">2</span>, <span class="dv">1</span>, <span class="dv">3</span>]))</span></code><button title="Copiar al portapapeles" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="3">
<pre><code>Array([0.24472848, 0.09003057, 0.66524094], dtype=float32)</code></pre>
</div>
</div>
<p>Se puede observar que <span class="math inline">\(\textsf{softmax}\)</span> transforma los valores del vector <span class="math inline">\(\mathbf v\)</span> en probabilidades.</p>
<p>Para seguir explicando este tipo de regresión logística se utilizará el problema del Iris, el cual se obtiene de la siguiente manera, es importante notar que las entradas están normalizadas para tener media cero y desviación estándar uno.</p>
<div id="ba67d5d7" class="cell" data-execution_count="4">
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a>D, y <span class="op">=</span> load_iris(return_X_y<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a>normalize <span class="op">=</span> StandardScaler().fit(D)</span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a>D <span class="op">=</span> normalize.transform(D)</span></code><button title="Copiar al portapapeles" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>El siguiente paso es generar el modelo de la Regresión Logística Multinomial, el cual depende de una matriz de coeficientes <span class="math inline">\(W \in \mathbb R^{K \times d}\)</span> y <span class="math inline">\(\mathbf w_0 \in \mathbb R^d.\)</span> Los parámetros iniciales se puede generar con la función <code>parametros_iniciales</code> tal y como se muestra a continuación.</p>
<div id="9d919723" class="cell" data-execution_count="5">
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a>n_labels <span class="op">=</span> np.unique(y).shape[<span class="dv">0</span>]</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> parametros_iniciales(key<span class="op">=</span><span class="dv">0</span>):</span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a>    key <span class="op">=</span> jax.random.PRNGKey(key)</span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a>    d <span class="op">=</span> D.shape[<span class="dv">1</span>]</span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a>    params <span class="op">=</span> []</span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a>    normal <span class="op">=</span> jax.random.normal</span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a>    sqrt <span class="op">=</span> jnp.sqrt</span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(n_labels):</span>
<span id="cb5-9"><a href="#cb5-9" aria-hidden="true" tabindex="-1"></a>        key, subkey <span class="op">=</span> jax.random.split(key)</span>
<span id="cb5-10"><a href="#cb5-10" aria-hidden="true" tabindex="-1"></a>        _ <span class="op">=</span> <span class="bu">dict</span>(w<span class="op">=</span>normal(subkey, (d, )) <span class="op">*</span> sqrt(<span class="dv">2</span> <span class="op">/</span> d),</span>
<span id="cb5-11"><a href="#cb5-11" aria-hidden="true" tabindex="-1"></a>                 w0<span class="op">=</span>jnp.ones(<span class="dv">1</span>))</span>
<span id="cb5-12"><a href="#cb5-12" aria-hidden="true" tabindex="-1"></a>        params.append(_)</span>
<span id="cb5-13"><a href="#cb5-13" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> params</span></code><button title="Copiar al portapapeles" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Utilizando los parámetros en el formato anterior, hace que el modelo se pueda implementar con las siguientes instrucciones. Donde el ciclo es por cada uno de los parámetros de las <span class="math inline">\(K\)</span> clases y la última línea calcula el <span class="math inline">\(\textsf{softmax}.\)</span></p>
<div id="a5465c08" class="cell" data-execution_count="6">
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="at">@jax.jit</span></span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> modelo(params, X):</span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a>    o <span class="op">=</span> []</span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> p <span class="kw">in</span> params:</span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a>        o.append(X <span class="op">@</span> p[<span class="st">'w'</span>] <span class="op">+</span> p[<span class="st">'w0'</span>])</span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> jax.nn.softmax(jnp.array(o), axis<span class="op">=</span><span class="dv">0</span>).T</span></code><button title="Copiar al portapapeles" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Una característica importante es que la función de perdida, en este caso, a la Entropía Cruzada (<a href="04Rendimiento.html#sec-entropia-cruzada" class="quarto-xref"><span>Sección&nbsp;4.2.7</span></a>), requiere codificada la probabilidad de cada clase en un vector, donde el índice con probabilidad <span class="math inline">\(1\)</span> corresponde a la clase, esto se puede realizar con el siguiente código.</p>
<div id="8ba0bf0d" class="cell" data-execution_count="7">
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a>y_oh <span class="op">=</span> jax.nn.one_hot(y, n_labels)</span></code><button title="Copiar al portapapeles" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Ahora se cuenta con todos los elementos para implementar la función de Entropía Cruzada para múltiples clases, la cual se muestra en el siguiente fragmento.</p>
<div id="e7bf8608" class="cell" data-execution_count="8">
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="at">@jax.jit</span></span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> media_entropia_cruzada(params, X, y_oh):</span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a>    hy <span class="op">=</span> modelo(params, X)</span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="op">-</span> (y_oh <span class="op">*</span> jnp.log(hy)).<span class="bu">sum</span>(axis<span class="op">=</span><span class="dv">1</span>).mean()</span></code><button title="Copiar al portapapeles" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<section id="optimización" class="level3" data-number="11.3.1">
<h3 data-number="11.3.1" class="anchored" data-anchor-id="optimización"><span class="header-section-number">11.3.1</span> Optimización</h3>
<p>El siguiente paso es encontrar los parámetros del modelo, para esto se utiliza el método de optimización visto en Regresión Logística ( <a href="10Optimizacion.html#sec-regresion-logistica-optimizacion" class="quarto-xref"><span>Sección&nbsp;10.5</span></a>) con algunos ajustes. Lo primero es que se desarrolla todo en una función <code>fit</code> que recibe el parámetro <span class="math inline">\(\eta\)</span>, los parámetros a identificar y el número de épocas, es decir, el número de iteraciones que se va a realizar el procedimiento.</p>
<p>Dentro de la función <code>fit</code>se observa la función <code>update</code> que calcula los nuevos parámetros, también regresa el valor del error, esto para poder visualizar como va aprendiendo el modelo. La primera linea después de la función <code>update</code> genera la función que calculará el valor y el gradiente de la función <code>media_entropia_cruzada</code>. Finalmente viene el ciclo donde se realizan las actualizaciones de los parámetros y se guarda el error calculado en cada época.</p>
<div id="d9d836da" class="cell" data-execution_count="9">
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> fit(eta, params, epocas<span class="op">=</span><span class="dv">500</span>):</span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a>    <span class="at">@jax.jit</span></span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> update(params, eta, X, y_oh):</span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a>        _ , gs <span class="op">=</span> error_grad(params, X, y_oh)</span>
<span id="cb9-5"><a href="#cb9-5" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> _, jax.tree_map(<span class="kw">lambda</span> p, g: p <span class="op">-</span> eta <span class="op">*</span> g,</span>
<span id="cb9-6"><a href="#cb9-6" aria-hidden="true" tabindex="-1"></a>                               params, gs)</span>
<span id="cb9-7"><a href="#cb9-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-8"><a href="#cb9-8" aria-hidden="true" tabindex="-1"></a>    error_grad  <span class="op">=</span> jax.value_and_grad(media_entropia_cruzada)</span>
<span id="cb9-9"><a href="#cb9-9" aria-hidden="true" tabindex="-1"></a>    error <span class="op">=</span> []</span>
<span id="cb9-10"><a href="#cb9-10" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(epocas):</span>
<span id="cb9-11"><a href="#cb9-11" aria-hidden="true" tabindex="-1"></a>        _, params <span class="op">=</span> update(params, eta, D, y_oh)</span>
<span id="cb9-12"><a href="#cb9-12" aria-hidden="true" tabindex="-1"></a>        error.append(_)</span>
<span id="cb9-13"><a href="#cb9-13" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> params, error</span></code><button title="Copiar al portapapeles" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="sec-adam" class="level3" data-number="11.3.2">
<h3 data-number="11.3.2" class="anchored" data-anchor-id="sec-adam"><span class="header-section-number">11.3.2</span> Optimización Método Adam</h3>
<p>Como se había visto en la <a href="10Optimizacion.html#sec-actualizacion-parametros" class="quarto-xref"><span>Sección&nbsp;10.6</span></a> existen diferentes métodos para encontrar los parámetros, en particular en esta sección se utilizará el método Adam (implementado en la librería <a href="https://optax.readthedocs.io">optax</a>) para encontrar los parámetros de la Regresión Logística Multinomial. Se decide utilizar este método dado que su uso es frecuente en la identificación de parámetros de redes neuronales.</p>
<p>Siguiendo la misma estructura que la función <code>fit</code>, la función <code>adam</code> recibe tres parámetros el primer es la instancia de optimizador, la segunda son los parámetros y finalmente el número de épocas que se va a ejecutar. La primera línea de la función <code>update</code> (que se encuentra en <code>adam</code>) calcula el valor de la función de error y su gradiente, estos son utilizados por <code>optimizer.update</code> para calcular la actualización de parámetros así como el nuevo estado del optimizador, los nuevos parámetros son calculados en la tercera línea y la función regresa los nuevos parámetros, el estado del optimizador y el error en esa iteración. La primera línea después de <code>update</code> inicializa el optimizador, después se general la función que calculará el valor y gradiente de la función <code>media_entropia_cruzada</code>. El ciclo llama la función <code>update</code> y guarda el error encontrado en cada época.</p>
<div id="807b652c" class="cell" data-execution_count="10">
<div class="sourceCode cell-code" id="cb10"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> adam(optimizer, params, epocas<span class="op">=</span><span class="dv">500</span>):</span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a>    <span class="at">@jax.jit</span></span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> update(params, opt_state, X, y_oh):</span>
<span id="cb10-4"><a href="#cb10-4" aria-hidden="true" tabindex="-1"></a>        loss_value, grads <span class="op">=</span> error_grad(params,</span>
<span id="cb10-5"><a href="#cb10-5" aria-hidden="true" tabindex="-1"></a>                                       X,</span>
<span id="cb10-6"><a href="#cb10-6" aria-hidden="true" tabindex="-1"></a>                                       y_oh)</span>
<span id="cb10-7"><a href="#cb10-7" aria-hidden="true" tabindex="-1"></a>        updates, opt_state <span class="op">=</span> optimizer.update(grads,</span>
<span id="cb10-8"><a href="#cb10-8" aria-hidden="true" tabindex="-1"></a>                                              opt_state,</span>
<span id="cb10-9"><a href="#cb10-9" aria-hidden="true" tabindex="-1"></a>                                              params)</span>
<span id="cb10-10"><a href="#cb10-10" aria-hidden="true" tabindex="-1"></a>        params <span class="op">=</span> optax.apply_updates(params, updates)</span>
<span id="cb10-11"><a href="#cb10-11" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> params, opt_state, loss_value</span>
<span id="cb10-12"><a href="#cb10-12" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb10-13"><a href="#cb10-13" aria-hidden="true" tabindex="-1"></a>    opt_state <span class="op">=</span> optimizer.init(params)</span>
<span id="cb10-14"><a href="#cb10-14" aria-hidden="true" tabindex="-1"></a>    error_grad  <span class="op">=</span> jax.value_and_grad(media_entropia_cruzada)    </span>
<span id="cb10-15"><a href="#cb10-15" aria-hidden="true" tabindex="-1"></a>    error <span class="op">=</span> []</span>
<span id="cb10-16"><a href="#cb10-16" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(epocas):</span>
<span id="cb10-17"><a href="#cb10-17" aria-hidden="true" tabindex="-1"></a>        params, opt_state, loss_value <span class="op">=</span> update(params,</span>
<span id="cb10-18"><a href="#cb10-18" aria-hidden="true" tabindex="-1"></a>                                               opt_state,</span>
<span id="cb10-19"><a href="#cb10-19" aria-hidden="true" tabindex="-1"></a>                                               D,</span>
<span id="cb10-20"><a href="#cb10-20" aria-hidden="true" tabindex="-1"></a>                                               y_oh)</span>
<span id="cb10-21"><a href="#cb10-21" aria-hidden="true" tabindex="-1"></a>        error.append(loss_value)</span>
<span id="cb10-22"><a href="#cb10-22" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> params, error</span></code><button title="Copiar al portapapeles" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="comparación-entre-optimizadores" class="level3" data-number="11.3.3">
<h3 data-number="11.3.3" class="anchored" data-anchor-id="comparación-entre-optimizadores"><span class="header-section-number">11.3.3</span> Comparación entre Optimizadores</h3>
<p>Los optimizadores descritos anteriormente se pueden utilizar con el siguiente código, donde la primera línea calcula los parámetros iniciales, después se llama a la función <code>fit</code> para encontrar los parámetros con el primer método. La tercera línea genera una instancia del optimizador Adam; el cual se pasa a la función <code>adam</code> para encontrar los parámetros con este método.</p>
<div id="52842ac8" class="cell" data-execution_count="11">
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a>params <span class="op">=</span> parametros_iniciales()</span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a>p1, error1 <span class="op">=</span> fit(<span class="fl">1e-2</span>, params)</span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a>optimizer <span class="op">=</span> optax.adam(learning_rate<span class="op">=</span><span class="fl">1e-2</span>)</span>
<span id="cb11-4"><a href="#cb11-4" aria-hidden="true" tabindex="-1"></a>p2, error2 <span class="op">=</span> adam(optimizer, params)</span></code><button title="Copiar al portapapeles" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>La <a href="#fig-redes-neuronales-adam" class="quarto-xref">Figura&nbsp;<span>11.1</span></a> muestra cómo la media de la Entropía Cruzada se minimiza con respecto a las épocas para los dos métodos. Se puede observar como el método <code>adam</code> converge más rápido y llega a un valor menor de Entropía Cruzada.</p>
<div id="cell-fig-redes-neuronales-adam" class="cell" data-execution_count="12">
<details>
<summary>Código</summary>
<div class="sourceCode cell-code" id="cb12"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a>df <span class="op">=</span> pd.DataFrame(<span class="bu">dict</span>(entropia<span class="op">=</span>np.array(error1),</span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a>                       optimizador<span class="op">=</span><span class="st">'fit'</span>,</span>
<span id="cb12-3"><a href="#cb12-3" aria-hidden="true" tabindex="-1"></a>                       epoca<span class="op">=</span>np.arange(<span class="dv">1</span>, <span class="dv">501</span>)))</span>
<span id="cb12-4"><a href="#cb12-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-5"><a href="#cb12-5" aria-hidden="true" tabindex="-1"></a>df <span class="op">=</span> pd.concat((df, pd.DataFrame(<span class="bu">dict</span>(entropia<span class="op">=</span>np.array(error2),</span>
<span id="cb12-6"><a href="#cb12-6" aria-hidden="true" tabindex="-1"></a>                                      optimizador<span class="op">=</span><span class="st">'adam'</span>,</span>
<span id="cb12-7"><a href="#cb12-7" aria-hidden="true" tabindex="-1"></a>                                      epoca<span class="op">=</span>np.arange(<span class="dv">1</span>, <span class="dv">501</span>)))))</span>
<span id="cb12-8"><a href="#cb12-8" aria-hidden="true" tabindex="-1"></a>_ <span class="op">=</span> sns.relplot(data<span class="op">=</span>df, x<span class="op">=</span><span class="st">'epoca'</span>, y<span class="op">=</span><span class="st">'entropia'</span>, hue<span class="op">=</span><span class="st">'optimizador'</span>, kind<span class="op">=</span><span class="st">'line'</span>)</span></code><button title="Copiar al portapapeles" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div id="fig-redes-neuronales-adam" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="11RedesNeuronales_files/figure-html/fig-redes-neuronales-adam-output-1.png" width="550" height="470" class="figure-img"></p>
<figcaption class="figure-caption">Figura&nbsp;11.1: Comparación de diferentes optimizadores.</figcaption>
</figure>
</div>
</div>
</div>
<p>Finalmente la exactitud (<a href="04Rendimiento.html#sec-accuracy" class="quarto-xref"><span>Sección&nbsp;4.2.2</span></a>) en el conjunto de entrenamiento del modelo estimado con <code>fit</code> es <span class="math inline">\(0.8867\)</span> (i.e., <code>(y == modelo(p1, D).argmax(axis=1)).mean()</code>) y del estimado con <code>adam</code> es <span class="math inline">\(0.9667\)</span>.</p>
</section>
</section>
<section id="sec-preceptron" class="level2" data-number="11.4">
<h2 data-number="11.4" class="anchored" data-anchor-id="sec-preceptron"><span class="header-section-number">11.4</span> Perceptrón</h2>
<p>La unidad básica de procesamiento en una red neuronal es el perceptrón, el cual es un viejo conocido de Discriminantes Lineales (<a href="09Lineal.html" class="quarto-xref"><span>Capítulo&nbsp;9</span></a>), es decir, <span class="math inline">\(g(\mathbf x) = \mathbf w \cdot \mathbf x + \mathbf w_0.\)</span> En problemas de clasificación binaria se encuentran los parámetros de <span class="math inline">\(g(\mathbf x)\)</span> de tal manera que genera un hiperplano y se clasifican los elementos de acuerdo al lado positivo o negativo del hiperplano. En problemas de Regresión (<a href="03Parametricos.html#sec-regresion-ols" class="quarto-xref"><span>Sección&nbsp;3.9</span></a>) los parámetros de <span class="math inline">\(g(\mathbf x)\)</span> se encuentran utilizando mínimos cuadrados.</p>
<p>En el case de tener <span class="math inline">\(K&gt;2\)</span> clases entonces el problema se puede afrontar entrenando <span class="math inline">\(g_k(\mathbf x)\)</span> perceptrones (<span class="math inline">\(k=1, \ldots, K\)</span>) tal y como se realizó Discriminantes Lineales (<a href="09Lineal.html#sec-multiples-clases" class="quarto-xref"><span>Sección&nbsp;9.3.3</span></a>). De manera concisa se puede definir a <span class="math inline">\(g: \mathbb R^d \rightarrow \mathbb R^K\)</span>, es decir, <span class="math inline">\(g(\mathbf x)=W \mathbf x + \mathbf w_0\)</span> tal y como se realizó en Regresión Logística Multinomial (<a href="#sec-regresion-logistica-multinomial" class="quarto-xref"><span>Sección&nbsp;11.3</span></a>). En el caso de desear conocer la probabilidad de pertenencia a una clase, en el caso binario se utilizó <span class="math inline">\(g(\mathbf x) = \textsf{sigmoid}(\mathbf w \cdot \mathbf x + \mathbf w_0)\)</span> y en el caso multiclase <span class="math inline">\(g(\mathbf x) = \textsf{softmax}(W \mathbf x + \mathbf w_0).\)</span></p>
<section id="composición-de-perceptrones-lineales" class="level3" data-number="11.4.1">
<h3 data-number="11.4.1" class="anchored" data-anchor-id="composición-de-perceptrones-lineales"><span class="header-section-number">11.4.1</span> Composición de Perceptrones Lineales</h3>
<p>Se puede realizar una composición de perceptrones de la siguiente manera, sea <span class="math inline">\(g_1: \mathbb R^d \rightarrow \mathbb R^{d'}\)</span> y <span class="math inline">\(g_2: \mathbb R^{d'} \rightarrow \mathbb R^K,\)</span> es decir <span class="math inline">\(g = g_2 \circ g_1.\)</span> Realizando está composición en las ecuaciones descritas anteriormente se tiene</p>
<p><span class="math display">\[
\begin{split}
\hat{\mathbf{y}}_1 &amp;= g_1(\mathbf x) \\
\hat{\mathbf y} &amp;= g_2(\hat{\mathbf{y}}_1)
\end{split}
\]</span></p>
<p>Expandiendo las ecuaciones anteriores se tiene</p>
<p><span class="math display">\[
\begin{split}
\hat{\mathbf{y}}_1 &amp;= W_1 \mathbf x + \mathbf w_{1_0}\\
\hat{\mathbf y} &amp;= W_2 \hat{\mathbf{y}}_1 + \mathbf w_{2_0}\\
&amp;= W_2 (W_1 \mathbf x + \mathbf w_{1_0}) + \mathbf w_{2_0}\\
&amp;= W_2 W_1 \mathbf x + W_2 \mathbf w_{1_0} + \mathbf w_{2_0}\\
&amp;= W \mathbf x + \mathbf w_0
\end{split}
\]</span></p>
<p>como se puede observar la composición realizada da como resultado una red donde se tienen que identificar <span class="math inline">\(W \in \mathbb R^{K \times d}\)</span> y <span class="math inline">\(\mathbf w_0 \in \mathbb R^d,\)</span> es decir, son <span class="math inline">\(K\)</span> perceptrones equivalentes al modelado de Regresión Logística Multinomial. Esto es porque la composición fue con funciones lineales.</p>
</section>
</section>
<section id="perceptrón-multicapa" class="level2" data-number="11.5">
<h2 data-number="11.5" class="anchored" data-anchor-id="perceptrón-multicapa"><span class="header-section-number">11.5</span> Perceptrón Multicapa</h2>
<p>Para evitar que la composición de perceptrones colapsen a una función equivalente, es necesario incluir una función no lineal, sea <span class="math inline">\(\phi\)</span> esta función no lineal, (a esta función se le conoce como <strong>función de activación</strong>) entonces se puede observar que la composición <span class="math inline">\(g=g_2 \circ g_1\)</span> donde <span class="math inline">\(g_1 = \phi(W_1 \mathbf x + \mathbf w_{1_0})\)</span> resulta en</p>
<p><span class="math display">\[
\begin{split}
\hat{\mathbf{y}}_1 &amp;= \phi(W_1 \mathbf x + \mathbf w_{1_0}) \\
\hat{\mathbf y} &amp;= W_2 \hat{\mathbf{y}}_1 + \mathbf w_{2_0}
\end{split}.
\]</span></p>
<p>A la estructura anterior se le conoce como una red neuronal de una capa oculta, la salida de la capa oculta está en <span class="math inline">\(\hat{\mathbf{y}}_1\)</span> y la salida de la red es <span class="math inline">\(\hat{\mathbf y}.\)</span> Siguiendo la notación anterior se puede definir una red neuronal con dos capas ocultas de la siguiente manera</p>
<p><span class="math display">\[
\begin{split}
\hat{\mathbf{y}}_1 &amp;= \phi(W_1 \mathbf x + \mathbf w_{1_0}) \\
\hat{\mathbf y}_2 &amp;= \phi_2(W_2 \hat{\mathbf{y}}_1 + \mathbf w_{2_0})\\
\hat{\mathbf y} &amp;= (W_3 \hat{\mathbf{y}}_2 + \mathbf w_{3_0})
\end{split},
\]</span></p>
<p>donde la salida de la primera capa oculta (<span class="math inline">\(\hat{\mathbf{y}}_1\)</span>) es la entrada de la segunda capa oculta y su salida (<span class="math inline">\(\hat{\mathbf{y}}_2\)</span>) se convierte en la entrada de la capa de salida.</p>
<section id="desvanecimiento-del-gradiente" class="level3" data-number="11.5.1">
<h3 data-number="11.5.1" class="anchored" data-anchor-id="desvanecimiento-del-gradiente"><span class="header-section-number">11.5.1</span> Desvanecimiento del Gradiente</h3>
<p>Por lo expuesto hasta el momento se podría pensar que una candidata para ser la función <span class="math inline">\(\phi\)</span> es la función <span class="math inline">\(\textsf{sigmoid},\)</span> aunque esto es factible, esta presenta el problema de desvanecimiento del gradiente. Para ejemplificar este problema, se utilizan dos funciones <span class="math inline">\(g_1(x) = w_1 x + 1.0\)</span> y <span class="math inline">\(g_1(x) = w_2 x + 1.0;\)</span> haciéndose la composición de estas dos funciones <span class="math inline">\(g=g_2 \circ g_1.\)</span></p>
<p>Las siguientes instrucciones implementan las funciones anteriores utilizando la librería <a href="https://jax.readthedocs.io">JAX</a>.</p>
<div id="b5f8a2d2" class="cell" data-execution_count="14">
<div class="sourceCode cell-code" id="cb13"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a><span class="at">@jax.jit</span></span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> g_1(params, x):</span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> jax.nn.sigmoid(params[<span class="st">'w1'</span>] <span class="op">*</span> x <span class="op">+</span> <span class="fl">1.0</span>)</span>
<span id="cb13-4"><a href="#cb13-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-5"><a href="#cb13-5" aria-hidden="true" tabindex="-1"></a><span class="at">@jax.jit</span></span>
<span id="cb13-6"><a href="#cb13-6" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> g_2(params, x):</span>
<span id="cb13-7"><a href="#cb13-7" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> jax.nn.sigmoid(params[<span class="st">'w2'</span>] <span class="op">*</span> x <span class="op">+</span> <span class="fl">1.0</span>)</span>
<span id="cb13-8"><a href="#cb13-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-9"><a href="#cb13-9" aria-hidden="true" tabindex="-1"></a><span class="at">@jax.jit</span></span>
<span id="cb13-10"><a href="#cb13-10" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> g(params, x):</span>
<span id="cb13-11"><a href="#cb13-11" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> g_2(params, g_1(params, x))</span></code><button title="Copiar al portapapeles" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Utilizando unos parámetros aleatorios generados con el siguiente código</p>
<div id="6585d695" class="cell" data-execution_count="15">
<div class="sourceCode cell-code" id="cb14"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a>key <span class="op">=</span> jax.random.PRNGKey(<span class="dv">0</span>)</span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a>key, subkey1, subkey2 <span class="op">=</span> jax.random.split(key, num<span class="op">=</span><span class="dv">3</span>)</span>
<span id="cb14-3"><a href="#cb14-3" aria-hidden="true" tabindex="-1"></a>params <span class="op">=</span> <span class="bu">dict</span>(w1<span class="op">=</span>jax.random.normal(subkey1),</span>
<span id="cb14-4"><a href="#cb14-4" aria-hidden="true" tabindex="-1"></a>              w2<span class="op">=</span>jax.random.normal(subkey2))</span></code><button title="Copiar al portapapeles" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>se tiene que la derivada de <span class="math inline">\(g_1\)</span> con respecto a <span class="math inline">\(w_1\)</span> (i.e., <code>jax.grad(g_1)(params, 1.0)</code>) y <span class="math inline">\(g_2\)</span> con respecto a <span class="math inline">\(w_2\)</span> (i.e., <code>jax.grad(g_2)(params, 1.0)</code>) es 0.1418 y 0.1171, respectivamente. El problema viene cuando se calcula <span class="math inline">\(\frac{\partial g}{\partial w_1}\)</span> en este caso se obsevar que el gradiente es pequeño comparado con el gradiente obtenido en <span class="math inline">\(\frac{d g}{d w_1},\)</span> i.e., <code>jax.grad(g)(params, 1.0)</code>, donde se observa que el gradiente para <span class="math inline">\(w_1\)</span> corresponde a 0.0157, el gradiente de <span class="math inline">\(w_2\)</span> sigue en la misma mágnitud teniendo un valor de 0.1077.</p>
<p>El problema de desvanecimiento de gradiente, hace que el gradiente disminuya de manera exponencial, entonces los pesos asociados a las capas alejadas de la capa de salida reciben un gradiente equivalente a cero y no se cuenta con información para actualizar sus pesos. Por este motivo es recomendable utilizar funciones de activación que no presenten esta característica, una muy utilizada es <span class="math inline">\(\textsf{ReLU}(x) = \max(0, x)\)</span>.</p>
</section>
</section>
<section id="ejemplo-dígitos" class="level2" data-number="11.6">
<h2 data-number="11.6" class="anchored" data-anchor-id="ejemplo-dígitos"><span class="header-section-number">11.6</span> Ejemplo: Dígitos</h2>
<p>Para ejemplificar el uso de una red neuronal en un poblema de clasificación se utilizarán los datos de Dígitos, los cuales se pueden obtener con las siguientes instrucciones.</p>
<div id="b8353913" class="cell" data-execution_count="17">
<div class="sourceCode cell-code" id="cb15"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a>X, y <span class="op">=</span> load_digits(return_X_y<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a>T, G, y_t, y_g <span class="op">=</span> train_test_split(X, y, test_size<span class="op">=</span><span class="fl">0.2</span>)</span></code><button title="Copiar al portapapeles" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Un procedimiento necesario en redes neuronales es que los datos estén normalizados, tradicionalmente esto se realiza haciendo que los datos tengan media cero y desviación estandar uno. En las siguientes lineas se normalizan los datos usando la clase <code>StandardScaler</code> se puede observar que los parámetros para la normalización son encontrados en el conjunto de entrenamiento (<code>T</code>) y aplicados tanto al conjunto de entrenamiento como el conjunto de prueba <code>G</code>.</p>
<div id="534ab6ec" class="cell" data-execution_count="18">
<div class="sourceCode cell-code" id="cb16"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a>normalize <span class="op">=</span> StandardScaler().fit(T)</span>
<span id="cb16-2"><a href="#cb16-2" aria-hidden="true" tabindex="-1"></a>T <span class="op">=</span> normalize.transform(T)</span>
<span id="cb16-3"><a href="#cb16-3" aria-hidden="true" tabindex="-1"></a>G <span class="op">=</span> normalize.transform(G)</span></code><button title="Copiar al portapapeles" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Como se realizó previamente es necesario convertir las clases de salida para que cada ejemplo sea un vector unitario donde el índice con el valor <span class="math inline">\(1\)</span> representa la clase, esto se realiza con las siguientes instrucciones.</p>
<div id="28ccb077" class="cell" data-execution_count="19">
<div class="sourceCode cell-code" id="cb17"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a>n_labels <span class="op">=</span> np.unique(y).shape[<span class="dv">0</span>]</span>
<span id="cb17-2"><a href="#cb17-2" aria-hidden="true" tabindex="-1"></a>yt_oh <span class="op">=</span> jax.nn.one_hot(y_t, n_labels)</span></code><button title="Copiar al portapapeles" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Es momento de decidir la estructura de la red neuronal, las únicas dos restricciones es que la primera capa tiene que tener la dimensión del vector de entrada, en esta caso corresponde a <span class="math inline">\(64\)</span> (<code>T.shape[1]</code>) y la última capa tiene que tener de salida el número de clases, en este caso <span class="math inline">\(10\)</span> (<code>n_labels</code>), el resto de las capas ocultas pueden tener cualquier valor solamente es necesario que la dimensiones sean coherentes con la operación que se va a realizar.</p>
<p>La red que se va a implementar es la siguiente, como super-índice se encuentran las dimensiones para que sea más fácil seguir la estructura de la red.</p>
<p><span class="math display">\[
\begin{split}
\hat{\mathbf{y}}_1^{32} &amp;= \phi(W^{32\times 64} \mathbf x^{64} + \mathbf w_{1_0}^{32})\\
\hat{\mathbf{y}}_2^{16} &amp;= \phi(W^{16\times 32} \hat{\mathbf{y}}_1^{32} + \mathbf w_{2_0}^{16})\\
\hat{\mathbf{y}}^{10} &amp;= W^{10\times 16} \hat{\mathbf{y}}_2^{16} + \mathbf w_{3_0}^{10}
\end{split}
\]</span></p>
<p>Considerando que las entradas se encuentran en una matrix <span class="math inline">\(X^{N\times64},\)</span> entonces se puede definir esta estructura en términos de múltiplicación de matrices, lo cual queda como</p>
<p><span class="math display">\[
\begin{split}
\hat{Y}_1^{N\times 32} &amp;= \phi(X^{N\times 64} W^{64\times 32} + \mathbf w_{1_0}^{32})\\
\hat{Y}_2^{N\times 16} &amp;= \phi(\hat{Y}_1^{N\times 32} W^{32\times 16} + \mathbf w_{2_0}^{16})\\
\hat{Y}^{N\times 10} &amp;= \hat{Y}_2^{N\times 16} W^{16\times 10} + \mathbf w_{3_0}^{10}\\
\end{split},
\]</span></p>
<p>donde la suma con el término <span class="math inline">\(\mathbf w_0\)</span> se realiza en la dimensión que corresponde y se replica tantas veces para cumplir con la otra dimensión. Esta configuración se puede expresar en un lista como la que se muestra a continuación.</p>
<div id="ca33e2f5" class="cell" data-execution_count="21">
<div class="sourceCode cell-code" id="cb18"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a>d <span class="op">=</span> [<span class="dv">64</span>, <span class="dv">32</span>, <span class="dv">16</span>, <span class="dv">10</span>]</span></code><button title="Copiar al portapapeles" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Utilizando esta notación los parámetros iniciales de la red se pueden generar con la siguiente función, se puede observar como el ciclo está iterando por los elementos de <code>d</code> creando pares, para generar las dimensiones adecuadas para las matrices <span class="math inline">\(W.\)</span></p>
<div id="8a89d777" class="cell" data-execution_count="22">
<div class="sourceCode cell-code" id="cb19"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> parametros_iniciales(d, key<span class="op">=</span><span class="dv">0</span>):</span>
<span id="cb19-2"><a href="#cb19-2" aria-hidden="true" tabindex="-1"></a>    key <span class="op">=</span> jax.random.PRNGKey(key)</span>
<span id="cb19-3"><a href="#cb19-3" aria-hidden="true" tabindex="-1"></a>    params <span class="op">=</span> []</span>
<span id="cb19-4"><a href="#cb19-4" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> init, end <span class="kw">in</span> <span class="bu">zip</span>(d, d[<span class="dv">1</span>:]):</span>
<span id="cb19-5"><a href="#cb19-5" aria-hidden="true" tabindex="-1"></a>        key, subkey1, subkey2 <span class="op">=</span> jax.random.split(key, num<span class="op">=</span><span class="dv">3</span>)</span>
<span id="cb19-6"><a href="#cb19-6" aria-hidden="true" tabindex="-1"></a>        _ <span class="op">=</span> <span class="bu">dict</span>(w<span class="op">=</span>jax.random.normal(subkey1, (init, end)) <span class="op">*</span> jnp.sqrt(<span class="dv">2</span> <span class="op">/</span> (init <span class="op">*</span> end)),</span>
<span id="cb19-7"><a href="#cb19-7" aria-hidden="true" tabindex="-1"></a>                 w0<span class="op">=</span>jax.random.normal(subkey2, (end, )) <span class="op">*</span> jnp.sqrt(<span class="dv">2</span> <span class="op">/</span> end))</span>
<span id="cb19-8"><a href="#cb19-8" aria-hidden="true" tabindex="-1"></a>        params.append(_)</span>
<span id="cb19-9"><a href="#cb19-9" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> params</span></code><button title="Copiar al portapapeles" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Habiendo generado los parámetros iniciales de la red, es momento para implmentar la red, la siguiente función implementa la red, se puede observar como es realizan las operaciones matriciales tal y como se mostraron en las ecuaciones anteriores. La función de activiación <span class="math inline">\(\phi\)</span> seleccionada fue <span class="math inline">\(\textsf{ReLU}\)</span> que está implementada en la función <code>jax.nn.relu</code>.</p>
<div id="277b7aeb" class="cell" data-execution_count="23">
<div class="sourceCode cell-code" id="cb20"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a><span class="at">@jax.jit</span></span>
<span id="cb20-2"><a href="#cb20-2" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> ann(params, D):</span>
<span id="cb20-3"><a href="#cb20-3" aria-hidden="true" tabindex="-1"></a>    y1 <span class="op">=</span> jax.nn.relu(D <span class="op">@</span> params[<span class="dv">0</span>][<span class="st">'w'</span>] <span class="op">+</span> params[<span class="dv">0</span>][<span class="st">'w0'</span>])</span>
<span id="cb20-4"><a href="#cb20-4" aria-hidden="true" tabindex="-1"></a>    y2 <span class="op">=</span> jax.nn.relu(y1 <span class="op">@</span> params[<span class="dv">1</span>][<span class="st">'w'</span>] <span class="op">+</span> params[<span class="dv">1</span>][<span class="st">'w0'</span>])</span>
<span id="cb20-5"><a href="#cb20-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> y2 <span class="op">@</span> params[<span class="dv">2</span>][<span class="st">'w'</span>] <span class="op">+</span> params[<span class="dv">2</span>][<span class="st">'w0'</span>]</span></code><button title="Copiar al portapapeles" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Como medida de error se usa la Entropía Cruzada (<a href="04Rendimiento.html#sec-entropia-cruzada" class="quarto-xref"><span>Sección&nbsp;4.2.7</span></a>) tal y como se implementa a continuación. El caso <span class="math inline">\(0 \log 0\)</span> corresponde a un valor no definido lo cual genera que el valor de la función tampoco este definido, para proteger la función en ese caso se usa <code>jnp.nansum</code> que trata los valores no definidos como ceros.</p>
<div id="273ff6c4" class="cell" data-execution_count="24">
<div class="sourceCode cell-code" id="cb21"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a><span class="at">@jax.jit</span></span>
<span id="cb21-2"><a href="#cb21-2" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> media_entropia_cruzada(params, D, y_oh):</span>
<span id="cb21-3"><a href="#cb21-3" aria-hidden="true" tabindex="-1"></a>    hy <span class="op">=</span> jax.nn.softmax(ann(params, D), axis<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb21-4"><a href="#cb21-4" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="op">-</span> jnp.nansum(y_oh <span class="op">*</span> jnp.log(hy), axis<span class="op">=</span><span class="dv">1</span>).mean()</span></code><button title="Copiar al portapapeles" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>En esta ocasión se utiliza el optimizador Adam, tal y como se muestra en la siguiente función. La única diferencia con respecto al visto previamente es la función <code>update_finite</code> que actualiza los parámetros siempre y cuando el nuevo valor sea un valor numérico, de los contrario se queda con el valor anterior.</p>
<div id="7b9c430b" class="cell" data-execution_count="25">
<div class="sourceCode cell-code" id="cb22"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb22-1"><a href="#cb22-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> adam(optimizer, params, epocas<span class="op">=</span><span class="dv">500</span>):</span>
<span id="cb22-2"><a href="#cb22-2" aria-hidden="true" tabindex="-1"></a>    <span class="at">@jax.jit</span></span>
<span id="cb22-3"><a href="#cb22-3" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> update_finite(a, b):</span>
<span id="cb22-4"><a href="#cb22-4" aria-hidden="true" tabindex="-1"></a>        m <span class="op">=</span> jnp.isfinite(b)</span>
<span id="cb22-5"><a href="#cb22-5" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> jnp.where(m, b, a)</span>
<span id="cb22-6"><a href="#cb22-6" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb22-7"><a href="#cb22-7" aria-hidden="true" tabindex="-1"></a>    <span class="at">@jax.jit</span></span>
<span id="cb22-8"><a href="#cb22-8" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> update(params, opt_state, X, y_oh):</span>
<span id="cb22-9"><a href="#cb22-9" aria-hidden="true" tabindex="-1"></a>        loss_value, grads <span class="op">=</span> error_grad(params, X, y_oh)</span>
<span id="cb22-10"><a href="#cb22-10" aria-hidden="true" tabindex="-1"></a>        updates, opt_state <span class="op">=</span> optimizer.update(grads, opt_state, params)</span>
<span id="cb22-11"><a href="#cb22-11" aria-hidden="true" tabindex="-1"></a>        params <span class="op">=</span> optax.apply_updates(params, updates)</span>
<span id="cb22-12"><a href="#cb22-12" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> params, opt_state, loss_value</span>
<span id="cb22-13"><a href="#cb22-13" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb22-14"><a href="#cb22-14" aria-hidden="true" tabindex="-1"></a>    opt_state <span class="op">=</span> optimizer.init(params)</span>
<span id="cb22-15"><a href="#cb22-15" aria-hidden="true" tabindex="-1"></a>    error_grad  <span class="op">=</span> jax.value_and_grad(media_entropia_cruzada)    </span>
<span id="cb22-16"><a href="#cb22-16" aria-hidden="true" tabindex="-1"></a>    error <span class="op">=</span> []</span>
<span id="cb22-17"><a href="#cb22-17" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(epocas):</span>
<span id="cb22-18"><a href="#cb22-18" aria-hidden="true" tabindex="-1"></a>        p, opt_state, loss_value <span class="op">=</span> update(params, opt_state, T, yt_oh)</span>
<span id="cb22-19"><a href="#cb22-19" aria-hidden="true" tabindex="-1"></a>        params <span class="op">=</span> jax.tree_map(update_finite, params, p)</span>
<span id="cb22-20"><a href="#cb22-20" aria-hidden="true" tabindex="-1"></a>        error.append(loss_value)</span>
<span id="cb22-21"><a href="#cb22-21" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> params, error</span></code><button title="Copiar al portapapeles" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Finalmente, la red creada se entrana utilizando las siguientes instrucciones, donde la primera linea genera los parámetros iniciales, después se inicializa el optimizador y en la línea final se llama al optimizador con los parámetros y número de épocas.</p>
<div id="6d97c21b" class="cell" data-execution_count="26">
<div class="sourceCode cell-code" id="cb23"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb23-1"><a href="#cb23-1" aria-hidden="true" tabindex="-1"></a>params <span class="op">=</span> parametros_iniciales(d)</span>
<span id="cb23-2"><a href="#cb23-2" aria-hidden="true" tabindex="-1"></a>optimizer <span class="op">=</span> optax.adam(learning_rate<span class="op">=</span><span class="fl">1e-2</span>)</span>
<span id="cb23-3"><a href="#cb23-3" aria-hidden="true" tabindex="-1"></a>p, error <span class="op">=</span> adam(optimizer, params, epocas<span class="op">=</span><span class="dv">500</span>)</span></code><button title="Copiar al portapapeles" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>El resultado de esta red, es que el error en el conjunto de prueba es <span class="math inline">\(0.0250\)</span> calculado con la siguiente instrucción <code>(ann(p, G).argmax(axis=1) != y_g).mean()</code>.</p>


<!-- -->

</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copiado");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copiado");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  const viewSource = window.document.getElementById('quarto-view-source') ||
                     window.document.getElementById('quarto-code-tools-source');
  if (viewSource) {
    const sourceUrl = viewSource.getAttribute("data-quarto-source-url");
    viewSource.addEventListener("click", function(e) {
      if (sourceUrl) {
        // rstudio viewer pane
        if (/\bcapabilities=\b/.test(window.location)) {
          window.open(sourceUrl);
        } else {
          window.location.href = sourceUrl;
        }
      } else {
        const modal = new bootstrap.Modal(document.getElementById('quarto-embedded-source-code-modal'));
        modal.show();
      }
      return false;
    });
  }
  function toggleCodeHandler(show) {
    return function(e) {
      const detailsSrc = window.document.querySelectorAll(".cell > details > .sourceCode");
      for (let i=0; i<detailsSrc.length; i++) {
        const details = detailsSrc[i].parentElement;
        if (show) {
          details.open = true;
        } else {
          details.removeAttribute("open");
        }
      }
      const cellCodeDivs = window.document.querySelectorAll(".cell > .sourceCode");
      const fromCls = show ? "hidden" : "unhidden";
      const toCls = show ? "unhidden" : "hidden";
      for (let i=0; i<cellCodeDivs.length; i++) {
        const codeDiv = cellCodeDivs[i];
        if (codeDiv.classList.contains(fromCls)) {
          codeDiv.classList.remove(fromCls);
          codeDiv.classList.add(toCls);
        } 
      }
      return false;
    }
  }
  const hideAllCode = window.document.getElementById("quarto-hide-all-code");
  if (hideAllCode) {
    hideAllCode.addEventListener("click", toggleCodeHandler(false));
  }
  const showAllCode = window.document.getElementById("quarto-show-all-code");
  if (showAllCode) {
    showAllCode.addEventListener("click", toggleCodeHandler(true));
  }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        for (let i = 0; i < 2; i++) {
          container.appendChild(note.children[i].cloneNode(true));
        }
        return container.innerHTML
      } else {
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      return note.innerHTML;
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        console.log("RESIZE");
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="../capitulos/10Optimizacion.html" class="pagination-link">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Optimización</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="../capitulos/12Ensambles.html" class="pagination-link">
        <span class="nav-page-text"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">Ensambles</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav><div class="modal fade" id="quarto-embedded-source-code-modal" tabindex="-1" aria-labelledby="quarto-embedded-source-code-modal-label" aria-hidden="true"><div class="modal-dialog modal-dialog-scrollable"><div class="modal-content"><div class="modal-header"><h5 class="modal-title" id="quarto-embedded-source-code-modal-label">Ejecutar el código</h5><button class="btn-close" data-bs-dismiss="modal"></button></div><div class="modal-body"><div class="">
<div class="sourceCode" id="cb24" data-shortcodes="false"><pre class="sourceCode markdown code-with-copy"><code class="sourceCode markdown"><span id="cb24-1"><a href="#cb24-1" aria-hidden="true" tabindex="-1"></a><span class="fu"># Redes Neuronales</span></span>
<span id="cb24-2"><a href="#cb24-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-3"><a href="#cb24-3" aria-hidden="true" tabindex="-1"></a>El **objetivo** de la unidad es conocer, diseñar y aplicar redes neuronales artificiales para problemas de regresión y clasificación.</span>
<span id="cb24-4"><a href="#cb24-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-5"><a href="#cb24-5" aria-hidden="true" tabindex="-1"></a><span class="fu">## Paquetes usados</span></span>
<span id="cb24-6"><a href="#cb24-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-9"><a href="#cb24-9" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb24-10"><a href="#cb24-10" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: true</span></span>
<span id="cb24-11"><a href="#cb24-11" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> jax</span>
<span id="cb24-12"><a href="#cb24-12" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> jax.lax <span class="im">as</span> lax</span>
<span id="cb24-13"><a href="#cb24-13" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> jax.numpy <span class="im">as</span> jnp</span>
<span id="cb24-14"><a href="#cb24-14" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> optax</span>
<span id="cb24-15"><a href="#cb24-15" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.datasets <span class="im">import</span> load_iris, load_digits</span>
<span id="cb24-16"><a href="#cb24-16" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.preprocessing <span class="im">import</span> StandardScaler</span>
<span id="cb24-17"><a href="#cb24-17" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> train_test_split</span>
<span id="cb24-18"><a href="#cb24-18" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb24-19"><a href="#cb24-19" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb24-20"><a href="#cb24-20" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> seaborn <span class="im">as</span> sns</span>
<span id="cb24-21"><a href="#cb24-21" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb24-22"><a href="#cb24-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-25"><a href="#cb24-25" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb24-26"><a href="#cb24-26" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: false</span></span>
<span id="cb24-27"><a href="#cb24-27" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> IPython.display <span class="im">import</span> Markdown</span>
<span id="cb24-28"><a href="#cb24-28" aria-hidden="true" tabindex="-1"></a>sns.set_style(<span class="st">'whitegrid'</span>)</span>
<span id="cb24-29"><a href="#cb24-29" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb24-30"><a href="#cb24-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-31"><a href="#cb24-31" aria-hidden="true" tabindex="-1"></a>::: {.content-visible when-format="html"}</span>
<span id="cb24-32"><a href="#cb24-32" aria-hidden="true" tabindex="-1"></a>---</span>
<span id="cb24-33"><a href="#cb24-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-34"><a href="#cb24-34" aria-hidden="true" tabindex="-1"></a>{{&lt; video https://www.youtube.com/embed/5C9wNeDQ5eU width="560" height="315" &gt;}}</span>
<span id="cb24-35"><a href="#cb24-35" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-36"><a href="#cb24-36" aria-hidden="true" tabindex="-1"></a>---</span>
<span id="cb24-37"><a href="#cb24-37" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb24-38"><a href="#cb24-38" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-39"><a href="#cb24-39" aria-hidden="true" tabindex="-1"></a><span class="fu">## Introducción</span></span>
<span id="cb24-40"><a href="#cb24-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-41"><a href="#cb24-41" aria-hidden="true" tabindex="-1"></a>Las redes neuronales son sin duda uno de los algoritmos de aprendizaje supervisado que mas han tomado auge en los últimos tiempos. Para iniciar la descripción de redes neuronales se toma de base el algoritmo de Regresión Logística (@sec-regresion-logistica-optimizacion) pero en el caso de múltiples clases, es decir, Regresión Logística Multinomial.</span>
<span id="cb24-42"><a href="#cb24-42" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-43"><a href="#cb24-43" aria-hidden="true" tabindex="-1"></a><span class="fu">## Regresión Logística Multinomial {#sec-regresion-logistica-multinomial}</span></span>
<span id="cb24-44"><a href="#cb24-44" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-45"><a href="#cb24-45" aria-hidden="true" tabindex="-1"></a>La idea de regresión logística es modelar $\mathbb P(\mathcal Y=y \mid \mathcal X=\mathbf x)=\textsf{Ber}(y \mid \textsf{sigmoid}(\mathbf w \cdot \mathbf x + w_0)),$ es decir, que la </span>
<span id="cb24-46"><a href="#cb24-46" aria-hidden="true" tabindex="-1"></a>clase $y$ esta modelada como una distribución Bernoulli con parámetro $\textsf{sigmoid}(\mathbf w \cdot \mathbf x + w_0).$ Siguiendo está definición que es equivalente a la mostrada <span class="co">[</span><span class="ot">anteriormente</span><span class="co">](/AprendizajeComputacional/capitulos/10Optimizacion/#sec:regresion-logistica-optimizacion)</span>, se puede modelar a un problema de multiples clases como $\mathbb P(\mathcal Y=y \mid \mathcal X=\mathbf x)=\textsf{Cat}(y \mid \textsf{softmax}(W \mathbf x + \mathbf w_0)),$ es decir, que la clase proviene de una distribución Categórica con parámetros $\textsf{softmax}(W \mathbf x + \mathbf w_0),$ donde $W \in \mathbb R^{K \times d},$ $\mathbf x \in \mathbb R^d$ y $\mathbf w_0 \in \mathbb R^d.$ </span>
<span id="cb24-47"><a href="#cb24-47" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-48"><a href="#cb24-48" aria-hidden="true" tabindex="-1"></a>La función $\textsf{softmax}(\mathbf v)$, donde $\mathbf v = W \mathbf x + \mathbf w_0$ está definida como:</span>
<span id="cb24-49"><a href="#cb24-49" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-50"><a href="#cb24-50" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb24-51"><a href="#cb24-51" aria-hidden="true" tabindex="-1"></a>\mathbf v_i = \frac{\exp \mathbf v_i}{\sum_{j=1}^K \exp \mathbf v_j}.</span>
<span id="cb24-52"><a href="#cb24-52" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb24-53"><a href="#cb24-53" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-54"><a href="#cb24-54" aria-hidden="true" tabindex="-1"></a>La función <span class="in">`jax.nn.softmax`</span> implementa $\textsf{softmax}$; en el siguiente ejemplo se calcula para el vector <span class="in">`[2, 1, 3]`</span> </span>
<span id="cb24-55"><a href="#cb24-55" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-58"><a href="#cb24-58" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb24-59"><a href="#cb24-59" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: true</span></span>
<span id="cb24-60"><a href="#cb24-60" aria-hidden="true" tabindex="-1"></a>jax.nn.softmax(np.array([<span class="dv">2</span>, <span class="dv">1</span>, <span class="dv">3</span>]))</span>
<span id="cb24-61"><a href="#cb24-61" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb24-62"><a href="#cb24-62" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-63"><a href="#cb24-63" aria-hidden="true" tabindex="-1"></a>Se puede observar que $\textsf{softmax}$ transforma los valores del vector $\mathbf v$ en probabilidades. </span>
<span id="cb24-64"><a href="#cb24-64" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-65"><a href="#cb24-65" aria-hidden="true" tabindex="-1"></a>Para seguir explicando este tipo de regresión logística se utilizará el problema del Iris, el cual se obtiene de la siguiente manera, es importante notar que las entradas están normalizadas para tener media cero y desviación estándar uno. </span>
<span id="cb24-66"><a href="#cb24-66" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-69"><a href="#cb24-69" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb24-70"><a href="#cb24-70" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: true</span></span>
<span id="cb24-71"><a href="#cb24-71" aria-hidden="true" tabindex="-1"></a>D, y <span class="op">=</span> load_iris(return_X_y<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb24-72"><a href="#cb24-72" aria-hidden="true" tabindex="-1"></a>normalize <span class="op">=</span> StandardScaler().fit(D)</span>
<span id="cb24-73"><a href="#cb24-73" aria-hidden="true" tabindex="-1"></a>D <span class="op">=</span> normalize.transform(D)</span>
<span id="cb24-74"><a href="#cb24-74" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb24-75"><a href="#cb24-75" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-76"><a href="#cb24-76" aria-hidden="true" tabindex="-1"></a>El siguiente paso es generar el modelo de la Regresión Logística Multinomial, el cual depende de una matriz de coeficientes $W \in \mathbb R^{K \times d}$ y $\mathbf w_0 \in \mathbb R^d.$ Los parámetros iniciales se puede generar con la función <span class="in">`parametros_iniciales`</span> tal y como se muestra a continuación.</span>
<span id="cb24-77"><a href="#cb24-77" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-80"><a href="#cb24-80" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb24-81"><a href="#cb24-81" aria-hidden="true" tabindex="-1"></a><span class="co">#|  echo: true</span></span>
<span id="cb24-82"><a href="#cb24-82" aria-hidden="true" tabindex="-1"></a>n_labels <span class="op">=</span> np.unique(y).shape[<span class="dv">0</span>]</span>
<span id="cb24-83"><a href="#cb24-83" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> parametros_iniciales(key<span class="op">=</span><span class="dv">0</span>):</span>
<span id="cb24-84"><a href="#cb24-84" aria-hidden="true" tabindex="-1"></a>    key <span class="op">=</span> jax.random.PRNGKey(key)</span>
<span id="cb24-85"><a href="#cb24-85" aria-hidden="true" tabindex="-1"></a>    d <span class="op">=</span> D.shape[<span class="dv">1</span>]</span>
<span id="cb24-86"><a href="#cb24-86" aria-hidden="true" tabindex="-1"></a>    params <span class="op">=</span> []</span>
<span id="cb24-87"><a href="#cb24-87" aria-hidden="true" tabindex="-1"></a>    normal <span class="op">=</span> jax.random.normal</span>
<span id="cb24-88"><a href="#cb24-88" aria-hidden="true" tabindex="-1"></a>    sqrt <span class="op">=</span> jnp.sqrt</span>
<span id="cb24-89"><a href="#cb24-89" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(n_labels):</span>
<span id="cb24-90"><a href="#cb24-90" aria-hidden="true" tabindex="-1"></a>        key, subkey <span class="op">=</span> jax.random.split(key)</span>
<span id="cb24-91"><a href="#cb24-91" aria-hidden="true" tabindex="-1"></a>        _ <span class="op">=</span> <span class="bu">dict</span>(w<span class="op">=</span>normal(subkey, (d, )) <span class="op">*</span> sqrt(<span class="dv">2</span> <span class="op">/</span> d),</span>
<span id="cb24-92"><a href="#cb24-92" aria-hidden="true" tabindex="-1"></a>                 w0<span class="op">=</span>jnp.ones(<span class="dv">1</span>))</span>
<span id="cb24-93"><a href="#cb24-93" aria-hidden="true" tabindex="-1"></a>        params.append(_)</span>
<span id="cb24-94"><a href="#cb24-94" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> params</span>
<span id="cb24-95"><a href="#cb24-95" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb24-96"><a href="#cb24-96" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-97"><a href="#cb24-97" aria-hidden="true" tabindex="-1"></a>Utilizando los parámetros en el formato anterior, hace que el modelo se pueda implementar con las siguientes instrucciones. Donde el ciclo es por cada uno de los parámetros de las $K$ clases y la última línea calcula el $\textsf{softmax}.$</span>
<span id="cb24-98"><a href="#cb24-98" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-101"><a href="#cb24-101" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb24-102"><a href="#cb24-102" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: true</span></span>
<span id="cb24-103"><a href="#cb24-103" aria-hidden="true" tabindex="-1"></a><span class="at">@jax.jit</span></span>
<span id="cb24-104"><a href="#cb24-104" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> modelo(params, X):</span>
<span id="cb24-105"><a href="#cb24-105" aria-hidden="true" tabindex="-1"></a>    o <span class="op">=</span> []</span>
<span id="cb24-106"><a href="#cb24-106" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> p <span class="kw">in</span> params:</span>
<span id="cb24-107"><a href="#cb24-107" aria-hidden="true" tabindex="-1"></a>        o.append(X <span class="op">@</span> p[<span class="st">'w'</span>] <span class="op">+</span> p[<span class="st">'w0'</span>])</span>
<span id="cb24-108"><a href="#cb24-108" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> jax.nn.softmax(jnp.array(o), axis<span class="op">=</span><span class="dv">0</span>).T</span>
<span id="cb24-109"><a href="#cb24-109" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb24-110"><a href="#cb24-110" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-111"><a href="#cb24-111" aria-hidden="true" tabindex="-1"></a>Una característica importante es que la función de perdida, en este caso, a la Entropía Cruzada (@sec-entropia-cruzada), requiere codificada la probabilidad de cada clase en un vector, donde el índice con probabilidad $1$ corresponde a la clase, esto se puede realizar con el siguiente código. </span>
<span id="cb24-112"><a href="#cb24-112" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-115"><a href="#cb24-115" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb24-116"><a href="#cb24-116" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: true</span></span>
<span id="cb24-117"><a href="#cb24-117" aria-hidden="true" tabindex="-1"></a>y_oh <span class="op">=</span> jax.nn.one_hot(y, n_labels)</span>
<span id="cb24-118"><a href="#cb24-118" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb24-119"><a href="#cb24-119" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-120"><a href="#cb24-120" aria-hidden="true" tabindex="-1"></a>Ahora se cuenta con todos los elementos para implementar la función de Entropía Cruzada para múltiples clases, la cual se muestra en el siguiente fragmento. </span>
<span id="cb24-121"><a href="#cb24-121" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-124"><a href="#cb24-124" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb24-125"><a href="#cb24-125" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: true</span></span>
<span id="cb24-126"><a href="#cb24-126" aria-hidden="true" tabindex="-1"></a><span class="at">@jax.jit</span></span>
<span id="cb24-127"><a href="#cb24-127" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> media_entropia_cruzada(params, X, y_oh):</span>
<span id="cb24-128"><a href="#cb24-128" aria-hidden="true" tabindex="-1"></a>    hy <span class="op">=</span> modelo(params, X)</span>
<span id="cb24-129"><a href="#cb24-129" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="op">-</span> (y_oh <span class="op">*</span> jnp.log(hy)).<span class="bu">sum</span>(axis<span class="op">=</span><span class="dv">1</span>).mean()</span>
<span id="cb24-130"><a href="#cb24-130" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb24-131"><a href="#cb24-131" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-132"><a href="#cb24-132" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-133"><a href="#cb24-133" aria-hidden="true" tabindex="-1"></a><span class="fu">### Optimización</span></span>
<span id="cb24-134"><a href="#cb24-134" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-135"><a href="#cb24-135" aria-hidden="true" tabindex="-1"></a>El siguiente paso es encontrar los parámetros del modelo, para esto se utiliza el método de optimización visto en Regresión Logística ( @sec-regresion-logistica-optimizacion) con algunos ajustes. Lo primero es que se desarrolla todo en una función <span class="in">`fit`</span> que recibe el parámetro $\eta$, los parámetros a identificar y el número de épocas, es decir, el número de iteraciones que se va a realizar el procedimiento. </span>
<span id="cb24-136"><a href="#cb24-136" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-137"><a href="#cb24-137" aria-hidden="true" tabindex="-1"></a>Dentro de la función <span class="in">`fit`</span>se observa la función <span class="in">`update`</span> que calcula los nuevos parámetros, también regresa el valor del error, esto para poder visualizar como va aprendiendo el modelo. La primera linea después de la función <span class="in">`update`</span> genera la función que calculará el valor y el gradiente de la función <span class="in">`media_entropia_cruzada`</span>. Finalmente viene el ciclo donde se realizan las actualizaciones de los parámetros y se guarda el error calculado en cada época. </span>
<span id="cb24-138"><a href="#cb24-138" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-141"><a href="#cb24-141" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb24-142"><a href="#cb24-142" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: true</span></span>
<span id="cb24-143"><a href="#cb24-143" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> fit(eta, params, epocas<span class="op">=</span><span class="dv">500</span>):</span>
<span id="cb24-144"><a href="#cb24-144" aria-hidden="true" tabindex="-1"></a>    <span class="at">@jax.jit</span></span>
<span id="cb24-145"><a href="#cb24-145" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> update(params, eta, X, y_oh):</span>
<span id="cb24-146"><a href="#cb24-146" aria-hidden="true" tabindex="-1"></a>        _ , gs <span class="op">=</span> error_grad(params, X, y_oh)</span>
<span id="cb24-147"><a href="#cb24-147" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> _, jax.tree_map(<span class="kw">lambda</span> p, g: p <span class="op">-</span> eta <span class="op">*</span> g,</span>
<span id="cb24-148"><a href="#cb24-148" aria-hidden="true" tabindex="-1"></a>                               params, gs)</span>
<span id="cb24-149"><a href="#cb24-149" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-150"><a href="#cb24-150" aria-hidden="true" tabindex="-1"></a>    error_grad  <span class="op">=</span> jax.value_and_grad(media_entropia_cruzada)</span>
<span id="cb24-151"><a href="#cb24-151" aria-hidden="true" tabindex="-1"></a>    error <span class="op">=</span> []</span>
<span id="cb24-152"><a href="#cb24-152" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(epocas):</span>
<span id="cb24-153"><a href="#cb24-153" aria-hidden="true" tabindex="-1"></a>        _, params <span class="op">=</span> update(params, eta, D, y_oh)</span>
<span id="cb24-154"><a href="#cb24-154" aria-hidden="true" tabindex="-1"></a>        error.append(_)</span>
<span id="cb24-155"><a href="#cb24-155" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> params, error</span>
<span id="cb24-156"><a href="#cb24-156" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb24-157"><a href="#cb24-157" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-158"><a href="#cb24-158" aria-hidden="true" tabindex="-1"></a><span class="fu">### Optimización Método Adam {#sec-adam}</span></span>
<span id="cb24-159"><a href="#cb24-159" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-160"><a href="#cb24-160" aria-hidden="true" tabindex="-1"></a>Como se había visto en la @sec-actualizacion-parametros existen diferentes métodos para encontrar los parámetros, en particular en esta sección se utilizará el método Adam (implementado en la librería <span class="co">[</span><span class="ot">optax</span><span class="co">](https://optax.readthedocs.io)</span>) para encontrar los parámetros de la Regresión Logística Multinomial. Se decide utilizar este método dado que su uso es frecuente en la identificación de parámetros de redes neuronales. </span>
<span id="cb24-161"><a href="#cb24-161" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-162"><a href="#cb24-162" aria-hidden="true" tabindex="-1"></a>Siguiendo la misma estructura que la función <span class="in">`fit`</span>, la función <span class="in">`adam`</span> recibe tres parámetros el primer es la instancia de optimizador, la segunda son los parámetros y finalmente el número de épocas que se va a ejecutar. La primera línea de la función <span class="in">`update`</span> (que se encuentra en <span class="in">`adam`</span>) calcula el valor de la función de error y su gradiente, estos son utilizados por <span class="in">`optimizer.update`</span> para calcular la actualización de parámetros así como el nuevo estado del optimizador, los nuevos parámetros son calculados en la tercera línea y la función regresa los nuevos parámetros, el estado del optimizador y el error en esa iteración. La primera línea después de <span class="in">`update`</span> inicializa el optimizador, después se general la función que calculará el valor y gradiente de la función <span class="in">`media_entropia_cruzada`</span>. El ciclo llama la función <span class="in">`update`</span> y guarda el error encontrado en cada época. </span>
<span id="cb24-163"><a href="#cb24-163" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-166"><a href="#cb24-166" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb24-167"><a href="#cb24-167" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: true</span></span>
<span id="cb24-168"><a href="#cb24-168" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-169"><a href="#cb24-169" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> adam(optimizer, params, epocas<span class="op">=</span><span class="dv">500</span>):</span>
<span id="cb24-170"><a href="#cb24-170" aria-hidden="true" tabindex="-1"></a>    <span class="at">@jax.jit</span></span>
<span id="cb24-171"><a href="#cb24-171" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> update(params, opt_state, X, y_oh):</span>
<span id="cb24-172"><a href="#cb24-172" aria-hidden="true" tabindex="-1"></a>        loss_value, grads <span class="op">=</span> error_grad(params,</span>
<span id="cb24-173"><a href="#cb24-173" aria-hidden="true" tabindex="-1"></a>                                       X,</span>
<span id="cb24-174"><a href="#cb24-174" aria-hidden="true" tabindex="-1"></a>                                       y_oh)</span>
<span id="cb24-175"><a href="#cb24-175" aria-hidden="true" tabindex="-1"></a>        updates, opt_state <span class="op">=</span> optimizer.update(grads,</span>
<span id="cb24-176"><a href="#cb24-176" aria-hidden="true" tabindex="-1"></a>                                              opt_state,</span>
<span id="cb24-177"><a href="#cb24-177" aria-hidden="true" tabindex="-1"></a>                                              params)</span>
<span id="cb24-178"><a href="#cb24-178" aria-hidden="true" tabindex="-1"></a>        params <span class="op">=</span> optax.apply_updates(params, updates)</span>
<span id="cb24-179"><a href="#cb24-179" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> params, opt_state, loss_value</span>
<span id="cb24-180"><a href="#cb24-180" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb24-181"><a href="#cb24-181" aria-hidden="true" tabindex="-1"></a>    opt_state <span class="op">=</span> optimizer.init(params)</span>
<span id="cb24-182"><a href="#cb24-182" aria-hidden="true" tabindex="-1"></a>    error_grad  <span class="op">=</span> jax.value_and_grad(media_entropia_cruzada)    </span>
<span id="cb24-183"><a href="#cb24-183" aria-hidden="true" tabindex="-1"></a>    error <span class="op">=</span> []</span>
<span id="cb24-184"><a href="#cb24-184" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(epocas):</span>
<span id="cb24-185"><a href="#cb24-185" aria-hidden="true" tabindex="-1"></a>        params, opt_state, loss_value <span class="op">=</span> update(params,</span>
<span id="cb24-186"><a href="#cb24-186" aria-hidden="true" tabindex="-1"></a>                                               opt_state,</span>
<span id="cb24-187"><a href="#cb24-187" aria-hidden="true" tabindex="-1"></a>                                               D,</span>
<span id="cb24-188"><a href="#cb24-188" aria-hidden="true" tabindex="-1"></a>                                               y_oh)</span>
<span id="cb24-189"><a href="#cb24-189" aria-hidden="true" tabindex="-1"></a>        error.append(loss_value)</span>
<span id="cb24-190"><a href="#cb24-190" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> params, error</span>
<span id="cb24-191"><a href="#cb24-191" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb24-192"><a href="#cb24-192" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-193"><a href="#cb24-193" aria-hidden="true" tabindex="-1"></a><span class="fu">### Comparación entre Optimizadores</span></span>
<span id="cb24-194"><a href="#cb24-194" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-195"><a href="#cb24-195" aria-hidden="true" tabindex="-1"></a>Los optimizadores descritos anteriormente se pueden utilizar con el siguiente código, donde la primera línea calcula los parámetros iniciales, después se llama a la función <span class="in">`fit`</span> para encontrar los parámetros con el primer método. La tercera línea genera una instancia del optimizador Adam; el cual se pasa a la función <span class="in">`adam`</span> para encontrar los parámetros con este método. </span>
<span id="cb24-196"><a href="#cb24-196" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-199"><a href="#cb24-199" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb24-200"><a href="#cb24-200" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: true</span></span>
<span id="cb24-201"><a href="#cb24-201" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-202"><a href="#cb24-202" aria-hidden="true" tabindex="-1"></a>params <span class="op">=</span> parametros_iniciales()</span>
<span id="cb24-203"><a href="#cb24-203" aria-hidden="true" tabindex="-1"></a>p1, error1 <span class="op">=</span> fit(<span class="fl">1e-2</span>, params)</span>
<span id="cb24-204"><a href="#cb24-204" aria-hidden="true" tabindex="-1"></a>optimizer <span class="op">=</span> optax.adam(learning_rate<span class="op">=</span><span class="fl">1e-2</span>)</span>
<span id="cb24-205"><a href="#cb24-205" aria-hidden="true" tabindex="-1"></a>p2, error2 <span class="op">=</span> adam(optimizer, params)</span>
<span id="cb24-206"><a href="#cb24-206" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb24-207"><a href="#cb24-207" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-208"><a href="#cb24-208" aria-hidden="true" tabindex="-1"></a>La @fig-redes-neuronales-adam muestra cómo la media de la Entropía Cruzada se minimiza con respecto a las épocas para los dos métodos. Se puede observar como el método <span class="in">`adam`</span> converge más rápido y llega a un valor menor de Entropía Cruzada. </span>
<span id="cb24-209"><a href="#cb24-209" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-212"><a href="#cb24-212" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb24-213"><a href="#cb24-213" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-fold: true</span></span>
<span id="cb24-214"><a href="#cb24-214" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: Comparación de diferentes optimizadores.</span></span>
<span id="cb24-215"><a href="#cb24-215" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-redes-neuronales-adam</span></span>
<span id="cb24-216"><a href="#cb24-216" aria-hidden="true" tabindex="-1"></a>df <span class="op">=</span> pd.DataFrame(<span class="bu">dict</span>(entropia<span class="op">=</span>np.array(error1),</span>
<span id="cb24-217"><a href="#cb24-217" aria-hidden="true" tabindex="-1"></a>                       optimizador<span class="op">=</span><span class="st">'fit'</span>,</span>
<span id="cb24-218"><a href="#cb24-218" aria-hidden="true" tabindex="-1"></a>                       epoca<span class="op">=</span>np.arange(<span class="dv">1</span>, <span class="dv">501</span>)))</span>
<span id="cb24-219"><a href="#cb24-219" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-220"><a href="#cb24-220" aria-hidden="true" tabindex="-1"></a>df <span class="op">=</span> pd.concat((df, pd.DataFrame(<span class="bu">dict</span>(entropia<span class="op">=</span>np.array(error2),</span>
<span id="cb24-221"><a href="#cb24-221" aria-hidden="true" tabindex="-1"></a>                                      optimizador<span class="op">=</span><span class="st">'adam'</span>,</span>
<span id="cb24-222"><a href="#cb24-222" aria-hidden="true" tabindex="-1"></a>                                      epoca<span class="op">=</span>np.arange(<span class="dv">1</span>, <span class="dv">501</span>)))))</span>
<span id="cb24-223"><a href="#cb24-223" aria-hidden="true" tabindex="-1"></a>_ <span class="op">=</span> sns.relplot(data<span class="op">=</span>df, x<span class="op">=</span><span class="st">'epoca'</span>, y<span class="op">=</span><span class="st">'entropia'</span>, hue<span class="op">=</span><span class="st">'optimizador'</span>, kind<span class="op">=</span><span class="st">'line'</span>)</span>
<span id="cb24-224"><a href="#cb24-224" aria-hidden="true" tabindex="-1"></a><span class="in">```</span>  </span>
<span id="cb24-225"><a href="#cb24-225" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-228"><a href="#cb24-228" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb24-229"><a href="#cb24-229" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: false</span></span>
<span id="cb24-230"><a href="#cb24-230" aria-hidden="true" tabindex="-1"></a>_ <span class="op">=</span> (y <span class="op">==</span> modelo(p1, D).argmax(axis<span class="op">=</span><span class="dv">1</span>)).mean()</span>
<span id="cb24-231"><a href="#cb24-231" aria-hidden="true" tabindex="-1"></a>acc_fit_f <span class="op">=</span> Markdown(<span class="ss">f'$</span><span class="sc">{</span>_<span class="sc">:0.4f}</span><span class="ss">$'</span>)</span>
<span id="cb24-232"><a href="#cb24-232" aria-hidden="true" tabindex="-1"></a>_ <span class="op">=</span> (y <span class="op">==</span> modelo(p2, D).argmax(axis<span class="op">=</span><span class="dv">1</span>)).mean()</span>
<span id="cb24-233"><a href="#cb24-233" aria-hidden="true" tabindex="-1"></a>acc_adam_f <span class="op">=</span> Markdown(<span class="ss">f'$</span><span class="sc">{</span>_<span class="sc">:0.4f}</span><span class="ss">$'</span>)</span>
<span id="cb24-234"><a href="#cb24-234" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb24-235"><a href="#cb24-235" aria-hidden="true" tabindex="-1"></a>Finalmente la exactitud (@sec-accuracy) en el conjunto de entrenamiento del modelo estimado con <span class="in">`fit`</span> es <span class="in">`{python} acc_fit_f`</span> (i.e., <span class="in">`(y == modelo(p1, D).argmax(axis=1)).mean()`</span>) y del estimado con <span class="in">`adam`</span> es <span class="in">`{python} acc_adam_f`</span>.</span>
<span id="cb24-236"><a href="#cb24-236" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-237"><a href="#cb24-237" aria-hidden="true" tabindex="-1"></a><span class="fu">## Perceptrón {#sec-preceptron}</span></span>
<span id="cb24-238"><a href="#cb24-238" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-239"><a href="#cb24-239" aria-hidden="true" tabindex="-1"></a>La unidad básica de procesamiento en una red neuronal es el perceptrón, el cual es un viejo conocido de Discriminantes Lineales (@sec-discriminantes-lineales), es decir,  $g(\mathbf x) = \mathbf w \cdot \mathbf x + \mathbf w_0.$ En problemas de clasificación binaria se encuentran los parámetros de $g(\mathbf x)$ de tal manera que genera un hiperplano y se clasifican los elementos de acuerdo al lado positivo o negativo del hiperplano. En problemas de Regresión (@sec-regresion-ols) los parámetros de $g(\mathbf x)$ se encuentran utilizando mínimos cuadrados. </span>
<span id="cb24-240"><a href="#cb24-240" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-241"><a href="#cb24-241" aria-hidden="true" tabindex="-1"></a>En el case de tener $K&gt;2$ clases entonces el problema se puede afrontar entrenando $g_k(\mathbf x)$ perceptrones ($k=1, \ldots, K$) tal y como se realizó Discriminantes Lineales (@sec-multiples-clases). De manera concisa se puede definir a $g: \mathbb R^d \rightarrow \mathbb R^K$, es decir, $g(\mathbf x)=W \mathbf x + \mathbf w_0$ tal y como se realizó en Regresión Logística Multinomial (@sec-regresion-logistica-multinomial). En el caso de desear conocer la probabilidad de pertenencia a una clase, en el caso binario se utilizó $g(\mathbf x) = \textsf{sigmoid}(\mathbf w \cdot \mathbf x + \mathbf w_0)$ y en el caso multiclase $g(\mathbf x) = \textsf{softmax}(W \mathbf x + \mathbf w_0).$</span>
<span id="cb24-242"><a href="#cb24-242" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-243"><a href="#cb24-243" aria-hidden="true" tabindex="-1"></a><span class="fu">### Composición de Perceptrones Lineales</span></span>
<span id="cb24-244"><a href="#cb24-244" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-245"><a href="#cb24-245" aria-hidden="true" tabindex="-1"></a>Se puede realizar una composición de perceptrones de la siguiente manera, sea $g_1: \mathbb R^d \rightarrow \mathbb R^{d'}$ y $g_2: \mathbb R^{d'} \rightarrow \mathbb R^K,$ es decir $g = g_2 \circ g_1.$ Realizando está composición en las ecuaciones descritas anteriormente se tiene </span>
<span id="cb24-246"><a href="#cb24-246" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-247"><a href="#cb24-247" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb24-248"><a href="#cb24-248" aria-hidden="true" tabindex="-1"></a>\begin{split}</span>
<span id="cb24-249"><a href="#cb24-249" aria-hidden="true" tabindex="-1"></a>\hat{\mathbf{y}}_1 &amp;= g_1(\mathbf x) <span class="sc">\\</span></span>
<span id="cb24-250"><a href="#cb24-250" aria-hidden="true" tabindex="-1"></a>\hat{\mathbf y} &amp;= g_2(\hat{\mathbf{y}}_1)</span>
<span id="cb24-251"><a href="#cb24-251" aria-hidden="true" tabindex="-1"></a>\end{split}</span>
<span id="cb24-252"><a href="#cb24-252" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb24-253"><a href="#cb24-253" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-254"><a href="#cb24-254" aria-hidden="true" tabindex="-1"></a>Expandiendo las ecuaciones anteriores se tiene </span>
<span id="cb24-255"><a href="#cb24-255" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-256"><a href="#cb24-256" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb24-257"><a href="#cb24-257" aria-hidden="true" tabindex="-1"></a>\begin{split}</span>
<span id="cb24-258"><a href="#cb24-258" aria-hidden="true" tabindex="-1"></a>\hat{\mathbf{y}}_1 &amp;= W_1 \mathbf x + \mathbf w_{1_0}<span class="sc">\\</span></span>
<span id="cb24-259"><a href="#cb24-259" aria-hidden="true" tabindex="-1"></a>\hat{\mathbf y} &amp;= W_2 \hat{\mathbf{y}}_1 + \mathbf w_{2_0}<span class="sc">\\</span></span>
<span id="cb24-260"><a href="#cb24-260" aria-hidden="true" tabindex="-1"></a>&amp;= W_2 (W_1 \mathbf x + \mathbf w_{1_0}) + \mathbf w_{2_0}<span class="sc">\\</span></span>
<span id="cb24-261"><a href="#cb24-261" aria-hidden="true" tabindex="-1"></a>&amp;= W_2 W_1 \mathbf x + W_2 \mathbf w_{1_0} + \mathbf w_{2_0}<span class="sc">\\</span></span>
<span id="cb24-262"><a href="#cb24-262" aria-hidden="true" tabindex="-1"></a>&amp;= W \mathbf x + \mathbf w_0</span>
<span id="cb24-263"><a href="#cb24-263" aria-hidden="true" tabindex="-1"></a>\end{split}</span>
<span id="cb24-264"><a href="#cb24-264" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb24-265"><a href="#cb24-265" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-266"><a href="#cb24-266" aria-hidden="true" tabindex="-1"></a>como se puede observar la composición realizada da como resultado una red donde se tienen que identificar $W \in \mathbb R^{K \times d}$ y $\mathbf w_0 \in \mathbb R^d,$ es decir, son $K$ perceptrones equivalentes al modelado de Regresión Logística Multinomial. Esto es porque la composición fue con funciones lineales.</span>
<span id="cb24-267"><a href="#cb24-267" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-268"><a href="#cb24-268" aria-hidden="true" tabindex="-1"></a><span class="fu">## Perceptrón Multicapa</span></span>
<span id="cb24-269"><a href="#cb24-269" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-270"><a href="#cb24-270" aria-hidden="true" tabindex="-1"></a>Para evitar que la composición de perceptrones colapsen a una función equivalente, es necesario incluir una función no lineal, sea $\phi$ esta función no lineal, (a esta función se le conoce como **función de activación**) entonces se puede observar que la composición $g=g_2 \circ g_1$ donde $g_1 = \phi(W_1 \mathbf x + \mathbf w_{1_0})$ resulta en</span>
<span id="cb24-271"><a href="#cb24-271" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-272"><a href="#cb24-272" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb24-273"><a href="#cb24-273" aria-hidden="true" tabindex="-1"></a>\begin{split}</span>
<span id="cb24-274"><a href="#cb24-274" aria-hidden="true" tabindex="-1"></a>\hat{\mathbf{y}}_1 &amp;= \phi(W_1 \mathbf x + \mathbf w_{1_0}) <span class="sc">\\</span></span>
<span id="cb24-275"><a href="#cb24-275" aria-hidden="true" tabindex="-1"></a>\hat{\mathbf y} &amp;= W_2 \hat{\mathbf{y}}_1 + \mathbf w_{2_0}</span>
<span id="cb24-276"><a href="#cb24-276" aria-hidden="true" tabindex="-1"></a>\end{split}.</span>
<span id="cb24-277"><a href="#cb24-277" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb24-278"><a href="#cb24-278" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-279"><a href="#cb24-279" aria-hidden="true" tabindex="-1"></a>A la estructura anterior se le conoce como una red neuronal de una capa oculta, la salida de la capa oculta está en $\hat{\mathbf{y}}_1$ y la salida de la red es $\hat{\mathbf y}.$ Siguiendo la notación anterior se puede definir una red neuronal con dos capas ocultas de la siguiente manera</span>
<span id="cb24-280"><a href="#cb24-280" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-281"><a href="#cb24-281" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb24-282"><a href="#cb24-282" aria-hidden="true" tabindex="-1"></a>\begin{split}</span>
<span id="cb24-283"><a href="#cb24-283" aria-hidden="true" tabindex="-1"></a>\hat{\mathbf{y}}_1 &amp;= \phi(W_1 \mathbf x + \mathbf w_{1_0}) <span class="sc">\\</span></span>
<span id="cb24-284"><a href="#cb24-284" aria-hidden="true" tabindex="-1"></a>\hat{\mathbf y}_2 &amp;= \phi_2(W_2 \hat{\mathbf{y}}_1 + \mathbf w_{2_0})<span class="sc">\\</span></span>
<span id="cb24-285"><a href="#cb24-285" aria-hidden="true" tabindex="-1"></a>\hat{\mathbf y} &amp;= (W_3 \hat{\mathbf{y}}_2 + \mathbf w_{3_0})</span>
<span id="cb24-286"><a href="#cb24-286" aria-hidden="true" tabindex="-1"></a>\end{split},</span>
<span id="cb24-287"><a href="#cb24-287" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb24-288"><a href="#cb24-288" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-289"><a href="#cb24-289" aria-hidden="true" tabindex="-1"></a>donde la salida de la primera capa oculta ($\hat{\mathbf{y}}_1$) es la entrada de la segunda capa oculta y su salida ($\hat{\mathbf{y}}_2$) se convierte en la entrada de la capa de salida. </span>
<span id="cb24-290"><a href="#cb24-290" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-291"><a href="#cb24-291" aria-hidden="true" tabindex="-1"></a><span class="fu">### Desvanecimiento del Gradiente</span></span>
<span id="cb24-292"><a href="#cb24-292" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-293"><a href="#cb24-293" aria-hidden="true" tabindex="-1"></a>Por lo expuesto hasta el momento se podría pensar que una candidata para ser la función $\phi$ es la función $\textsf{sigmoid},$ aunque esto es factible, esta presenta el problema de desvanecimiento del gradiente. Para ejemplificar este problema, se utilizan dos funciones $g_1(x) = w_1 x + 1.0$ y $g_1(x) = w_2 x + 1.0;$ haciéndose la composición de estas dos funciones $g=g_2 \circ g_1.$ </span>
<span id="cb24-294"><a href="#cb24-294" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-295"><a href="#cb24-295" aria-hidden="true" tabindex="-1"></a>Las siguientes instrucciones implementan las funciones anteriores utilizando la librería <span class="co">[</span><span class="ot">JAX</span><span class="co">](https://jax.readthedocs.io)</span>.</span>
<span id="cb24-296"><a href="#cb24-296" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-299"><a href="#cb24-299" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb24-300"><a href="#cb24-300" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: true</span></span>
<span id="cb24-301"><a href="#cb24-301" aria-hidden="true" tabindex="-1"></a><span class="at">@jax.jit</span></span>
<span id="cb24-302"><a href="#cb24-302" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> g_1(params, x):</span>
<span id="cb24-303"><a href="#cb24-303" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> jax.nn.sigmoid(params[<span class="st">'w1'</span>] <span class="op">*</span> x <span class="op">+</span> <span class="fl">1.0</span>)</span>
<span id="cb24-304"><a href="#cb24-304" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-305"><a href="#cb24-305" aria-hidden="true" tabindex="-1"></a><span class="at">@jax.jit</span></span>
<span id="cb24-306"><a href="#cb24-306" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> g_2(params, x):</span>
<span id="cb24-307"><a href="#cb24-307" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> jax.nn.sigmoid(params[<span class="st">'w2'</span>] <span class="op">*</span> x <span class="op">+</span> <span class="fl">1.0</span>)</span>
<span id="cb24-308"><a href="#cb24-308" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-309"><a href="#cb24-309" aria-hidden="true" tabindex="-1"></a><span class="at">@jax.jit</span></span>
<span id="cb24-310"><a href="#cb24-310" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> g(params, x):</span>
<span id="cb24-311"><a href="#cb24-311" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> g_2(params, g_1(params, x))</span>
<span id="cb24-312"><a href="#cb24-312" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb24-313"><a href="#cb24-313" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-314"><a href="#cb24-314" aria-hidden="true" tabindex="-1"></a>Utilizando unos parámetros aleatorios generados con el siguiente código </span>
<span id="cb24-315"><a href="#cb24-315" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-318"><a href="#cb24-318" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb24-319"><a href="#cb24-319" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: true</span></span>
<span id="cb24-320"><a href="#cb24-320" aria-hidden="true" tabindex="-1"></a>key <span class="op">=</span> jax.random.PRNGKey(<span class="dv">0</span>)</span>
<span id="cb24-321"><a href="#cb24-321" aria-hidden="true" tabindex="-1"></a>key, subkey1, subkey2 <span class="op">=</span> jax.random.split(key, num<span class="op">=</span><span class="dv">3</span>)</span>
<span id="cb24-322"><a href="#cb24-322" aria-hidden="true" tabindex="-1"></a>params <span class="op">=</span> <span class="bu">dict</span>(w1<span class="op">=</span>jax.random.normal(subkey1),</span>
<span id="cb24-323"><a href="#cb24-323" aria-hidden="true" tabindex="-1"></a>              w2<span class="op">=</span>jax.random.normal(subkey2))</span>
<span id="cb24-324"><a href="#cb24-324" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb24-325"><a href="#cb24-325" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-326"><a href="#cb24-326" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-329"><a href="#cb24-329" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb24-330"><a href="#cb24-330" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: false</span></span>
<span id="cb24-331"><a href="#cb24-331" aria-hidden="true" tabindex="-1"></a>_ <span class="op">=</span> jax.grad(g_1)(params, <span class="fl">1.0</span>)</span>
<span id="cb24-332"><a href="#cb24-332" aria-hidden="true" tabindex="-1"></a>g1_f <span class="op">=</span> Markdown(<span class="ss">f'</span><span class="sc">{</span>_[<span class="st">"w1"</span>]<span class="sc">:0.4f}</span><span class="ss">'</span>)</span>
<span id="cb24-333"><a href="#cb24-333" aria-hidden="true" tabindex="-1"></a>_ <span class="op">=</span> jax.grad(g_2)(params, <span class="fl">1.0</span>)</span>
<span id="cb24-334"><a href="#cb24-334" aria-hidden="true" tabindex="-1"></a>g2_f <span class="op">=</span> Markdown(<span class="ss">f'</span><span class="sc">{</span>_[<span class="st">"w2"</span>]<span class="sc">:0.4f}</span><span class="ss">'</span>)</span>
<span id="cb24-335"><a href="#cb24-335" aria-hidden="true" tabindex="-1"></a>tot <span class="op">=</span> jax.grad(g)(params, <span class="fl">1.0</span>)</span>
<span id="cb24-336"><a href="#cb24-336" aria-hidden="true" tabindex="-1"></a>g12_f <span class="op">=</span> Markdown(<span class="ss">f'</span><span class="sc">{</span>tot[<span class="st">"w1"</span>]<span class="sc">:0.4f}</span><span class="ss">'</span>)</span>
<span id="cb24-337"><a href="#cb24-337" aria-hidden="true" tabindex="-1"></a>g22_f <span class="op">=</span> Markdown(<span class="ss">f'</span><span class="sc">{</span>tot[<span class="st">"w2"</span>]<span class="sc">:0.4f}</span><span class="ss">'</span>)</span>
<span id="cb24-338"><a href="#cb24-338" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb24-339"><a href="#cb24-339" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-340"><a href="#cb24-340" aria-hidden="true" tabindex="-1"></a>se tiene que la derivada de $g_1$ con respecto a $w_1$ (i.e., <span class="in">`jax.grad(g_1)(params, 1.0)`</span>) y $g_2$ con respecto a $w_2$ (i.e., <span class="in">`jax.grad(g_2)(params, 1.0)`</span>) es <span class="in">`{python} g1_f`</span> y <span class="in">`{python} g2_f`</span>, respectivamente. El problema viene cuando se calcula $\frac{\partial g}{\partial w_1}$ en este caso se obsevar que el gradiente es pequeño comparado con el gradiente obtenido en $\frac{d g}{d w_1},$ i.e., <span class="in">`jax.grad(g)(params, 1.0)`</span>, donde se observa que el gradiente para $w_1$ corresponde a <span class="in">`{python} g12_f`</span>, el gradiente de $w_2$ sigue en la misma mágnitud teniendo un valor de <span class="in">`{python} g22_f`</span>. </span>
<span id="cb24-341"><a href="#cb24-341" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-342"><a href="#cb24-342" aria-hidden="true" tabindex="-1"></a>El problema de desvanecimiento de gradiente, hace que el gradiente disminuya de manera exponencial, entonces los pesos asociados a las capas alejadas de la capa de salida reciben un gradiente equivalente a cero y no se cuenta con información para actualizar sus pesos. Por este motivo es recomendable utilizar funciones de activación que no presenten esta característica, una muy utilizada es $\textsf{ReLU}(x) = \max(0, x)$.</span>
<span id="cb24-343"><a href="#cb24-343" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-344"><a href="#cb24-344" aria-hidden="true" tabindex="-1"></a><span class="fu">## Ejemplo: Dígitos</span></span>
<span id="cb24-345"><a href="#cb24-345" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-346"><a href="#cb24-346" aria-hidden="true" tabindex="-1"></a>Para ejemplificar el uso de una red neuronal en un poblema de clasificación se utilizarán los datos de Dígitos, los cuales se pueden obtener con las siguientes instrucciones. </span>
<span id="cb24-347"><a href="#cb24-347" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-350"><a href="#cb24-350" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb24-351"><a href="#cb24-351" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: true</span></span>
<span id="cb24-352"><a href="#cb24-352" aria-hidden="true" tabindex="-1"></a>X, y <span class="op">=</span> load_digits(return_X_y<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb24-353"><a href="#cb24-353" aria-hidden="true" tabindex="-1"></a>T, G, y_t, y_g <span class="op">=</span> train_test_split(X, y, test_size<span class="op">=</span><span class="fl">0.2</span>)</span>
<span id="cb24-354"><a href="#cb24-354" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb24-355"><a href="#cb24-355" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-356"><a href="#cb24-356" aria-hidden="true" tabindex="-1"></a>Un procedimiento necesario en redes neuronales es que los datos estén normalizados, tradicionalmente esto se realiza haciendo que los datos tengan media cero y desviación estandar uno. En las siguientes lineas se normalizan los datos usando la clase <span class="in">`StandardScaler`</span> se puede observar que los parámetros para la normalización son encontrados en el conjunto de entrenamiento (<span class="in">`T`</span>) y aplicados tanto al conjunto de entrenamiento como el conjunto de prueba <span class="in">`G`</span>. </span>
<span id="cb24-357"><a href="#cb24-357" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-360"><a href="#cb24-360" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb24-361"><a href="#cb24-361" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: true</span></span>
<span id="cb24-362"><a href="#cb24-362" aria-hidden="true" tabindex="-1"></a>normalize <span class="op">=</span> StandardScaler().fit(T)</span>
<span id="cb24-363"><a href="#cb24-363" aria-hidden="true" tabindex="-1"></a>T <span class="op">=</span> normalize.transform(T)</span>
<span id="cb24-364"><a href="#cb24-364" aria-hidden="true" tabindex="-1"></a>G <span class="op">=</span> normalize.transform(G)</span>
<span id="cb24-365"><a href="#cb24-365" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb24-366"><a href="#cb24-366" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-367"><a href="#cb24-367" aria-hidden="true" tabindex="-1"></a>Como se realizó previamente es necesario convertir las clases de salida para que cada ejemplo sea un vector unitario donde el índice con el valor $1$ representa la clase, esto se realiza con las siguientes instrucciones. </span>
<span id="cb24-368"><a href="#cb24-368" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-371"><a href="#cb24-371" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb24-372"><a href="#cb24-372" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: true</span></span>
<span id="cb24-373"><a href="#cb24-373" aria-hidden="true" tabindex="-1"></a>n_labels <span class="op">=</span> np.unique(y).shape[<span class="dv">0</span>]</span>
<span id="cb24-374"><a href="#cb24-374" aria-hidden="true" tabindex="-1"></a>yt_oh <span class="op">=</span> jax.nn.one_hot(y_t, n_labels)</span>
<span id="cb24-375"><a href="#cb24-375" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb24-376"><a href="#cb24-376" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-379"><a href="#cb24-379" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb24-380"><a href="#cb24-380" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: false</span></span>
<span id="cb24-381"><a href="#cb24-381" aria-hidden="true" tabindex="-1"></a>t_shape_f <span class="op">=</span> Markdown(<span class="ss">f'$</span><span class="sc">{</span>T<span class="sc">.</span>shape[<span class="dv">1</span>]<span class="sc">}</span><span class="ss">$'</span>)</span>
<span id="cb24-382"><a href="#cb24-382" aria-hidden="true" tabindex="-1"></a>n_labels_f <span class="op">=</span> Markdown(<span class="ss">f'$</span><span class="sc">{</span>n_labels<span class="sc">}</span><span class="ss">$'</span>)</span>
<span id="cb24-383"><a href="#cb24-383" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb24-384"><a href="#cb24-384" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-385"><a href="#cb24-385" aria-hidden="true" tabindex="-1"></a>Es momento de decidir la estructura de la red neuronal, las únicas dos restricciones es que la primera capa tiene que tener la dimensión del vector de entrada, en esta caso corresponde a <span class="in">`{python} t_shape_f`</span> (<span class="in">`T.shape[1]`</span>) y la última capa tiene que tener de salida el número de clases, en este caso <span class="in">`{python} n_labels_f`</span> (<span class="in">`n_labels`</span>), el resto de las capas ocultas pueden tener cualquier valor solamente es necesario que la dimensiones sean coherentes con la operación que se va a realizar. </span>
<span id="cb24-386"><a href="#cb24-386" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-387"><a href="#cb24-387" aria-hidden="true" tabindex="-1"></a>La red que se va a implementar es la siguiente, como super-índice se encuentran las dimensiones para que sea más fácil seguir la estructura de la red. </span>
<span id="cb24-388"><a href="#cb24-388" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-389"><a href="#cb24-389" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb24-390"><a href="#cb24-390" aria-hidden="true" tabindex="-1"></a>\begin{split}</span>
<span id="cb24-391"><a href="#cb24-391" aria-hidden="true" tabindex="-1"></a>\hat{\mathbf{y}}_1^{32} &amp;= \phi(W^{32\times 64} \mathbf x^{64} + \mathbf w_{1_0}^{32})<span class="sc">\\</span></span>
<span id="cb24-392"><a href="#cb24-392" aria-hidden="true" tabindex="-1"></a>\hat{\mathbf{y}}_2^{16} &amp;= \phi(W^{16\times 32} \hat{\mathbf{y}}_1^{32} + \mathbf w_{2_0}^{16})<span class="sc">\\</span></span>
<span id="cb24-393"><a href="#cb24-393" aria-hidden="true" tabindex="-1"></a>\hat{\mathbf{y}}^{10} &amp;= W^{10\times 16} \hat{\mathbf{y}}_2^{16} + \mathbf w_{3_0}^{10}</span>
<span id="cb24-394"><a href="#cb24-394" aria-hidden="true" tabindex="-1"></a>\end{split}</span>
<span id="cb24-395"><a href="#cb24-395" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb24-396"><a href="#cb24-396" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-397"><a href="#cb24-397" aria-hidden="true" tabindex="-1"></a>Considerando que las entradas se encuentran en una matrix $X^{N\times64},$ entonces se puede definir esta estructura en términos de múltiplicación de matrices, lo cual queda como</span>
<span id="cb24-398"><a href="#cb24-398" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-399"><a href="#cb24-399" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb24-400"><a href="#cb24-400" aria-hidden="true" tabindex="-1"></a>\begin{split}</span>
<span id="cb24-401"><a href="#cb24-401" aria-hidden="true" tabindex="-1"></a>\hat{Y}_1^{N\times 32} &amp;= \phi(X^{N\times 64} W^{64\times 32} + \mathbf w_{1_0}^{32})<span class="sc">\\</span></span>
<span id="cb24-402"><a href="#cb24-402" aria-hidden="true" tabindex="-1"></a>\hat{Y}_2^{N\times 16} &amp;= \phi(\hat{Y}_1^{N\times 32} W^{32\times 16} + \mathbf w_{2_0}^{16})<span class="sc">\\</span></span>
<span id="cb24-403"><a href="#cb24-403" aria-hidden="true" tabindex="-1"></a>\hat{Y}^{N\times 10} &amp;= \hat{Y}_2^{N\times 16} W^{16\times 10} + \mathbf w_{3_0}^{10}<span class="sc">\\</span></span>
<span id="cb24-404"><a href="#cb24-404" aria-hidden="true" tabindex="-1"></a>\end{split},</span>
<span id="cb24-405"><a href="#cb24-405" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb24-406"><a href="#cb24-406" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-407"><a href="#cb24-407" aria-hidden="true" tabindex="-1"></a>donde la suma con el término $\mathbf w_0$ se realiza en la dimensión que corresponde y se replica tantas veces para cumplir con la otra dimensión. Esta configuración se puede expresar en un lista como la que se muestra a continuación. </span>
<span id="cb24-408"><a href="#cb24-408" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-411"><a href="#cb24-411" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb24-412"><a href="#cb24-412" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: true</span></span>
<span id="cb24-413"><a href="#cb24-413" aria-hidden="true" tabindex="-1"></a>d <span class="op">=</span> [<span class="dv">64</span>, <span class="dv">32</span>, <span class="dv">16</span>, <span class="dv">10</span>]</span>
<span id="cb24-414"><a href="#cb24-414" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb24-415"><a href="#cb24-415" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-416"><a href="#cb24-416" aria-hidden="true" tabindex="-1"></a>Utilizando esta notación los parámetros iniciales de la red se pueden generar con la siguiente función, se puede observar como el ciclo está iterando por los elementos de <span class="in">`d`</span> creando pares, para generar las dimensiones adecuadas para las matrices $W.$</span>
<span id="cb24-417"><a href="#cb24-417" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-420"><a href="#cb24-420" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb24-421"><a href="#cb24-421" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: true</span></span>
<span id="cb24-422"><a href="#cb24-422" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> parametros_iniciales(d, key<span class="op">=</span><span class="dv">0</span>):</span>
<span id="cb24-423"><a href="#cb24-423" aria-hidden="true" tabindex="-1"></a>    key <span class="op">=</span> jax.random.PRNGKey(key)</span>
<span id="cb24-424"><a href="#cb24-424" aria-hidden="true" tabindex="-1"></a>    params <span class="op">=</span> []</span>
<span id="cb24-425"><a href="#cb24-425" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> init, end <span class="kw">in</span> <span class="bu">zip</span>(d, d[<span class="dv">1</span>:]):</span>
<span id="cb24-426"><a href="#cb24-426" aria-hidden="true" tabindex="-1"></a>        key, subkey1, subkey2 <span class="op">=</span> jax.random.split(key, num<span class="op">=</span><span class="dv">3</span>)</span>
<span id="cb24-427"><a href="#cb24-427" aria-hidden="true" tabindex="-1"></a>        _ <span class="op">=</span> <span class="bu">dict</span>(w<span class="op">=</span>jax.random.normal(subkey1, (init, end)) <span class="op">*</span> jnp.sqrt(<span class="dv">2</span> <span class="op">/</span> (init <span class="op">*</span> end)),</span>
<span id="cb24-428"><a href="#cb24-428" aria-hidden="true" tabindex="-1"></a>                 w0<span class="op">=</span>jax.random.normal(subkey2, (end, )) <span class="op">*</span> jnp.sqrt(<span class="dv">2</span> <span class="op">/</span> end))</span>
<span id="cb24-429"><a href="#cb24-429" aria-hidden="true" tabindex="-1"></a>        params.append(_)</span>
<span id="cb24-430"><a href="#cb24-430" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> params</span>
<span id="cb24-431"><a href="#cb24-431" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb24-432"><a href="#cb24-432" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-433"><a href="#cb24-433" aria-hidden="true" tabindex="-1"></a>Habiendo generado los parámetros iniciales de la red, es momento para implmentar la red, la siguiente función implementa la red, se puede observar como es realizan las operaciones matriciales tal y como se mostraron en las ecuaciones anteriores. La función de activiación $\phi$ seleccionada fue $\textsf{ReLU}$ que está implementada en la función <span class="in">`jax.nn.relu`</span>.</span>
<span id="cb24-434"><a href="#cb24-434" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-437"><a href="#cb24-437" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb24-438"><a href="#cb24-438" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: true</span></span>
<span id="cb24-439"><a href="#cb24-439" aria-hidden="true" tabindex="-1"></a><span class="at">@jax.jit</span></span>
<span id="cb24-440"><a href="#cb24-440" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> ann(params, D):</span>
<span id="cb24-441"><a href="#cb24-441" aria-hidden="true" tabindex="-1"></a>    y1 <span class="op">=</span> jax.nn.relu(D <span class="op">@</span> params[<span class="dv">0</span>][<span class="st">'w'</span>] <span class="op">+</span> params[<span class="dv">0</span>][<span class="st">'w0'</span>])</span>
<span id="cb24-442"><a href="#cb24-442" aria-hidden="true" tabindex="-1"></a>    y2 <span class="op">=</span> jax.nn.relu(y1 <span class="op">@</span> params[<span class="dv">1</span>][<span class="st">'w'</span>] <span class="op">+</span> params[<span class="dv">1</span>][<span class="st">'w0'</span>])</span>
<span id="cb24-443"><a href="#cb24-443" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> y2 <span class="op">@</span> params[<span class="dv">2</span>][<span class="st">'w'</span>] <span class="op">+</span> params[<span class="dv">2</span>][<span class="st">'w0'</span>]</span>
<span id="cb24-444"><a href="#cb24-444" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb24-445"><a href="#cb24-445" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-446"><a href="#cb24-446" aria-hidden="true" tabindex="-1"></a>Como medida de error se usa la Entropía Cruzada (@sec-entropia-cruzada) tal y como se implementa a continuación. El caso $0 \log 0$ corresponde a un valor no definido lo cual genera que el valor de la función tampoco este definido, para proteger la función en ese caso se usa <span class="in">`jnp.nansum`</span> que trata los valores no definidos como ceros. </span>
<span id="cb24-447"><a href="#cb24-447" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-450"><a href="#cb24-450" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb24-451"><a href="#cb24-451" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: true</span></span>
<span id="cb24-452"><a href="#cb24-452" aria-hidden="true" tabindex="-1"></a><span class="at">@jax.jit</span></span>
<span id="cb24-453"><a href="#cb24-453" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> media_entropia_cruzada(params, D, y_oh):</span>
<span id="cb24-454"><a href="#cb24-454" aria-hidden="true" tabindex="-1"></a>    hy <span class="op">=</span> jax.nn.softmax(ann(params, D), axis<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb24-455"><a href="#cb24-455" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="op">-</span> jnp.nansum(y_oh <span class="op">*</span> jnp.log(hy), axis<span class="op">=</span><span class="dv">1</span>).mean()</span>
<span id="cb24-456"><a href="#cb24-456" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb24-457"><a href="#cb24-457" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-458"><a href="#cb24-458" aria-hidden="true" tabindex="-1"></a>En esta ocasión se utiliza el optimizador Adam, tal y como se muestra en la siguiente función. La única diferencia con respecto al visto previamente es la función <span class="in">`update_finite`</span> que actualiza los parámetros siempre y cuando el nuevo valor sea un valor numérico, de los contrario se queda con el valor anterior. </span>
<span id="cb24-459"><a href="#cb24-459" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-462"><a href="#cb24-462" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb24-463"><a href="#cb24-463" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: true</span></span>
<span id="cb24-464"><a href="#cb24-464" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> adam(optimizer, params, epocas<span class="op">=</span><span class="dv">500</span>):</span>
<span id="cb24-465"><a href="#cb24-465" aria-hidden="true" tabindex="-1"></a>    <span class="at">@jax.jit</span></span>
<span id="cb24-466"><a href="#cb24-466" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> update_finite(a, b):</span>
<span id="cb24-467"><a href="#cb24-467" aria-hidden="true" tabindex="-1"></a>        m <span class="op">=</span> jnp.isfinite(b)</span>
<span id="cb24-468"><a href="#cb24-468" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> jnp.where(m, b, a)</span>
<span id="cb24-469"><a href="#cb24-469" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb24-470"><a href="#cb24-470" aria-hidden="true" tabindex="-1"></a>    <span class="at">@jax.jit</span></span>
<span id="cb24-471"><a href="#cb24-471" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> update(params, opt_state, X, y_oh):</span>
<span id="cb24-472"><a href="#cb24-472" aria-hidden="true" tabindex="-1"></a>        loss_value, grads <span class="op">=</span> error_grad(params, X, y_oh)</span>
<span id="cb24-473"><a href="#cb24-473" aria-hidden="true" tabindex="-1"></a>        updates, opt_state <span class="op">=</span> optimizer.update(grads, opt_state, params)</span>
<span id="cb24-474"><a href="#cb24-474" aria-hidden="true" tabindex="-1"></a>        params <span class="op">=</span> optax.apply_updates(params, updates)</span>
<span id="cb24-475"><a href="#cb24-475" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> params, opt_state, loss_value</span>
<span id="cb24-476"><a href="#cb24-476" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb24-477"><a href="#cb24-477" aria-hidden="true" tabindex="-1"></a>    opt_state <span class="op">=</span> optimizer.init(params)</span>
<span id="cb24-478"><a href="#cb24-478" aria-hidden="true" tabindex="-1"></a>    error_grad  <span class="op">=</span> jax.value_and_grad(media_entropia_cruzada)    </span>
<span id="cb24-479"><a href="#cb24-479" aria-hidden="true" tabindex="-1"></a>    error <span class="op">=</span> []</span>
<span id="cb24-480"><a href="#cb24-480" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(epocas):</span>
<span id="cb24-481"><a href="#cb24-481" aria-hidden="true" tabindex="-1"></a>        p, opt_state, loss_value <span class="op">=</span> update(params, opt_state, T, yt_oh)</span>
<span id="cb24-482"><a href="#cb24-482" aria-hidden="true" tabindex="-1"></a>        params <span class="op">=</span> jax.tree_map(update_finite, params, p)</span>
<span id="cb24-483"><a href="#cb24-483" aria-hidden="true" tabindex="-1"></a>        error.append(loss_value)</span>
<span id="cb24-484"><a href="#cb24-484" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> params, error</span>
<span id="cb24-485"><a href="#cb24-485" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb24-486"><a href="#cb24-486" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-487"><a href="#cb24-487" aria-hidden="true" tabindex="-1"></a>Finalmente, la red creada se entrana utilizando las siguientes instrucciones, donde la primera linea genera los parámetros iniciales, después se inicializa el optimizador y en la línea final se llama al optimizador con los parámetros y número de épocas.</span>
<span id="cb24-488"><a href="#cb24-488" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-491"><a href="#cb24-491" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb24-492"><a href="#cb24-492" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: true</span></span>
<span id="cb24-493"><a href="#cb24-493" aria-hidden="true" tabindex="-1"></a>params <span class="op">=</span> parametros_iniciales(d)</span>
<span id="cb24-494"><a href="#cb24-494" aria-hidden="true" tabindex="-1"></a>optimizer <span class="op">=</span> optax.adam(learning_rate<span class="op">=</span><span class="fl">1e-2</span>)</span>
<span id="cb24-495"><a href="#cb24-495" aria-hidden="true" tabindex="-1"></a>p, error <span class="op">=</span> adam(optimizer, params, epocas<span class="op">=</span><span class="dv">500</span>)</span>
<span id="cb24-496"><a href="#cb24-496" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb24-497"><a href="#cb24-497" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-500"><a href="#cb24-500" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb24-501"><a href="#cb24-501" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: false</span></span>
<span id="cb24-502"><a href="#cb24-502" aria-hidden="true" tabindex="-1"></a>error_f <span class="op">=</span> (ann(p, G).argmax(axis<span class="op">=</span><span class="dv">1</span>) <span class="op">!=</span> y_g).mean()</span>
<span id="cb24-503"><a href="#cb24-503" aria-hidden="true" tabindex="-1"></a>error_f <span class="op">=</span> Markdown(<span class="ss">f'$</span><span class="sc">{</span>error_f<span class="sc">:0.4f}</span><span class="ss">$'</span>)</span>
<span id="cb24-504"><a href="#cb24-504" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb24-505"><a href="#cb24-505" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-506"><a href="#cb24-506" aria-hidden="true" tabindex="-1"></a>El resultado de esta red, es que el error en el conjunto de prueba es <span class="in">`{python} error_f`</span> calculado con la siguiente instrucción <span class="in">`(ann(p, G).argmax(axis=1) != y_g).mean()`</span>. </span>
</code><button title="Copiar al portapapeles" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div></div></div></div></div>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
      &nbsp;
    </div>   
    <div class="nav-footer-center">
      &nbsp;
    </div>
    <div class="nav-footer-right">
<p><a href="http://creativecommons.org/licenses/by-sa/4.0/"><img src="https://i.creativecommons.org/l/by-sa/4.0/80x15.png" class="img-fluid"></a> <br> Esta obra está bajo una <a href="http://creativecommons.org/licenses/by-sa/4.0/">Licencia Creative Commons Atribución-CompartirIgual 4.0 Internacional</a></p>
</div>
  </div>
</footer>




</body></html>