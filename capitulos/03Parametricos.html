<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="es" xml:lang="es"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.5.40">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Aprendizaje Computacional – 3&nbsp; Métodos Paramétricos</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<link href="../capitulos/04Rendimiento.html" rel="next">
<link href="../capitulos/02Teoria_Decision.html" rel="prev">
<script src="../site_libs/quarto-html/quarto.js"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "Sin resultados",
    "search-matching-documents-text": "documentos encontrados",
    "search-copy-link-title": "Copiar el enlace en la búsqueda",
    "search-hide-matches-text": "Ocultar resultados adicionales",
    "search-more-match-text": "resultado adicional en este documento",
    "search-more-matches-text": "resultados adicionales en este documento",
    "search-clear-button-title": "Borrar",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancelar",
    "search-submit-button-title": "Enviar",
    "search-label": "Buscar"
  }
}</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="nav-sidebar floating">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Alternar barra lateral" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../capitulos/03Parametricos.html"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Métodos Paramétricos</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Alternar barra lateral" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Buscar" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="../">Aprendizaje Computacional</a> 
        <div class="sidebar-tools-main">
    <a href="https://github.com/INGEOTEC/AprendizajeComputacional" title="Ejecutar el código" class="quarto-navigation-tool px-1" aria-label="Ejecutar el código"><i class="bi bi-github"></i></a>
    <a href="../Aprendizaje-Computacional.pdf" title="Descargar PDF" class="quarto-navigation-tool px-1" aria-label="Descargar PDF"><i class="bi bi-file-pdf"></i></a>
</div>
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Buscar"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Prefacio</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../capitulos/01Introduccion.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Introducción</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../capitulos/02Teoria_Decision.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Teoría de Decisión Bayesiana</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../capitulos/03Parametricos.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Métodos Paramétricos</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../capitulos/04Rendimiento.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Rendimiento</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../capitulos/05ReduccionDim.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Reducción de Dimensión</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../capitulos/06Agrupamiento.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Agrupamiento</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../capitulos/07NoParametricos.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Métodos No Paramétricos</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../capitulos/08Arboles.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Árboles de Decisión</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../capitulos/09Lineal.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Discriminantes Lineales</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../capitulos/10Optimizacion.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Optimización</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../capitulos/11RedesNeuronales.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Redes Neuronales</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../capitulos/12Ensambles.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">Ensambles</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../capitulos/13Comparacion.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">Comparación de Algoritmos</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../capitulos/17Referencias.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Referencias</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true">
 <span class="menu-text">Apéndices</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true" aria-label="Alternar sección">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../capitulos/14Estadistica.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">A</span>&nbsp; <span class="chapter-title">Estadística</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../capitulos/15Codigo.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">B</span>&nbsp; <span class="chapter-title">Código</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../capitulos/16ConjuntosDatos.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">C</span>&nbsp; <span class="chapter-title">Conjunto de Datos</span></span></a>
  </div>
</li>
      </ul>
  </li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Tabla de contenidos</h2>
   
  <ul>
  <li><a href="#paquetes-usados" id="toc-paquetes-usados" class="nav-link active" data-scroll-target="#paquetes-usados">Paquetes usados</a></li>
  <li><a href="#introducción" id="toc-introducción" class="nav-link" data-scroll-target="#introducción"><span class="header-section-number">3.1</span> Introducción</a></li>
  <li><a href="#sec-metodologia-met-parametricos" id="toc-sec-metodologia-met-parametricos" class="nav-link" data-scroll-target="#sec-metodologia-met-parametricos"><span class="header-section-number">3.2</span> Metodología</a></li>
  <li><a href="#sec-estimacion-parametros-gnal" id="toc-sec-estimacion-parametros-gnal" class="nav-link" data-scroll-target="#sec-estimacion-parametros-gnal"><span class="header-section-number">3.3</span> Estimación de Parámetros</a>
  <ul class="collapse">
  <li><a href="#sec-verosimilitud" id="toc-sec-verosimilitud" class="nav-link" data-scroll-target="#sec-verosimilitud"><span class="header-section-number">3.3.1</span> Verosimilitud</a></li>
  <li><a href="#sec-distribucción-de-bernoulli" id="toc-sec-distribucción-de-bernoulli" class="nav-link" data-scroll-target="#sec-distribucción-de-bernoulli"><span class="header-section-number">3.3.2</span> Distribución de Bernoulli</a></li>
  <li><a href="#sec-estimacion-distribucion-gausiana" id="toc-sec-estimacion-distribucion-gausiana" class="nav-link" data-scroll-target="#sec-estimacion-distribucion-gausiana"><span class="header-section-number">3.3.3</span> Ejemplo: Distribución Gausiana</a></li>
  </ul></li>
  <li><a href="#metodología-de-clasificación" id="toc-metodología-de-clasificación" class="nav-link" data-scroll-target="#metodología-de-clasificación"><span class="header-section-number">3.4</span> Metodología de Clasificación</a></li>
  <li><a href="#sec-conjunto-entre-prueba" id="toc-sec-conjunto-entre-prueba" class="nav-link" data-scroll-target="#sec-conjunto-entre-prueba"><span class="header-section-number">3.5</span> Conjunto de Entrenamiento y Prueba</a>
  <ul class="collapse">
  <li><a href="#sec-model-clasificacion" id="toc-sec-model-clasificacion" class="nav-link" data-scroll-target="#sec-model-clasificacion"><span class="header-section-number">3.5.1</span> Modelo</a></li>
  <li><a href="#sec-estimacion-parametros" id="toc-sec-estimacion-parametros" class="nav-link" data-scroll-target="#sec-estimacion-parametros"><span class="header-section-number">3.5.2</span> Estimación de Parámetros</a></li>
  <li><a href="#sec-prediccion" id="toc-sec-prediccion" class="nav-link" data-scroll-target="#sec-prediccion"><span class="header-section-number">3.5.3</span> Predicción</a></li>
  <li><a href="#sec-rendimiento" id="toc-sec-rendimiento" class="nav-link" data-scroll-target="#sec-rendimiento"><span class="header-section-number">3.5.4</span> Rendimiento</a></li>
  </ul></li>
  <li><a href="#clasificador-bayesiano-ingenuo" id="toc-clasificador-bayesiano-ingenuo" class="nav-link" data-scroll-target="#clasificador-bayesiano-ingenuo"><span class="header-section-number">3.6</span> Clasificador Bayesiano Ingenuo</a></li>
  <li><a href="#sec-ejemplo-breast-cancer-wisconsin" id="toc-sec-ejemplo-breast-cancer-wisconsin" class="nav-link" data-scroll-target="#sec-ejemplo-breast-cancer-wisconsin"><span class="header-section-number">3.7</span> Ejemplo: Breast Cancer Wisconsin</a>
  <ul class="collapse">
  <li><a href="#entrenamiento" id="toc-entrenamiento" class="nav-link" data-scroll-target="#entrenamiento"><span class="header-section-number">3.7.1</span> Entrenamiento</a></li>
  <li><a href="#predicción" id="toc-predicción" class="nav-link" data-scroll-target="#predicción"><span class="header-section-number">3.7.2</span> Predicción</a></li>
  <li><a href="#sec-gaussina-perf-breast_cancer" id="toc-sec-gaussina-perf-breast_cancer" class="nav-link" data-scroll-target="#sec-gaussina-perf-breast_cancer"><span class="header-section-number">3.7.3</span> Rendimiento</a></li>
  </ul></li>
  <li><a href="#diferencias-en-rendimiento" id="toc-diferencias-en-rendimiento" class="nav-link" data-scroll-target="#diferencias-en-rendimiento"><span class="header-section-number">3.8</span> Diferencias en Rendimiento</a></li>
  <li><a href="#sec-regresion-ols" id="toc-sec-regresion-ols" class="nav-link" data-scroll-target="#sec-regresion-ols"><span class="header-section-number">3.9</span> Regresión</a>
  <ul class="collapse">
  <li><a href="#sec-diabetes" id="toc-sec-diabetes" class="nav-link" data-scroll-target="#sec-diabetes"><span class="header-section-number">3.9.1</span> Ejemplo: Diabetes</a></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<div class="quarto-title-block"><div><h1 class="title"><span id="sec-metodos-parametricos" class="quarto-section-identifier"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Métodos Paramétricos</span></span></h1><button type="button" class="btn code-tools-button dropdown-toggle" id="quarto-code-tools-menu" data-bs-toggle="dropdown" aria-expanded="false"><i class="bi"></i> Código</button><ul class="dropdown-menu dropdown-menu-end" aria-labelelledby="quarto-code-tools-menu"><li><a id="quarto-show-all-code" class="dropdown-item" href="javascript:void(0)" role="button">Mostrar todo el código</a></li><li><a id="quarto-hide-all-code" class="dropdown-item" href="javascript:void(0)" role="button">Ocultar todo el código</a></li><li><hr class="dropdown-divider"></li><li><a id="quarto-view-source" class="dropdown-item" href="javascript:void(0)" role="button">Ver el código fuente</a></li></ul></div></div>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<p>El <strong>objetivo</strong> de la unidad es conocer las características de los modelos paramétricos y aplicar máxima verosimilitud para estimar los parámetros del modelo paramétrico en problemas de regresión y clasificación.</p>
<section id="paquetes-usados" class="level2 unnumbered">
<h2 class="unnumbered anchored" data-anchor-id="paquetes-usados">Paquetes usados</h2>
<div id="16465b5d" class="cell" data-execution_count="1">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> EvoMSA.model <span class="im">import</span> GaussianBayes</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.linear_model <span class="im">import</span> LinearRegression</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> train_test_split</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.datasets <span class="im">import</span> load_breast_cancer,<span class="op">\</span></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a>                             load_diabetes</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> scipy.stats <span class="im">import</span> norm, multivariate_normal</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> scipy.special <span class="im">import</span> logsumexp</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> matplotlib <span class="im">import</span> pylab <span class="im">as</span> plt</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> seaborn <span class="im">as</span> sns</span></code><button title="Copiar al portapapeles" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<hr>
<div class="quarto-video"><iframe data-external="1" src="https://www.youtube.com/embed/Tlwecs3dUPw" width="560" height="315" title="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div>
<hr>
</section>
<section id="introducción" class="level2" data-number="3.1">
<h2 data-number="3.1" class="anchored" data-anchor-id="introducción"><span class="header-section-number">3.1</span> Introducción</h2>
<p>Existen diferentes tipos de algoritmos que se puede utilizar para resolver problemas de aprendizaje supervisado y no supervisado. En particular, esta unidad se enfoca en presentar las técnicas que se pueden caracterizar como métodos paramétricos.</p>
<p>Los métodos paramétricos se identifican por asumir que los datos provienen de una distribución de la cual se desconocen los parámetros y el procedimiento es encontrar los parámetros de la distribución que mejor modelen los datos. Una vez obtenidos los parámetros se cuenta con todos los elementos para utilizar el modelo y predecir la característica para la cual fue entrenada.</p>
</section>
<section id="sec-metodologia-met-parametricos" class="level2" data-number="3.2">
<h2 data-number="3.2" class="anchored" data-anchor-id="sec-metodologia-met-parametricos"><span class="header-section-number">3.2</span> Metodología</h2>
<p>Hasta el momento se han presentado ejemplos de los pasos 4 y 5 de la metodología general (ver <a href="01Introduccion.html#sec-metodologia-general" class="quarto-xref"><span>Sección 1.2</span></a>); esto fue en la <a href="02Teoria_Decision.html#sec-prediccion-normal" class="quarto-xref"><span>Sección 2.3.2</span></a> y en la <a href="02Teoria_Decision.html#sec-error-clasificacion" class="quarto-xref"><span>Sección 2.4</span></a>. Esta sección complementa los ejemplos anteriores al utilizar todos pasos de la metodología general de aprendizaje supervisado (ver <a href="01Introduccion.html#sec-metodologia-general" class="quarto-xref"><span>Sección 1.2</span></a>). En particular se enfoca al paso 3 que corresponde al diseño del algoritmo <span class="math inline">\(f\)</span> que modela el fenómeno de interés utilizando los datos <span class="math inline">\(\mathcal T \subset \mathcal D.\)</span></p>
<p>El algoritmo <span class="math inline">\(f\)</span> corresponde a asumir que los datos <span class="math inline">\(\mathcal D\)</span> provienen de una distribución <span class="math inline">\(F\)</span> la cual tiene una serie de parámetros <span class="math inline">\(\theta\)</span> que son identificados con <span class="math inline">\(\mathcal T.\)</span></p>
</section>
<section id="sec-estimacion-parametros-gnal" class="level2" data-number="3.3">
<h2 data-number="3.3" class="anchored" data-anchor-id="sec-estimacion-parametros-gnal"><span class="header-section-number">3.3</span> Estimación de Parámetros</h2>
<p>Se inicia la descripción de métodos paramétricos presentando el procedimiento general para estimar los parámetros de una distribución. Se cuenta con un conjunto <span class="math inline">\(\mathcal D\)</span> donde los elementos <span class="math inline">\(x \in \mathcal D\)</span> son <span class="math inline">\(x \in \mathbb R^d\)</span>. Los elementos <span class="math inline">\(x \in \mathcal D\)</span> tienen un distribución <span class="math inline">\(F\)</span>, i.e., <span class="math inline">\(x \sim F\)</span>, son independientes y <span class="math inline">\(F\)</span> está definida por la función de densidad de probabilidad <span class="math inline">\(f_{\theta}\)</span>, que a su vez está definida por <span class="math inline">\(\theta\)</span> parámetros. Utilizando <span class="math inline">\(\mathcal D\)</span> el objetivo es identificar los parámetros <span class="math inline">\(\theta\)</span> que hacen observar a <span class="math inline">\(\mathcal D\)</span> lo más probable.</p>
<section id="sec-verosimilitud" class="level3" data-number="3.3.1">
<h3 data-number="3.3.1" class="anchored" data-anchor-id="sec-verosimilitud"><span class="header-section-number">3.3.1</span> Verosimilitud</h3>
<p>Una solución para maximizar el observar <span class="math inline">\(\mathcal D\)</span> es maximizando la verosimilitud. La verosimilitud es la función distribución conjunta de los elementos en <span class="math inline">\(\mathcal D\)</span>, i.e., <span class="math inline">\(f_\theta(x_1, x_2, \ldots, x_N).\)</span> Considerando que la muestras son independientes entonces <span class="math inline">\(f_\theta(x_1, x_2, \ldots, x_N) = \prod_{x \in \mathcal D} f_\theta (x).\)</span> La función de verosimilitud considera la ecuación anterior como una función de los parámetros <span class="math inline">\(\theta,\)</span> es decir,</p>
<p><span class="math display">\[
\mathcal L(\theta) = \prod_{x \in \mathcal D} f_\theta (x),
\]</span></p>
<p>siendo el logaritmo de la verosimilitud</p>
<p><span class="math display">\[
\ell(\theta) = \log \mathcal L(\theta) = \sum_{x \in \mathcal D} \log f_\theta (x).
\]</span></p>
</section>
<section id="sec-distribucción-de-bernoulli" class="level3" data-number="3.3.2">
<h3 data-number="3.3.2" class="anchored" data-anchor-id="sec-distribucción-de-bernoulli"><span class="header-section-number">3.3.2</span> Distribución de Bernoulli</h3>
<p>La verosimilitud se ejemplifica con la identificación del parámetro <span class="math inline">\(p\)</span> de una distribución Bernoulli. Una distribución Bernoulli modela dos estados, por un lado se tiene la clase negativa identificada por <span class="math inline">\(0\)</span>; identificando la clase positiva como <span class="math inline">\(1\)</span>. Entonces, la probabilidad de observar <span class="math inline">\(1\)</span> es <span class="math inline">\(\mathbb P(X=1) = p\)</span> y <span class="math inline">\(\mathbb P(X=0) = 1 - p\)</span>. Estas ecuaciones se pueden combinar para definir <span class="math inline">\(f_\theta(x) = p^x (1 - p)^{1-x}.\)</span></p>
<p>Utilizando el logaritmo de la verosimilitud se tiene:</p>
<p><span class="math display">\[
\ell(p) = \sum_{i=1}^N \log p^{x_i} (1 - p)^{1-x_i} = \sum_{i=1}^N x_i \log p + (1-x_i) \log (1 - p).
\]</span></p>
<p>Recordando que el máximo de <span class="math inline">\(\ell(\mathcal p)\)</span> se obtiene cuando <span class="math inline">\(\frac{d}{dp} \ell(\mathcal p) = 0\)</span>, entonces estimar <span class="math inline">\(p\)</span> corresponde a resolver lo siguiente:</p>
<p><span class="math display">\[
\begin{split}
\frac{d}{dp} \ell(\mathcal p) &amp;= 0 \\
\frac{d}{dp} [ \sum_{i=1}^N x_i \log p + (1-x_i) \log (1 - p)] &amp;= 0 \\
\frac{d}{d p} [ \sum_{i=1}^N x_i \log p + \log (1 - p) (N - \sum_{i=1}^N x_i) ] &amp;= 0\\
\sum_{i=1}^N x_i \frac{d}{d p} \log \mathcal p + (N - \sum_{i=1}^N x_i) \frac{d}{d p} \log (1 - \mathcal p) &amp;= 0\\
\sum_{i=1}^N x_i \frac{1}{p} + (N - \sum_{i=1}^N x_i) \frac{-1}{(1 - p)} &amp;= 0\\
\end{split}
\]</span></p>
<p>Realizando algunas operaciones algebraicas se obtiene:</p>
<p><span class="math display">\[
\hat p = \frac{1}{N}\sum_{i=1}^N x_i.
\]</span></p>
</section>
<section id="sec-estimacion-distribucion-gausiana" class="level3" data-number="3.3.3">
<h3 data-number="3.3.3" class="anchored" data-anchor-id="sec-estimacion-distribucion-gausiana"><span class="header-section-number">3.3.3</span> Ejemplo: Distribución Gausiana</h3>
<p>Esta sección sigue un camino práctico presentando el código para estimar los parámetros de una distribución Gausiana donde se conocen todos los parámetros. La distribución se usa para generar 1000 muestras y después de esta población se estiman los parámetros; de estas manera se tienen todos los elementos para comparar los parámetros reales <span class="math inline">\(\theta\)</span> de los parámetros estimados <span class="math inline">\(\hat \theta.\)</span></p>
<p>La distribución que se usará se utilizó para generar un problema sintético (ver <a href="02Teoria_Decision.html#sec-tres-normales" class="quarto-xref"><span>Sección 2.3.1</span></a>) de tres clases. Los parámetros de la distribución son: <span class="math inline">\(\mathbf \mu = [5, 5]^\intercal\)</span> y <span class="math inline">\(\Sigma = \begin{pmatrix} 4 &amp; 0 \\ 0 &amp; 2 \\ \end{pmatrix}.\)</span> La siguiente instrucción se puede utilizar para generar 1000 muestras de esa distribución.</p>
<div id="926affcf" class="cell" data-execution_count="3">
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a>D <span class="op">=</span> multivariate_normal(mean<span class="op">=</span>[<span class="dv">5</span>, <span class="dv">5</span>], </span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a>                        cov<span class="op">=</span>[[<span class="dv">4</span>, <span class="dv">0</span>], </span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a>                             [<span class="dv">0</span>, <span class="dv">2</span>]]).rvs(size<span class="op">=</span><span class="dv">1000</span>)</span></code><button title="Copiar al portapapeles" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>La media estimada de los datos en <code>D</code> se calcula usando la función <code>np.mean</code> de la siguiente manera</p>
<div id="83c1ecf7" class="cell" data-execution_count="4">
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a>mu <span class="op">=</span> np.mean(D, axis<span class="op">=</span><span class="dv">0</span>)</span></code><button title="Copiar al portapapeles" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="b42b6e36" class="cell" data-execution_count="5">
<div class="cell-output cell-output-display" data-execution_count="5">
<p>donde el eje de operación es el primero que corresponde al índice <span class="math inline">\(0.\)</span> La media estimada es: <span class="math inline">\(\hat \mu = [5.0341, 5.0985]^\intercal\)</span> con un <a href="ver @sec-error-estandar-media">error estándar</a> (<code>se</code>) de <span class="math inline">\([0.0657, 0.0449]^\intercal\)</span> que se calcula con el siguiente código.</p>
</div>
</div>
<div id="7e7a1063" class="cell" data-execution_count="6">
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a>se <span class="op">=</span> np.std(D, axis<span class="op">=</span><span class="dv">0</span>) <span class="op">/</span> np.sqrt(<span class="dv">1000</span>)</span></code><button title="Copiar al portapapeles" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Hasta el momento se ha estimado <span class="math inline">\(\mu\)</span>, falta por estimar <span class="math inline">\(\Sigma\)</span>, que se puede realizar con la siguiente instrucción</p>
<div id="f13b789f" class="cell" data-execution_count="7">
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a>cov <span class="op">=</span> np.cov(D, rowvar<span class="op">=</span><span class="va">False</span>)</span></code><button title="Copiar al portapapeles" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="4c54f50d" class="cell" data-execution_count="8">
<div class="cell-output cell-output-display" data-execution_count="8">
<p>donde el parámetro <code>rowvar</code> indica la forma en que están proporcionados los datos. La estimación da los siguientes valores <span class="math inline">\(\hat \Sigma = \begin{pmatrix} 4.3158&amp;0.2391 \\ 0.2391&amp;2.0178 \\ \end{pmatrix};\)</span> se puede observar que son similares al parámetro con que se simularon los datos.</p>
</div>
</div>
<p>Siguiendo con la inercia de presentar el error estándar de cada estimación, en las siguientes instrucciones se presenta el error estándar de <span class="math inline">\(\hat \Sigma\)</span>, el cual se calcula utilizando la técnica de bootstrap (ver <a href="14Estadistica.html#sec-bootstrap" class="quarto-xref"><span>Sección A.2</span></a>) implementada en el siguiente código.</p>
<div id="4e482c3d" class="cell" data-execution_count="9">
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a>S <span class="op">=</span> np.random.randint(D.shape[<span class="dv">0</span>],</span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a>                      size<span class="op">=</span>(<span class="dv">500</span>, D.shape[<span class="dv">0</span>]))</span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a>B <span class="op">=</span> [np.cov(D[s], rowvar<span class="op">=</span><span class="va">False</span>) <span class="cf">for</span> s <span class="kw">in</span> S]</span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a>se <span class="op">=</span> np.std(B, axis<span class="op">=</span><span class="dv">0</span>)</span></code><button title="Copiar al portapapeles" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="06cb3308" class="cell" data-execution_count="10">
<div class="cell-output cell-output-display" data-execution_count="10">
<p>Se puede observar que la función <code>np.cov</code> se ejecuta utilizando la muestra indicada en la variable <code>s</code>. El error estándar (<code>se</code>) de <span class="math inline">\(\hat \Sigma\)</span> corresponde a <span class="math inline">\(\begin{pmatrix} 0.1831&amp;0.0911 \\ 0.0911&amp;0.0912 \\ \end{pmatrix}.\)</span> Se observa que los elementos fuera de la diagonal tienen un error estándar tal que el cero se encuentra en el intervalo <span class="math inline">\(\hat \Sigma \pm se;\)</span> lo cual indica que el cero es un valor factible. Lo anterior se puede verificar tomando en cuenta que se conoce <span class="math inline">\(\Sigma\)</span> y que el parámetro real es <span class="math inline">\(0\)</span> para aquellos elementos fuera de la diagonal.</p>
</div>
</div>
</section>
</section>
<section id="metodología-de-clasificación" class="level2" data-number="3.4">
<h2 data-number="3.4" class="anchored" data-anchor-id="metodología-de-clasificación"><span class="header-section-number">3.4</span> Metodología de Clasificación</h2>
<p>Habiendo descrito el proceso para estimar los parámetros de una distribución, por un lado se presentó de manera teórica con la distribución Bernoulli (ver <a href="#sec-distribucción-de-bernoulli" class="quarto-xref"><span>Sección 3.3.2</span></a>) y de manera práctica con una distribución Gausiana (ver <a href="#sec-estimacion-distribucion-gausiana" class="quarto-xref"><span>Sección 3.3.3</span></a>), se está en la posición de usar todos estos elementos para presentar el proceso completo de clasificación. La metodología general de aprendizaje supervisado (ver <a href="01Introduccion.html#sec-metodologia-general" class="quarto-xref"><span>Sección 1.2</span></a>) está definida por cinco pasos, estos pasos se especializan para el problema de clasificación y regresión, utilizando modelos paramétricos, de la siguiente manera.</p>
<ol type="1">
<li>Todo empieza con un conjunto de datos <span class="math inline">\(\mathcal D\)</span> que tiene la información del fenómeno de interés.</li>
<li>Se selecciona el conjunto <span class="math inline">\(\mathcal T \subset \mathcal D,\)</span> el procedimiento se describe en la <a href="#sec-conjunto-entre-prueba" class="quarto-xref"><span>Sección 3.5</span></a>.</li>
<li>Se diseña un algoritmo, <span class="math inline">\(f\)</span>, el cual se basa en un modelo (ver <a href="#sec-model-clasificacion" class="quarto-xref"><span>Sección 3.5.1</span></a>) y la estimación de sus parámetros (ver <a href="#sec-estimacion-parametros" class="quarto-xref"><span>Sección 3.5.2</span></a>) utilizando <span class="math inline">\(\mathcal T.\)</span></li>
<li>En la <a href="#sec-prediccion" class="quarto-xref"><span>Sección 3.5.3</span></a> se describe el uso de <span class="math inline">\(f\)</span> para predecir.</li>
<li>La <a href="#sec-rendimiento" class="quarto-xref"><span>Sección 3.5.4</span></a> muestra el procedimiento para medir el rendimiento utilizando un conjunto de prueba (ver <a href="#sec-conjunto-entre-prueba" class="quarto-xref"><span>Sección 3.5</span></a>).</li>
</ol>
<p>La metodología de clasificación se ilustra utilizando el problema sintético (ver <a href="02Teoria_Decision.html#sec-tres-normales" class="quarto-xref"><span>Sección 2.3.1</span></a>) de tres clases que se presentó en el <a href="02Teoria_Decision.html" class="quarto-xref"><span>Capítulo 2</span></a>. Específicamente las entradas que definían a cada clase estaban en la variables <code>X_1</code>, <code>X_2</code> y <code>X_3</code>. Entonces las clases se pueden colocar en la variable <code>y</code> tal como se indica a continuación.</p>
<div id="3caaf4a5" class="cell" data-execution_count="12">
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> np.concatenate((X_1, X_2, X_3))</span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> np.array([<span class="dv">1</span>] <span class="op">*</span> <span class="dv">1000</span> <span class="op">+</span> [<span class="dv">2</span>] <span class="op">*</span> <span class="dv">1000</span> <span class="op">+</span> [<span class="dv">3</span>] <span class="op">*</span> <span class="dv">1000</span>)</span></code><button title="Copiar al portapapeles" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Las variables <code>X</code> y <code>y</code> contiene la información del conjunto <span class="math inline">\(\mathcal D = (\mathcal X, \mathcal Y)\)</span> donde cada renglón de <code>X</code> es una realización de la variable aleatoria <span class="math inline">\(\mathcal X\)</span> y equivalentemente cada elemento en <code>y</code> es una realización de <span class="math inline">\(\mathcal Y.\)</span></p>
</section>
<section id="sec-conjunto-entre-prueba" class="level2" data-number="3.5">
<h2 data-number="3.5" class="anchored" data-anchor-id="sec-conjunto-entre-prueba"><span class="header-section-number">3.5</span> Conjunto de Entrenamiento y Prueba</h2>
<p>En la <a href="#sec-estimacion-distribucion-gausiana" class="quarto-xref"><span>Sección 3.3.3</span></a> se había utilizado a <span class="math inline">\(\mathcal D\)</span> en el procedimiento de maximizar la verosimilitud, esto porque el objetivo en ese procedimiento era estimar los parámetros de la distribución. Pero el objetivo en aprendizaje supervisado es diseñar un algoritmo (función en este caso) que modele la relación entre <span class="math inline">\(\mathcal X\)</span> y <span class="math inline">\(\mathcal Y\)</span>. Para conocer esto es necesario medir el rendimiento del algoritmo en instancias que no han sido vistas en el entrenamiento.</p>
<p>En consecuencia, se requieren contar con datos para medir el rendimiento, a este conjunto de datos se le conoce como el conjunto de prueba, <span class="math inline">\(\mathcal G\)</span>. <span class="math inline">\(\mathcal G\)</span> se crea a partir de <span class="math inline">\(\mathcal D\)</span> de tal manera que <span class="math inline">\(\mathcal G \cap \mathcal T = \emptyset\)</span> y <span class="math inline">\(\mathcal D =  \mathcal G \cup \mathcal T.\)</span> La siguiente instrucción se puede utilizar para dividir la generación de estos conjuntos a partir de <span class="math inline">\(\mathcal D.\)</span></p>
<div id="f6804593" class="cell" data-execution_count="13">
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a>T, G, y_t, y_g <span class="op">=</span> train_test_split(X, y, test_size<span class="op">=</span><span class="fl">0.2</span>)</span></code><button title="Copiar al portapapeles" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>El parámetro <code>test_size</code> indica la proporción del tamaño de conjunto <span class="math inline">\(\mathcal G\)</span> en relación con el conjunto <span class="math inline">\(\mathcal D.\)</span></p>
<section id="sec-model-clasificacion" class="level3" data-number="3.5.1">
<h3 data-number="3.5.1" class="anchored" data-anchor-id="sec-model-clasificacion"><span class="header-section-number">3.5.1</span> Modelo</h3>
<p>El inicio de métodos paramétricos es el Teorema de Bayes (<a href="02Teoria_Decision.html#eq-teorema-bayes" class="quarto-xref">Ecuación&nbsp;<span>2.1</span></a>) <span class="math inline">\(\mathbb P(\mathcal Y \mid \mathcal X) = \frac{ \mathbb P(\mathcal X \mid \mathcal Y) \mathbb P(\mathcal Y)}{\mathbb P(\mathcal X)}\)</span> donde se usa la verosimilitud <span class="math inline">\(\mathbb P(\mathcal X \mid \mathcal Y)\)</span> y el prior <span class="math inline">\(\mathbb P(\mathcal Y)\)</span> para definir la probabilidad a posteriori <span class="math inline">\(\mathbb P(\mathcal Y \mid \mathcal X)\)</span>. En métodos paramétricos se asume que se puede modelar la verosimilitud con una distribución particular, que por lo generar es una distribución Gausiana multivariada. Es decir, la variable aleatoria <span class="math inline">\(\mathcal X\)</span> dado <span class="math inline">\(\mathcal Y\)</span> (<span class="math inline">\(\mathcal X_{\mid \mathcal Y}\)</span>) es <span class="math inline">\(\mathcal X_{\mid \mathcal Y} \sim \mathcal N(\mu_{\mathcal Y}, \Sigma_{\mathcal Y}),\)</span> donde se observa que los parámetros de la distribución Gausiana dependen de la variable aleatoria <span class="math inline">\(\mathcal Y\)</span> y estos pueden ser identificados cuando <span class="math inline">\(\mathcal Y\)</span> tiene un valor específico.</p>
</section>
<section id="sec-estimacion-parametros" class="level3" data-number="3.5.2">
<h3 data-number="3.5.2" class="anchored" data-anchor-id="sec-estimacion-parametros"><span class="header-section-number">3.5.2</span> Estimación de Parámetros</h3>
<p>Dado que por definición del problema (ver <a href="02Teoria_Decision.html#sec-tres-normales" class="quarto-xref"><span>Sección 2.3.1</span></a>) se conoce que la verosimilitud para cada clase proviene de una Gausiana, i.e., <span class="math inline">\(\mathcal X_{\mid \mathcal Y} \sim \mathcal N(\mu_{\mathcal Y}, \Sigma_{\mathcal Y}),\)</span> en esta sección se estimarán los parámetros utilizando este conocimiento.</p>
<p>El primer paso en la estimación de parámetros es calcular el prior <span class="math inline">\(\mathbb P(\mathcal Y)\)</span>, el cual corresponde a clasificar el evento sin observar el valor de <span class="math inline">\(\mathcal X.\)</span> Esto se puede modelar mediante una distribución Categórica con parámetros <span class="math inline">\(p_i\)</span> donde <span class="math inline">\(\sum_i^K p_i = 1\)</span>. Estos parámetros se pueden estimar utilizando la función <code>np.unique</code> de la siguiente manera</p>
<div id="3b579c54" class="cell" data-execution_count="14">
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a>labels, counts <span class="op">=</span> np.unique(y_t, return_counts<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a>prior <span class="op">=</span> counts <span class="op">/</span> counts.<span class="bu">sum</span>()</span></code><button title="Copiar al portapapeles" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>La variable <code>prior</code> contiene en el primer elemento <span class="math inline">\(\mathbb P(\mathcal Y=1) = 0.3292,\)</span> en el segundo <span class="math inline">\(\mathbb P(\mathcal Y=2) = 0.3412\)</span> y en el tercero <span class="math inline">\(\mathbb P(\mathcal Y=3) = 0.3296\)</span> que es aproximadamente <span class="math inline">\(\frac{1}{3}\)</span> el cual es el valor real del prior.</p>
<p>Siguiendo los pasos en estimación de parámetros de una Gausiana(<a href="#sec-estimacion-distribucion-gausiana" class="quarto-xref"><span>Sección 3.3.3</span></a>) se pueden estimar los parámetros para cada Gausiana dada la clase. Es decir, se tiene que estimar los parámetros <span class="math inline">\(\mu\)</span> y <span class="math inline">\(\Sigma\)</span> para la clase <span class="math inline">\(1\)</span>, <span class="math inline">\(2\)</span> y <span class="math inline">\(3\)</span>. Esto se puede realizar iterando por las etiquetas contenidas en la variable <code>labels</code> y seleccionando los datos en <code>T</code> que corresponden a la clase analizada, ver el uso de la variable <code>mask</code> en el slice de la línea 4 y 5. Después se inicializa una instancia de la clase <code>multivariate_normal</code> para ser utilizada en el cómputo de la función de densidad de probabilidad. El paso final es guardar las instancias de las distribuciones en la lista <code>likelihood</code>.</p>
<div id="622effb8" class="cell" data-execution_count="15">
<div class="sourceCode cell-code" id="cb10"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a>likelihood <span class="op">=</span> []</span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> k <span class="kw">in</span> labels:</span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a>    mask <span class="op">=</span> y_t <span class="op">==</span> k</span>
<span id="cb10-4"><a href="#cb10-4" aria-hidden="true" tabindex="-1"></a>    mu <span class="op">=</span> np.mean(T[mask], axis<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb10-5"><a href="#cb10-5" aria-hidden="true" tabindex="-1"></a>    cov <span class="op">=</span> np.cov(T[mask], rowvar<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb10-6"><a href="#cb10-6" aria-hidden="true" tabindex="-1"></a>    likelihood_k <span class="op">=</span> multivariate_normal(mean<span class="op">=</span>mu, cov<span class="op">=</span>cov)</span>
<span id="cb10-7"><a href="#cb10-7" aria-hidden="true" tabindex="-1"></a>    likelihood.append(likelihood_k)</span></code><button title="Copiar al portapapeles" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="8ad0c2ea" class="cell" data-execution_count="16">
<div class="cell-output cell-output-display" data-execution_count="16">
<p>Los valores estimados para la media, en cada clase son: <span class="math inline">\(\hat \mu_1 = [5.0133, 5.0293]^\intercal,\)</span> <span class="math inline">\(\hat \mu_2 = [1.4612, -1.4185]^\intercal\)</span> y <span class="math inline">\(\hat \mu_3 = [12.5317, -3.4426]^\intercal\)</span>. Para las matrices de covarianza, los valores estimados corresponden a <span class="math inline">\(\hat \Sigma_1 = \begin{pmatrix} 4.0789 &amp; -0.0132\\-0.0132 &amp; 1.8829 \\ \end{pmatrix},\)</span> <span class="math inline">\(\hat \Sigma_2 = \begin{pmatrix} 1.9812 &amp; 0.9541\\0.9541 &amp; 2.9672 \\ \end{pmatrix}\)</span> y <span class="math inline">\(\hat \Sigma_3 = \begin{pmatrix} 1.7917 &amp; 2.5068\\2.5068 &amp; 6.2007 \\ \end{pmatrix}.\)</span></p>
</div>
</div>
<p>Estas estimaciones se pueden comparar con los parámetros reales (<a href="02Teoria_Decision.html#sec-tres-normales" class="quarto-xref"><span>Sección 2.3.1</span></a>). También se puede calcular su error estándar para identificar si el parámetro real, <span class="math inline">\(\theta\)</span>, se encuentra en el intervalo definido por <span class="math inline">\(\hat \theta - 2\hat{se} \leq \hat \theta \leq \hat \theta + 2 \hat{se}\)</span> que corresponde aproximadamente al 95% de confianza asumiendo que la distribución de la estimación del parámetro es Gausiana.</p>
</section>
<section id="sec-prediccion" class="level3" data-number="3.5.3">
<h3 data-number="3.5.3" class="anchored" data-anchor-id="sec-prediccion"><span class="header-section-number">3.5.3</span> Predicción</h3>
<p>Una vez que se tiene la función que modela los datos, se está en condiciones de utilizarla para predecir(ver <a href="02Teoria_Decision.html#sec-prediccion-normal" class="quarto-xref"><span>Sección 2.3.2</span></a>) nuevos datos.</p>
<p>En esta ocasión se organiza el procedimiento de predicción en diferentes funciones, la primera función recibe los datos a predecir <code>X</code> y los componentes del modelo, que son la verosimilitud (<code>likelihood</code>) y el <code>prior</code>. La función calcula <span class="math inline">\(\mathbb P(\mathcal Y=y \mid \mathcal X=x)\)</span> que es la probabilidad de cada clase dada la entrada <span class="math inline">\(x\)</span>. Se puede observar en la primera línea que se usa la función de densidad de probabilidad (<code>pdf</code>) para cada clase y esta se multiplica por el <code>prior</code> y en la tercera línea se calcula la evidencia. Finalmente, se regresa el a posteriori.</p>
<div id="09206504" class="cell" data-execution_count="17">
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> predict_prob(X, likelihood, prior):</span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a>    likelihood <span class="op">=</span> [m.pdf(X) <span class="cf">for</span> m <span class="kw">in</span> likelihood]</span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a>    posterior <span class="op">=</span> np.vstack(likelihood).T <span class="op">*</span> prior</span>
<span id="cb11-4"><a href="#cb11-4" aria-hidden="true" tabindex="-1"></a>    evidence <span class="op">=</span> posterior.<span class="bu">sum</span>(axis<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb11-5"><a href="#cb11-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> posterior <span class="op">/</span> np.atleast_2d(evidence).T</span></code><button title="Copiar al portapapeles" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>La función <code>predict_proba</code> se utiliza como base para predecir la clase, para la cual se requiere el mapa entre índices y clases que se encuentra en la variable <code>labels</code>. Se observa que se llama a la función <code>predict_proba</code> y después se calcula el argumento que tiene la máxima probabilidad regresando la etiqueta asociada.</p>
<div id="dff3c25c" class="cell" data-execution_count="18">
<div class="sourceCode cell-code" id="cb12"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> predict(X, likelihood, prior, labels):</span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a>    _ <span class="op">=</span> predict_prob(X, likelihood, prior)</span>
<span id="cb12-3"><a href="#cb12-3" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> labels[np.argmax(_, axis<span class="op">=</span><span class="dv">1</span>)]</span></code><button title="Copiar al portapapeles" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="sec-rendimiento" class="level3" data-number="3.5.4">
<h3 data-number="3.5.4" class="anchored" data-anchor-id="sec-rendimiento"><span class="header-section-number">3.5.4</span> Rendimiento</h3>
<p>El rendimiento del algoritmo se mide en el conjunto de prueba <code>G</code>, utilizando como medida el error de clasificación (<a href="02Teoria_Decision.html#sec-error-clasificacion" class="quarto-xref"><span>Sección 2.4</span></a>). El primer paso es predecir las clases de los elementos en <code>G</code>, utilizando la función <code>predict</code> que fue diseñada anteriormente. Después se mide el error, con la instrucción de la segunda línea.</p>
<div id="408e8df4" class="cell" data-execution_count="19">
<div class="sourceCode cell-code" id="cb13"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a>hy <span class="op">=</span> predict(G, likelihood, prior, labels)</span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a>error <span class="op">=</span> (y_g <span class="op">!=</span> hy).mean()</span></code><button title="Copiar al portapapeles" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="14df2232" class="cell" data-execution_count="20">
<div class="cell-output cell-output-display" data-execution_count="20">
<p>El error que tiene el algoritmo en el conjunto de prueba es <span class="math inline">\(0.01,\)</span> el cual es ligeramente superior al encontrado con el modelo ideal.</p>
</div>
</div>
<div id="eb627457" class="cell" data-execution_count="21">
<div class="cell-output cell-output-display" data-execution_count="21">
<p>El error estándar se calcula con la siguiente instrucción el cual tiene un valor de <span class="math inline">\(0.0037.\)</span></p>
</div>
</div>
<div id="54e1f927" class="cell" data-execution_count="22">
<div class="sourceCode cell-code" id="cb14"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a>se_formula <span class="op">=</span> np.sqrt(error <span class="op">*</span> (<span class="dv">1</span> <span class="op">-</span> error) <span class="op">/</span> y_g.shape[<span class="dv">0</span>])</span></code><button title="Copiar al portapapeles" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
</section>
<section id="clasificador-bayesiano-ingenuo" class="level2" data-number="3.6">
<h2 data-number="3.6" class="anchored" data-anchor-id="clasificador-bayesiano-ingenuo"><span class="header-section-number">3.6</span> Clasificador Bayesiano Ingenuo</h2>
<p>Uno de los clasificadores mas utilizados, sencillo de implementar y competitivo, es el clasificador Bayesiano Ingenuo. En la <a href="#sec-model-clasificacion" class="quarto-xref"><span>Sección 3.5.1</span></a> se asumió que la variable aleatoria <span class="math inline">\(\mathcal X = (\mathcal X_1, \mathcal X_2, \ldots, \mathcal X_d)\)</span> dado <span class="math inline">\(\mathcal Y\)</span> (<span class="math inline">\(\mathcal X_{\mid \mathcal Y}\)</span>) es <span class="math inline">\(\mathcal X_{\mid \mathcal Y} \sim \mathcal N(\mu_{\mathcal Y}, \Sigma_{\mathcal Y}),\)</span> donde <span class="math inline">\(\mu_{\mathcal Y} \in \mathbb R^d\)</span>, <span class="math inline">\(\Sigma_{\mathcal Y} \in \mathbb R^{d \times d}\)</span> y <span class="math inline">\(f(\mathcal X_1, \mathcal X_2, \ldots, \mathcal X_d)\)</span> es la función de densidad de probabilidad conjunta.</p>
<p>En el clasificador Bayesiano Ingenuo se asume que las variables <span class="math inline">\(\mathcal X_i\)</span> y <span class="math inline">\(\mathcal X_j\)</span> para <span class="math inline">\(i \neq j\)</span> son independientes, esto trae como consecuencia que <span class="math inline">\(f(\mathcal X_1, \mathcal X_2, \ldots, \mathcal X_d) = \prod_i^d f(\mathcal X_i).\)</span> Esto quiere decir que cada variable está definida como una Gausina donde se tiene que identificar <span class="math inline">\(\mu\)</span> y <span class="math inline">\(\sigma^2.\)</span></p>
<p>La estimación de los parámetros de estas distribuciones se puede realizar utilizando un código similar siendo la única diferencia que en se calcula <span class="math inline">\(\sigma^2\)</span> de cada variable en lugar de la covarianza <span class="math inline">\(\Sigma\)</span>, esto se puede observar en la quinta línea donde se usa la función <code>np.var</code> en el primer eje. El resto del código es equivalente al usado en la <a href="#sec-estimacion-parametros" class="quarto-xref"><span>Sección 3.5.2</span></a>.</p>
<div id="a64fa818" class="cell" data-execution_count="23">
<div class="sourceCode cell-code" id="cb15"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a>likelihood <span class="op">=</span> []</span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> k <span class="kw">in</span> labels:</span>
<span id="cb15-3"><a href="#cb15-3" aria-hidden="true" tabindex="-1"></a>    mask <span class="op">=</span> y_t <span class="op">==</span> k</span>
<span id="cb15-4"><a href="#cb15-4" aria-hidden="true" tabindex="-1"></a>    mu <span class="op">=</span> np.mean(T[mask], axis<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb15-5"><a href="#cb15-5" aria-hidden="true" tabindex="-1"></a>    var <span class="op">=</span> np.var(T[mask], axis<span class="op">=</span><span class="dv">0</span>, ddof<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb15-6"><a href="#cb15-6" aria-hidden="true" tabindex="-1"></a>    likelihood_k <span class="op">=</span> multivariate_normal(mean<span class="op">=</span>mu, cov<span class="op">=</span>var)</span>
<span id="cb15-7"><a href="#cb15-7" aria-hidden="true" tabindex="-1"></a>    likelihood.append(likelihood_k)</span></code><button title="Copiar al portapapeles" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="3128041b" class="cell" data-execution_count="24">
<div class="cell-output cell-output-display" data-execution_count="24">
<p>Los parámetros estimados en la versión ingenua son equivalentes con respecto a las medias, i.e., <span class="math inline">\(\hat \mu_1 = [5.0133, 5.0293]^\intercal\)</span>, <span class="math inline">\(\hat \mu_2 = [1.4612, -1.4185] ^\intercal\)</span> y <span class="math inline">\(\hat \mu_3 = [12.5317, -3.4426]^\intercal\)</span>. La diferencia se puede observar en las varianzas, que a continuación se muestran como matrices de covarianza para resaltar la diferencia, i.e., <span class="math inline">\(\hat \Sigma_1 = \begin{pmatrix} 4.0789 &amp; 0.0000\\0.0000 &amp; 1.8829 \\ \end{pmatrix}\)</span>, <span class="math inline">\(\hat \Sigma_2 = \begin{pmatrix} 1.9812 &amp; 0.0000\\0.0000 &amp; 2.9672 \\ \end{pmatrix}\)</span> y <span class="math inline">\(\hat \Sigma_3 = \begin{pmatrix} 1.7917 &amp; 0.0000\\0.0000 &amp; 6.2007 \\ \end{pmatrix}\)</span> se observa como los elementos fuera de la diagonal son ceros, lo cual indica la independencia entra las variables de entrada.</p>
</div>
</div>
<p>Finalmente, el código para predecir se utiliza el código descrito en la <a href="#sec-prediccion" class="quarto-xref"><span>Sección 3.5.3</span></a> dado que el modelo está dado en las variables <code>likelihood</code> y <code>prior</code>.</p>
<div id="db3c6b18" class="cell" data-execution_count="25">
<div class="cell-output cell-output-display" data-execution_count="25">
<p>El <code>error</code> del clasificador Bayesiano Ingenuo, en el conjunto de prueba, es de <span class="math inline">\(0.01\)</span> y su error estándar (<code>se_formula</code>) es <span class="math inline">\(0.0041.\)</span></p>
</div>
</div>
</section>
<section id="sec-ejemplo-breast-cancer-wisconsin" class="level2" data-number="3.7">
<h2 data-number="3.7" class="anchored" data-anchor-id="sec-ejemplo-breast-cancer-wisconsin"><span class="header-section-number">3.7</span> Ejemplo: Breast Cancer Wisconsin</h2>
<p>Esta sección ilustra el uso del clasificador Bayesiano al generar dos modelos (Clasificador Bayesiano y Bayesiano Ingenuo) del conjunto de datos de <em>Breast Cancer Wisconsin.</em> Estos datos se pueden obtener utilizando la función <code>load_breast_cancer</code> tal y como se muestra a continuación.</p>
<div id="5bae95d5" class="cell" data-execution_count="26">
<div class="sourceCode cell-code" id="cb16"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a>X, y <span class="op">=</span> load_breast_cancer(return_X_y<span class="op">=</span><span class="va">True</span>)</span></code><button title="Copiar al portapapeles" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>El primer paso es contar con los conjuntos de <a href="../AprendizajeComputacional/capitulos/03Parametricos/#conjunto-de-entrenamiento-y-prueba">entrenamiento y prueba</a> para poder realizar de manera completa la evaluación del proceso de clasificación. Esto se realiza ejecutando la siguiente instrucción.</p>
<div id="6c92fc8b" class="cell" data-execution_count="27">
<div class="sourceCode cell-code" id="cb17"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a>T, G, y_t, y_g <span class="op">=</span> train_test_split(X, y, test_size<span class="op">=</span><span class="fl">0.2</span>)</span></code><button title="Copiar al portapapeles" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<section id="entrenamiento" class="level3" data-number="3.7.1">
<h3 data-number="3.7.1" class="anchored" data-anchor-id="entrenamiento"><span class="header-section-number">3.7.1</span> Entrenamiento</h3>
<p>Los dos modelos que se utilizarán será el clasificador Bayesiano Gausiano y Bayesiano Ingenuo, utilizando la clase <code>GaussianBayes</code> que se explica en el <a href="15Codigo.html" class="quarto-xref"><span>Apéndice B</span></a>. Las siguientes dos instrucciones inicializan estos dos clasificadores, la única diferencia es el parámetro <code>naive</code> que indica si el clasificador es ingenuo.</p>
<div id="30d3a2d6" class="cell" data-execution_count="28">
<div class="sourceCode cell-code" id="cb18"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a>gaussian <span class="op">=</span> GaussianBayes().fit(T, y_t)</span>
<span id="cb18-2"><a href="#cb18-2" aria-hidden="true" tabindex="-1"></a>naive <span class="op">=</span> GaussianBayes(naive<span class="op">=</span><span class="va">True</span>).fit(T, y_t)</span></code><button title="Copiar al portapapeles" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="predicción" class="level3" data-number="3.7.2">
<h3 data-number="3.7.2" class="anchored" data-anchor-id="predicción"><span class="header-section-number">3.7.2</span> Predicción</h3>
<p>Habiendo definido los dos clasificadores, las predicciones del conjunto de prueba se realiza de la siguiente manera.</p>
<div id="e06cb4a5" class="cell" data-execution_count="29">
<div class="sourceCode cell-code" id="cb19"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a>hy_gaussian <span class="op">=</span> gaussian.predict(G)</span>
<span id="cb19-2"><a href="#cb19-2" aria-hidden="true" tabindex="-1"></a>hy_naive <span class="op">=</span> naive.predict(G)</span></code><button title="Copiar al portapapeles" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="sec-gaussina-perf-breast_cancer" class="level3" data-number="3.7.3">
<h3 data-number="3.7.3" class="anchored" data-anchor-id="sec-gaussina-perf-breast_cancer"><span class="header-section-number">3.7.3</span> Rendimiento</h3>
<p>El rendimiento de ambos clasificadores se calcula de la siguiente manera</p>
<div id="33aed6de" class="cell" data-execution_count="30">
<div class="sourceCode cell-code" id="cb20"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a>error_gaussian <span class="op">=</span> (y_g <span class="op">!=</span> hy_gaussian).mean()</span>
<span id="cb20-2"><a href="#cb20-2" aria-hidden="true" tabindex="-1"></a>error_naive <span class="op">=</span> (y_g <span class="op">!=</span> hy_naive).mean()</span></code><button title="Copiar al portapapeles" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>El clasificador Bayesiano Gausiano tiene un error de <span class="math inline">\(0.0614\)</span> y el error de Bayesiano Ingenuo es <span class="math inline">\(0.0877.\)</span> Se ha visto que el error es una variable aleatoria, entonces la pregunta es saber si esta diferencia en rendimiento es significativa o es una diferencia que proviene de la aleatoriedad de los datos.</p>
</section>
</section>
<section id="diferencias-en-rendimiento" class="level2" data-number="3.8">
<h2 data-number="3.8" class="anchored" data-anchor-id="diferencias-en-rendimiento"><span class="header-section-number">3.8</span> Diferencias en Rendimiento</h2>
<p>Una manera de ver si existe una diferencia en rendimiento es calcular la diferencia entre los dos errores de clasificación, esto es</p>
<div id="551386c1" class="cell" data-execution_count="31">
<div class="sourceCode cell-code" id="cb21"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a>naive <span class="op">=</span> (y_g <span class="op">!=</span> hy_naive).mean()</span>
<span id="cb21-2"><a href="#cb21-2" aria-hidden="true" tabindex="-1"></a>completo <span class="op">=</span> (y_g <span class="op">!=</span> hy_gaussian).mean()</span>
<span id="cb21-3"><a href="#cb21-3" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span> naive <span class="op">&gt;</span> completo:</span>
<span id="cb21-4"><a href="#cb21-4" aria-hidden="true" tabindex="-1"></a>    diff <span class="op">=</span> naive <span class="op">-</span> completo</span>
<span id="cb21-5"><a href="#cb21-5" aria-hidden="true" tabindex="-1"></a><span class="cf">else</span>:</span>
<span id="cb21-6"><a href="#cb21-6" aria-hidden="true" tabindex="-1"></a>    diff <span class="op">=</span> completo <span class="op">-</span> naive</span></code><button title="Copiar al portapapeles" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>que tiene un valor de <span class="math inline">\(0.0088\)</span>. De la misma manera que se ha utilizado la técnica de bootstrap (<a href="14Estadistica.html#sec-bootstrap" class="quarto-xref"><span>Sección A.2</span></a>) para calcular el error estándar de la media, se puede usar para estimar el error estándar de la diferencia en rendimiento. El siguiente código muestra el procedimiento para estimar este error estándar.</p>
<div id="6459a697" class="cell" data-execution_count="33">
<div class="sourceCode cell-code" id="cb22"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb22-1"><a href="#cb22-1" aria-hidden="true" tabindex="-1"></a>S <span class="op">=</span> np.random.randint(y_g.shape[<span class="dv">0</span>],</span>
<span id="cb22-2"><a href="#cb22-2" aria-hidden="true" tabindex="-1"></a>                      size<span class="op">=</span>(<span class="dv">500</span>, y_g.shape[<span class="dv">0</span>]))</span>
<span id="cb22-3"><a href="#cb22-3" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span> naive <span class="op">&gt;</span> completo:</span>
<span id="cb22-4"><a href="#cb22-4" aria-hidden="true" tabindex="-1"></a>    diff_f <span class="op">=</span> <span class="kw">lambda</span> s: (y_g[s] <span class="op">!=</span> hy_naive[s]).mean() <span class="op">-\</span></span>
<span id="cb22-5"><a href="#cb22-5" aria-hidden="true" tabindex="-1"></a>                       (y_g[s] <span class="op">!=</span> hy_gaussian[s]).mean()</span>
<span id="cb22-6"><a href="#cb22-6" aria-hidden="true" tabindex="-1"></a><span class="cf">else</span>:</span>
<span id="cb22-7"><a href="#cb22-7" aria-hidden="true" tabindex="-1"></a>    diff_f <span class="op">=</span> <span class="kw">lambda</span> s: (y_g[s] <span class="op">!=</span> hy_gaussian[s]).mean() <span class="op">-\</span></span>
<span id="cb22-8"><a href="#cb22-8" aria-hidden="true" tabindex="-1"></a>                       (y_g[s] <span class="op">!=</span> hy_naive[s]).mean()</span>
<span id="cb22-9"><a href="#cb22-9" aria-hidden="true" tabindex="-1"></a>B <span class="op">=</span> [diff_f(s) <span class="cf">for</span> s <span class="kw">in</span> S]</span>
<span id="cb22-10"><a href="#cb22-10" aria-hidden="true" tabindex="-1"></a>se <span class="op">=</span> np.std(B, axis<span class="op">=</span><span class="dv">0</span>)</span></code><button title="Copiar al portapapeles" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>El error estándar de la diferencia de rendimiento es de <span class="math inline">\(0.0149\)</span>, una procedimiento simple para saber si la diferencia observada es significativa, es dividir la diferencia entre su error estándar dando un valor de <span class="math inline">\(0.5874\)</span>. En el caso que el valor absoluto fuera igual o superior a 2 se sabría que la diferencia es significativa con una confianza de al menos 95%, esto asumiendo que la diferencia se comporta como una distribución Gausiana.</p>
<p>El histograma de los datos que se tienen en la variable <code>B</code> se observa en la <a href="#fig-diff-cl-bayesianos" class="quarto-xref">Figura&nbsp;<span>3.1</span></a>. Se puede ver que la forma del histograma asemeja una distribución Gausiana y que el cero esta en el cuerpo de la Gausiana, tal y como lo confirmó el cociente que se calculado.</p>
<div id="cell-fig-diff-cl-bayesianos" class="cell" data-execution_count="35">
<details class="code-fold">
<summary>Código</summary>
<div class="sourceCode cell-code" id="cb23"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb23-1"><a href="#cb23-1" aria-hidden="true" tabindex="-1"></a>sns.set_style(<span class="st">'whitegrid'</span>)</span>
<span id="cb23-2"><a href="#cb23-2" aria-hidden="true" tabindex="-1"></a>fig <span class="op">=</span> sns.displot(B, kde<span class="op">=</span><span class="va">True</span>)</span></code><button title="Copiar al portapapeles" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div id="fig-diff-cl-bayesianos" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-diff-cl-bayesianos-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="03Parametricos_files/figure-html/fig-diff-cl-bayesianos-output-1.png" width="471" height="470" class="figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-diff-cl-bayesianos-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figura&nbsp;3.1: Diferencia entre Clasificadores Bayesianos
</figcaption>
</figure>
</div>
</div>
</div>
<p>Se puede conocer la probabilidad de manera exacta calculando el área bajo la curva a la izquierda del cero, este sería el valor <span class="math inline">\(p\)</span>, si este es menor a 0.05 quiere decir que se tiene una confianza mayor del 95% de que los rendimientos son diferentes. Para este ejemplo, el área se calcula con el siguiente código</p>
<div id="1a481a5d" class="cell" data-execution_count="36">
<div class="sourceCode cell-code" id="cb24"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb24-1"><a href="#cb24-1" aria-hidden="true" tabindex="-1"></a>dist <span class="op">=</span> norm(loc<span class="op">=</span>diff, scale<span class="op">=</span>se)</span>
<span id="cb24-2"><a href="#cb24-2" aria-hidden="true" tabindex="-1"></a>p_value <span class="op">=</span> dist.cdf(<span class="dv">0</span>)</span></code><button title="Copiar al portapapeles" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>teniendo el valor de <span class="math inline">\(0.2785\)</span>, lo que significa que se tiene una confianza del <span class="math inline">\(72\)</span>% de que los dos algoritmos son diferentes considerando el error de clasificación como medida de rendimiento.</p>
</section>
<section id="sec-regresion-ols" class="level2" data-number="3.9">
<h2 data-number="3.9" class="anchored" data-anchor-id="sec-regresion-ols"><span class="header-section-number">3.9</span> Regresión</h2>
<p>Hasta este momento se han revisado métodos paramétricos en clasificación, ahora es el turno de abordar el problema de regresión. La diferencia entre clasificación y regresión como se describió en la <a href="01Introduccion.html#sec-aprendizaje-supervisado" class="quarto-xref"><span>Sección 1.4</span></a> es que en regresión <span class="math inline">\(\mathcal Y \in \mathbb R.\)</span></p>
<p>El procedimiento de regresión que se describe en esta sección es regresión de <strong>Mínimos Cuadrados Ordinaria</strong> (OLS -<em>Ordinary Least Squares</em>-), en el cual se asume que <span class="math inline">\(\mathcal Y \sim \mathcal N(\mathbf w \cdot \mathbf x + \epsilon, \sigma^2)\)</span>, de tal manera que <span class="math inline">\(y = \mathbb E[\mathcal N(\mathbf w \cdot \mathbf x + \epsilon, \sigma^2)].\)</span></p>
<p>Trabajando con <span class="math inline">\(y = \mathbb E[\mathcal N(\mathbf w \cdot \mathbf x + \epsilon, \sigma^2)],\)</span> se considera lo siguiente <span class="math inline">\(y = \mathbb E[\mathcal N(\mathbf w \cdot \mathbf x, 0) + \mathcal N(0, \sigma^2)]\)</span> que implica que el error <span class="math inline">\(\epsilon\)</span> es independiente de <span class="math inline">\(\mathbf x\)</span>, lo cual se transforma en <span class="math inline">\(y = \mathbf w \cdot \mathbf x + \mathbb E[\epsilon],\)</span> donde <span class="math inline">\(\mathbb E[\epsilon]=0.\)</span> Por lo tanto <span class="math inline">\(y = \mathbf w \cdot \mathbf x.\)</span></p>
<p>La función de densidad de probabilidad de una Gausiana corresponde a</p>
<p><span class="math display">\[
f(\alpha) = \frac{1}{\sigma \sqrt{2 \pi}} \exp{-\frac{1}{2} (\frac{\alpha -  \mu}{\sigma})^2},
\]</span></p>
<p>donde <span class="math inline">\(\alpha\)</span>, en el caso de regresión, corresponde a <span class="math inline">\(\mathbf w \cdot \mathbf x\)</span> (i.e., <span class="math inline">\(\alpha = \mathbf w \cdot \mathbf x\)</span>).</p>
<p>Utilizando el método de verosimilitud el cual corresponde a maximizar</p>
<p><span class="math display">\[
\begin{split}
\mathcal L(\mathbf w, \sigma) &amp;= \prod_{(\mathbf x, y) \in \mathcal D} f(\mathbf w \cdot \mathbf x) \\
&amp;= \prod_{(\mathbf x, y) \in \mathcal D} \frac{1}{\sigma \sqrt{2\pi}} \exp{(-\frac{1}{2} (\frac{\mathbf w \cdot \mathbf x -  y}{\sigma})^2)} \\
\ell(\mathbf w, \sigma) &amp;= \sum_{(\mathbf x, y) \in \mathcal D}\log \frac{1}{\sigma \sqrt{2\pi}}  -\frac{1}{2} (\frac{\mathbf w \cdot \mathbf x -  y}{\sigma})^2 \\
&amp;= - \frac{1}{2\sigma^2}  \sum_{(\mathbf x, y) \in \mathcal D} (\mathbf w \cdot \mathbf x -  y)^2 - N \log \frac{1}{\sigma \sqrt{2\pi}}.
\end{split}
\]</span></p>
<p>El valor de cada parámetro se obtiene al calcular la derivada parcial con respecto al parámetro de interés, entonces se resuelven <span class="math inline">\(d\)</span> derivadas parciales para cada uno de los coeficientes <span class="math inline">\(\mathbf w\)</span>. En este proceso se observar que el término <span class="math inline">\(N \log \frac{1}{\sigma \sqrt{2\pi}}\)</span> no depende de <span class="math inline">\(\mathbf w\)</span> entonces no afecta el máximo siendo una constante en el proceso de derivación y por lo tanto se desprecia. Lo mismo pasa para la constante <span class="math inline">\(\frac{1}{2\sigma^2}\)</span>. Una vez obtenidos los parámetros <span class="math inline">\(\mathcal w\)</span> se obtiene el valor <span class="math inline">\(\sigma.\)</span></p>
<p>Una manera equivalente de plantear este problema es como un problema de algebra lineal, donde se tiene una matriz de observaciones <span class="math inline">\(X\)</span> que se construyen con las variables <span class="math inline">\(\mathbf x\)</span> de <span class="math inline">\(\mathcal X,\)</span> donde cada renglón de <span class="math inline">\(X\)</span> es una observación, y el vector dependiente <span class="math inline">\(\mathbf y\)</span> donde cada elemento es la respuesta correspondiente a la observación.</p>
<p>Viéndolo como un problema de algebra lineal lo que se tiene es</p>
<p><span class="math display">\[
X \mathbf w = \mathbf y,
\]</span></p>
<p>donde para identificar <span class="math inline">\(\mathbf w\)</span> se pueden realizar lo siguiente</p>
<p><span class="math display">\[
X^\intercal X \mathbf w = X^\intercal \mathbf y.
\]</span></p>
<p>Despejando <span class="math inline">\(\mathbf w\)</span> se tiene</p>
<p><span class="math display">\[
\mathbf w = (X^\intercal X)^{-1} X^\intercal \mathbf y.
\]</span></p>
<p>Previamente se ha presentado el error estándar de cada parámetro que se ha estimado, en caso de la regresión el error estándar (<a href="14Estadistica.html#sec-error-estandar-ols" class="quarto-xref"><span>Sección A.1.3</span></a>) de <span class="math inline">\(\mathcal w_j\)</span> es <span class="math inline">\(\sigma \sqrt{(X^\intercal X)^{-1}_{jj}}.\)</span></p>
<section id="sec-diabetes" class="level3" data-number="3.9.1">
<h3 data-number="3.9.1" class="anchored" data-anchor-id="sec-diabetes"><span class="header-section-number">3.9.1</span> Ejemplo: Diabetes</h3>
<p>Esta sección ilustra el proceso de resolver un problema de regresión utilizando OLS. El problema a resolver se obtiene mediante la función <code>load_diabetes</code> de la siguiente manera</p>
<div id="a7a0a61c" class="cell" data-execution_count="38">
<div class="sourceCode cell-code" id="cb25"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb25-1"><a href="#cb25-1" aria-hidden="true" tabindex="-1"></a>X, y <span class="op">=</span> load_diabetes(return_X_y<span class="op">=</span><span class="va">True</span>)</span></code><button title="Copiar al portapapeles" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>El siguiente paso es generar los conjuntos de entrenamiento y prueba (<a href="#sec-conjunto-entre-prueba" class="quarto-xref"><span>Sección 3.5</span></a>)</p>
<div id="6f21dcbe" class="cell" data-execution_count="39">
<div class="sourceCode cell-code" id="cb26"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb26-1"><a href="#cb26-1" aria-hidden="true" tabindex="-1"></a>T, G, y_t, y_g <span class="op">=</span> train_test_split(X, y, test_size<span class="op">=</span><span class="fl">0.2</span>)</span></code><button title="Copiar al portapapeles" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Con el conjunto de entrenamiento <code>T</code> y <code>y_t</code> se estiman los parámetros de la regresión lineal tal y como se muestra a continuación</p>
<div id="4b5a902f" class="cell" data-execution_count="40">
<div class="sourceCode cell-code" id="cb27"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb27-1"><a href="#cb27-1" aria-hidden="true" tabindex="-1"></a>m <span class="op">=</span> LinearRegression().fit(T, y_t)</span></code><button title="Copiar al portapapeles" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="11fb7329" class="cell" data-execution_count="41">
<div class="sourceCode cell-code" id="cb28"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb28-1"><a href="#cb28-1" aria-hidden="true" tabindex="-1"></a>w_f <span class="op">=</span> <span class="st">', '</span>.join([<span class="ss">f'</span><span class="sc">{</span>v<span class="sc">:0.2f}</span><span class="ss">'</span> <span class="cf">for</span> v <span class="kw">in</span> m.coef_[:<span class="dv">3</span>]])</span>
<span id="cb28-2"><a href="#cb28-2" aria-hidden="true" tabindex="-1"></a>w_f <span class="op">=</span> Markdown(<span class="ss">f'$</span><span class="ch">\\</span><span class="ss">mathbf w=[</span><span class="sc">{</span>w_f<span class="sc">}</span><span class="ss">, \ldots]$'</span>)</span>
<span id="cb28-3"><a href="#cb28-3" aria-hidden="true" tabindex="-1"></a>w_0 <span class="op">=</span> Markdown(<span class="ss">f'$w_0=</span><span class="sc">{</span>m<span class="sc">.</span>intercept_<span class="sc">:0.2f}</span><span class="ss">$'</span>)</span></code><button title="Copiar al portapapeles" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Los primeros tres coeficientes de la regresión lineal son <span class="math inline">\(\mathbf w=[-18.06, -281.54, 542.41, \ldots]\)</span> y <span class="math inline">\(w_0=154.04\)</span> lo cual se encuentran en las siguientes variables</p>
<div id="44cdd5c1" class="cell" data-execution_count="42">
<div class="sourceCode cell-code" id="cb29"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb29-1"><a href="#cb29-1" aria-hidden="true" tabindex="-1"></a>w <span class="op">=</span> m.coef_</span>
<span id="cb29-2"><a href="#cb29-2" aria-hidden="true" tabindex="-1"></a>w_0 <span class="op">=</span> m.intercept_</span></code><button title="Copiar al portapapeles" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>La pregunta es si estos coeficientes son estadísticamente diferentes de cero, esto se puede conocer calculando el error estándar de cada coeficiente. Para lo cual se requiere estimar <span class="math inline">\(\sigma\)</span> que corresponde a la desviación estándar del error tal y como se muestra en las siguientes instrucciones.</p>
<div id="16ce5347" class="cell" data-execution_count="43">
<div class="sourceCode cell-code" id="cb30"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb30-1"><a href="#cb30-1" aria-hidden="true" tabindex="-1"></a>error <span class="op">=</span> y_t <span class="op">-</span> m.predict(T)</span>
<span id="cb30-2"><a href="#cb30-2" aria-hidden="true" tabindex="-1"></a>std_error <span class="op">=</span> np.std(error)</span></code><button title="Copiar al portapapeles" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>El error estándar de <span class="math inline">\(\mathbf w\)</span> es</p>
<div id="f105ae62" class="cell" data-execution_count="44">
<div class="sourceCode cell-code" id="cb31"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb31-1"><a href="#cb31-1" aria-hidden="true" tabindex="-1"></a>diag <span class="op">=</span> np.arange(T.shape[<span class="dv">1</span>])</span>
<span id="cb31-2"><a href="#cb31-2" aria-hidden="true" tabindex="-1"></a>_ <span class="op">=</span> np.sqrt((np.dot(T.T, T)<span class="op">**</span>(<span class="op">-</span><span class="dv">1</span>))[diag, diag])</span>
<span id="cb31-3"><a href="#cb31-3" aria-hidden="true" tabindex="-1"></a>se <span class="op">=</span> std_error <span class="op">*</span> _</span></code><button title="Copiar al portapapeles" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>y para saber si los coeficientes son significativamente diferente de cero se calcula el cociente <code>m.coef_</code> entre <code>se</code>; teniendo los siguientes valores <span class="math inline">\([-0.31, -4.69, 9.03, \ldots]\)</span>, para las tres primeras componentes. Se observa que hay varios coeficientes con valor absoluto menor que 2, lo cual significa que esas variables tiene un coeficiente que estadísticamente no es diferente de cero.</p>
<p>La predicción del conjunto de prueba se puede realizar con la siguiente instrucción</p>
<div id="bf8217c3" class="cell" data-execution_count="46">
<div class="sourceCode cell-code" id="cb32"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb32-1"><a href="#cb32-1" aria-hidden="true" tabindex="-1"></a>hy <span class="op">=</span> m.predict(G)</span></code><button title="Copiar al portapapeles" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Finalmente, la <a href="#fig-regresion-lineal-scatter" class="quarto-xref">Figura&nbsp;<span>3.2</span></a> muestra las predicciones contra las mediciones reales. También se incluye la línea que ilustra el modelo ideal.</p>
<div id="cell-fig-regresion-lineal-scatter" class="cell" data-execution_count="47">
<details class="code-fold">
<summary>Código</summary>
<div class="sourceCode cell-code" id="cb33"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb33-1"><a href="#cb33-1" aria-hidden="true" tabindex="-1"></a>sns.scatterplot(x<span class="op">=</span>hy, y<span class="op">=</span>y_g)</span>
<span id="cb33-2"><a href="#cb33-2" aria-hidden="true" tabindex="-1"></a>_min <span class="op">=</span> <span class="bu">min</span>(y_g.<span class="bu">min</span>(), hy.<span class="bu">min</span>())</span>
<span id="cb33-3"><a href="#cb33-3" aria-hidden="true" tabindex="-1"></a>_max <span class="op">=</span> <span class="bu">max</span>(y_g.<span class="bu">max</span>(), hy.<span class="bu">max</span>())</span>
<span id="cb33-4"><a href="#cb33-4" aria-hidden="true" tabindex="-1"></a>sns.set_style(<span class="st">'whitegrid'</span>)</span>
<span id="cb33-5"><a href="#cb33-5" aria-hidden="true" tabindex="-1"></a>fig <span class="op">=</span> sns.lineplot(x<span class="op">=</span>[_min, _max], y<span class="op">=</span>[_min, _max])</span></code><button title="Copiar al portapapeles" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div id="fig-regresion-lineal-scatter" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-regresion-lineal-scatter-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="03Parametricos_files/figure-html/fig-regresion-lineal-scatter-output-1.png" width="581" height="414" class="figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-regresion-lineal-scatter-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figura&nbsp;3.2: Regresión Lineal
</figcaption>
</figure>
</div>
</div>
</div>
<p>Complementando el ejemplo anterior, se realiza un modelo que primero elimina las variables que no son estadísticamente diferentes de cero (primera línea) y después crea nuevas variables al incluir el cuadrado, ver las líneas dos y tres del siguiente código.</p>
<div id="ff480ec7" class="cell" data-execution_count="48">
<div class="sourceCode cell-code" id="cb34"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb34-1"><a href="#cb34-1" aria-hidden="true" tabindex="-1"></a>mask <span class="op">=</span> np.fabs(m.coef_ <span class="op">/</span> se) <span class="op">&gt;=</span> <span class="dv">2</span></span>
<span id="cb34-2"><a href="#cb34-2" aria-hidden="true" tabindex="-1"></a>T <span class="op">=</span> np.concatenate((T[:, mask], T[:, mask]<span class="op">**</span><span class="dv">2</span>), axis<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb34-3"><a href="#cb34-3" aria-hidden="true" tabindex="-1"></a>G <span class="op">=</span> np.concatenate((G[:, mask], G[:, mask]<span class="op">**</span><span class="dv">2</span>), axis<span class="op">=</span><span class="dv">1</span>)</span></code><button title="Copiar al portapapeles" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Se observa que la identificación de los coeficientes <span class="math inline">\(\mathbf w\)</span> sigue siendo lineal aun y cuando la representación ya no es lineal por incluir el cuadrado. Siguiendo los pasos descritos previamente, se inicializa el modelo y después se realiza la predicción.</p>
<div id="d4554385" class="cell" data-execution_count="49">
<div class="sourceCode cell-code" id="cb35"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb35-1"><a href="#cb35-1" aria-hidden="true" tabindex="-1"></a>m2 <span class="op">=</span> LinearRegression().fit(T, y_t)</span>
<span id="cb35-2"><a href="#cb35-2" aria-hidden="true" tabindex="-1"></a>hy2 <span class="op">=</span> m2.predict(G)</span></code><button title="Copiar al portapapeles" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>En este momento se compara si la diferencia entre el error cuadrático medio, del primer y segundo modelo, la diferencia es <span class="math inline">\(-62.0983\)</span> indicando que el primer modelo es mejor.</p>
<div class="sourceCode" id="cb36"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb36-1"><a href="#cb36-1" aria-hidden="true" tabindex="-1"></a>diff <span class="op">=</span> ((y_g <span class="op">-</span> hy2)<span class="op">**</span><span class="dv">2</span>).mean() <span class="op">-</span>  ((y_g <span class="op">-</span> hy)<span class="op">**</span><span class="dv">2</span>).mean()</span></code><button title="Copiar al portapapeles" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Para comprobar si esta diferencia es significativa se calcula el error estándar, utilizando bootstrap (<a href="14Estadistica.html#sec-bootstrap" class="quarto-xref"><span>Sección A.2</span></a>) tal y como se muestra a continuación.</p>
<div id="2d04ca1d" class="cell" data-execution_count="51">
<div class="sourceCode cell-code" id="cb37"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb37-1"><a href="#cb37-1" aria-hidden="true" tabindex="-1"></a>S <span class="op">=</span> np.random.randint(y_g.shape[<span class="dv">0</span>],</span>
<span id="cb37-2"><a href="#cb37-2" aria-hidden="true" tabindex="-1"></a>                      size<span class="op">=</span>(<span class="dv">500</span>, y_g.shape[<span class="dv">0</span>]))</span>
<span id="cb37-3"><a href="#cb37-3" aria-hidden="true" tabindex="-1"></a>B <span class="op">=</span> [((y_g[s] <span class="op">-</span> hy2[s])<span class="op">**</span><span class="dv">2</span>).mean() <span class="op">-</span></span>
<span id="cb37-4"><a href="#cb37-4" aria-hidden="true" tabindex="-1"></a>      ((y_g[s] <span class="op">-</span> hy[s])<span class="op">**</span><span class="dv">2</span>).mean()</span>
<span id="cb37-5"><a href="#cb37-5" aria-hidden="true" tabindex="-1"></a>     <span class="cf">for</span> s <span class="kw">in</span> S]</span>
<span id="cb37-6"><a href="#cb37-6" aria-hidden="true" tabindex="-1"></a>se <span class="op">=</span> np.std(B, axis<span class="op">=</span><span class="dv">0</span>)</span></code><button title="Copiar al portapapeles" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Finalmente, se calcula el área bajo la curva a la izquierda del cero, teniendo un valor de <span class="math inline">\(0.5000\)</span> lo cual indica que los dos modelos son similares. En este caso se prefiere el modelo más simple porque se observar que incluir el cuadrado de las variables no contribuye a generar un mejor model. El área bajo la curva se calcula con el siguiente código.</p>
<div id="d5216e80" class="cell" data-execution_count="53">
<div class="sourceCode cell-code" id="cb38"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb38-1"><a href="#cb38-1" aria-hidden="true" tabindex="-1"></a>dist <span class="op">=</span> norm(loc<span class="op">=</span>diff, scale<span class="op">=</span>se)</span>
<span id="cb38-2"><a href="#cb38-2" aria-hidden="true" tabindex="-1"></a>p_value <span class="op">=</span> dist.cdf(<span class="dv">0</span>)</span></code><button title="Copiar al portapapeles" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>


<!-- -->

</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copiado");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copiado");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    // For code content inside modals, clipBoardJS needs to be initialized with a container option
    // TODO: Check when it could be a function (https://github.com/zenorocha/clipboard.js/issues/860)
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
  const viewSource = window.document.getElementById('quarto-view-source') ||
                     window.document.getElementById('quarto-code-tools-source');
  if (viewSource) {
    const sourceUrl = viewSource.getAttribute("data-quarto-source-url");
    viewSource.addEventListener("click", function(e) {
      if (sourceUrl) {
        // rstudio viewer pane
        if (/\bcapabilities=\b/.test(window.location)) {
          window.open(sourceUrl);
        } else {
          window.location.href = sourceUrl;
        }
      } else {
        const modal = new bootstrap.Modal(document.getElementById('quarto-embedded-source-code-modal'));
        modal.show();
      }
      return false;
    });
  }
  function toggleCodeHandler(show) {
    return function(e) {
      const detailsSrc = window.document.querySelectorAll(".cell > details > .sourceCode");
      for (let i=0; i<detailsSrc.length; i++) {
        const details = detailsSrc[i].parentElement;
        if (show) {
          details.open = true;
        } else {
          details.removeAttribute("open");
        }
      }
      const cellCodeDivs = window.document.querySelectorAll(".cell > .sourceCode");
      const fromCls = show ? "hidden" : "unhidden";
      const toCls = show ? "unhidden" : "hidden";
      for (let i=0; i<cellCodeDivs.length; i++) {
        const codeDiv = cellCodeDivs[i];
        if (codeDiv.classList.contains(fromCls)) {
          codeDiv.classList.remove(fromCls);
          codeDiv.classList.add(toCls);
        } 
      }
      return false;
    }
  }
  const hideAllCode = window.document.getElementById("quarto-hide-all-code");
  if (hideAllCode) {
    hideAllCode.addEventListener("click", toggleCodeHandler(false));
  }
  const showAllCode = window.document.getElementById("quarto-show-all-code");
  if (showAllCode) {
    showAllCode.addEventListener("click", toggleCodeHandler(true));
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp("https:\/\/ingeotec\.github\.io\/AprendizajeComputacional");
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="../capitulos/02Teoria_Decision.html" class="pagination-link" aria-label="Teoría de Decisión Bayesiana">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Teoría de Decisión Bayesiana</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="../capitulos/04Rendimiento.html" class="pagination-link" aria-label="Rendimiento">
        <span class="nav-page-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Rendimiento</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav><div class="modal fade" id="quarto-embedded-source-code-modal" tabindex="-1" aria-labelledby="quarto-embedded-source-code-modal-label" aria-hidden="true"><div class="modal-dialog modal-dialog-scrollable"><div class="modal-content"><div class="modal-header"><h5 class="modal-title" id="quarto-embedded-source-code-modal-label">Ejecutar el código</h5><button class="btn-close" data-bs-dismiss="modal"></button></div><div class="modal-body"><div class="">
<div class="sourceCode" id="cb39" data-shortcodes="false"><pre class="sourceCode markdown code-with-copy"><code class="sourceCode markdown"><span id="cb39-1"><a href="#cb39-1" aria-hidden="true" tabindex="-1"></a><span class="fu"># Métodos Paramétricos {#sec-metodos-parametricos}</span></span>
<span id="cb39-2"><a href="#cb39-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-3"><a href="#cb39-3" aria-hidden="true" tabindex="-1"></a>El **objetivo** de la unidad es conocer las características de los modelos paramétricos y aplicar máxima verosimilitud para estimar los parámetros del modelo paramétrico en problemas de regresión y clasificación.</span>
<span id="cb39-4"><a href="#cb39-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-5"><a href="#cb39-5" aria-hidden="true" tabindex="-1"></a><span class="fu">## Paquetes usados {.unnumbered}</span></span>
<span id="cb39-6"><a href="#cb39-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-9"><a href="#cb39-9" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb39-10"><a href="#cb39-10" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: true</span></span>
<span id="cb39-11"><a href="#cb39-11" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> EvoMSA.model <span class="im">import</span> GaussianBayes</span>
<span id="cb39-12"><a href="#cb39-12" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.linear_model <span class="im">import</span> LinearRegression</span>
<span id="cb39-13"><a href="#cb39-13" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> train_test_split</span>
<span id="cb39-14"><a href="#cb39-14" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.datasets <span class="im">import</span> load_breast_cancer,<span class="op">\</span></span>
<span id="cb39-15"><a href="#cb39-15" aria-hidden="true" tabindex="-1"></a>                             load_diabetes</span>
<span id="cb39-16"><a href="#cb39-16" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> scipy.stats <span class="im">import</span> norm, multivariate_normal</span>
<span id="cb39-17"><a href="#cb39-17" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> scipy.special <span class="im">import</span> logsumexp</span>
<span id="cb39-18"><a href="#cb39-18" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> matplotlib <span class="im">import</span> pylab <span class="im">as</span> plt</span>
<span id="cb39-19"><a href="#cb39-19" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb39-20"><a href="#cb39-20" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb39-21"><a href="#cb39-21" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> seaborn <span class="im">as</span> sns</span>
<span id="cb39-22"><a href="#cb39-22" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb39-23"><a href="#cb39-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-26"><a href="#cb39-26" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb39-27"><a href="#cb39-27" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: false</span></span>
<span id="cb39-28"><a href="#cb39-28" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> IPython.display <span class="im">import</span> Markdown</span>
<span id="cb39-29"><a href="#cb39-29" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb39-30"><a href="#cb39-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-31"><a href="#cb39-31" aria-hidden="true" tabindex="-1"></a>::: {.content-visible when-format="html"}</span>
<span id="cb39-32"><a href="#cb39-32" aria-hidden="true" tabindex="-1"></a>---</span>
<span id="cb39-33"><a href="#cb39-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-34"><a href="#cb39-34" aria-hidden="true" tabindex="-1"></a>{{&lt; video https://www.youtube.com/embed/Tlwecs3dUPw width="560" height="315" &gt;}}</span>
<span id="cb39-35"><a href="#cb39-35" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-36"><a href="#cb39-36" aria-hidden="true" tabindex="-1"></a>---</span>
<span id="cb39-37"><a href="#cb39-37" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb39-38"><a href="#cb39-38" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-39"><a href="#cb39-39" aria-hidden="true" tabindex="-1"></a><span class="fu">## Introducción</span></span>
<span id="cb39-40"><a href="#cb39-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-41"><a href="#cb39-41" aria-hidden="true" tabindex="-1"></a>Existen diferentes tipos de algoritmos que se puede utilizar para resolver problemas de aprendizaje supervisado y no supervisado. En particular, esta unidad se enfoca en presentar las técnicas que se pueden caracterizar como métodos paramétricos. </span>
<span id="cb39-42"><a href="#cb39-42" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-43"><a href="#cb39-43" aria-hidden="true" tabindex="-1"></a>Los métodos paramétricos se identifican por asumir que los datos provienen de una distribución de la cual se desconocen los parámetros y el procedimiento es encontrar los parámetros de la distribución que mejor modelen los datos. Una vez obtenidos los parámetros se cuenta con todos los elementos para utilizar el modelo y predecir la característica para la cual fue entrenada. </span>
<span id="cb39-44"><a href="#cb39-44" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-45"><a href="#cb39-45" aria-hidden="true" tabindex="-1"></a><span class="fu">## Metodología {#sec-metodologia-met-parametricos}</span></span>
<span id="cb39-46"><a href="#cb39-46" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-47"><a href="#cb39-47" aria-hidden="true" tabindex="-1"></a>Hasta el momento se han presentado ejemplos de los pasos 4 y 5 de la metodología general (ver @sec-metodologia-general); esto fue en la @sec-prediccion-normal y en la @sec-error-clasificacion. Esta sección complementa los ejemplos anteriores al utilizar todos pasos de la metodología general de aprendizaje supervisado (ver @sec-metodologia-general). En particular se enfoca al paso 3 que corresponde al diseño del algoritmo $f$ que modela el fenómeno de interés utilizando los datos $\mathcal T \subset \mathcal D.$</span>
<span id="cb39-48"><a href="#cb39-48" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-49"><a href="#cb39-49" aria-hidden="true" tabindex="-1"></a>El algoritmo $f$ corresponde a asumir que los datos $\mathcal D$ provienen de </span>
<span id="cb39-50"><a href="#cb39-50" aria-hidden="true" tabindex="-1"></a>una distribución $F$ la cual tiene una serie de parámetros $\theta$ que </span>
<span id="cb39-51"><a href="#cb39-51" aria-hidden="true" tabindex="-1"></a>son identificados con $\mathcal T.$</span>
<span id="cb39-52"><a href="#cb39-52" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-53"><a href="#cb39-53" aria-hidden="true" tabindex="-1"></a><span class="fu">## Estimación de Parámetros {#sec-estimacion-parametros-gnal}</span></span>
<span id="cb39-54"><a href="#cb39-54" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-55"><a href="#cb39-55" aria-hidden="true" tabindex="-1"></a>Se inicia la descripción de métodos paramétricos presentando el procedimiento general para estimar los parámetros de una distribución. Se cuenta con un conjunto $\mathcal D$ donde los elementos $x \in \mathcal D$ son $x \in \mathbb R^d$. Los elementos $x \in \mathcal D$ tienen un distribución $F$, i.e., $x \sim F$, son independientes y $F$ está definida por la función de densidad de probabilidad $f_{\theta}$, que a su vez está definida por $\theta$ parámetros. Utilizando $\mathcal D$ el objetivo es identificar los parámetros $\theta$ que hacen observar a $\mathcal D$ lo más probable. </span>
<span id="cb39-56"><a href="#cb39-56" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-57"><a href="#cb39-57" aria-hidden="true" tabindex="-1"></a><span class="fu">### Verosimilitud {#sec-verosimilitud}</span></span>
<span id="cb39-58"><a href="#cb39-58" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-59"><a href="#cb39-59" aria-hidden="true" tabindex="-1"></a>Una solución para maximizar el observar $\mathcal D$ es maximizando la verosimilitud. La verosimilitud es la función distribución conjunta de los elementos en $\mathcal D$, i.e., $f_\theta(x_1, x_2, \ldots, x_N).$ Considerando que la muestras son independientes entonces $f_\theta(x_1, x_2, \ldots, x_N) = \prod_{x \in \mathcal D} f_\theta (x).$ La función de verosimilitud considera la ecuación anterior como una función de los parámetros $\theta,$ es decir,</span>
<span id="cb39-60"><a href="#cb39-60" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-61"><a href="#cb39-61" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb39-62"><a href="#cb39-62" aria-hidden="true" tabindex="-1"></a>\mathcal L(\theta) = \prod_{x \in \mathcal D} f_\theta (x),</span>
<span id="cb39-63"><a href="#cb39-63" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb39-64"><a href="#cb39-64" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-65"><a href="#cb39-65" aria-hidden="true" tabindex="-1"></a>siendo el logaritmo de la verosimilitud </span>
<span id="cb39-66"><a href="#cb39-66" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-67"><a href="#cb39-67" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb39-68"><a href="#cb39-68" aria-hidden="true" tabindex="-1"></a>\ell(\theta) = \log \mathcal L(\theta) = \sum_{x \in \mathcal D} \log f_\theta (x).</span>
<span id="cb39-69"><a href="#cb39-69" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb39-70"><a href="#cb39-70" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-71"><a href="#cb39-71" aria-hidden="true" tabindex="-1"></a><span class="fu">### Distribución de Bernoulli {#sec-distribucción-de-bernoulli}</span></span>
<span id="cb39-72"><a href="#cb39-72" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-73"><a href="#cb39-73" aria-hidden="true" tabindex="-1"></a>La verosimilitud se ejemplifica con la identificación del parámetro $p$ de una distribución Bernoulli. Una distribución Bernoulli modela dos estados, por un lado se tiene la clase negativa identificada por $0$; identificando la clase positiva como $1$. Entonces, la probabilidad de observar $1$ es $\mathbb P(X=1) = p$ y $\mathbb P(X=0) = 1 - p$. Estas ecuaciones se pueden combinar para definir $f_\theta(x) = p^x (1 - p)^{1-x}.$</span>
<span id="cb39-74"><a href="#cb39-74" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-75"><a href="#cb39-75" aria-hidden="true" tabindex="-1"></a>Utilizando el logaritmo de la verosimilitud se tiene:</span>
<span id="cb39-76"><a href="#cb39-76" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-77"><a href="#cb39-77" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb39-78"><a href="#cb39-78" aria-hidden="true" tabindex="-1"></a>\ell(p) = \sum_{i=1}^N \log p^{x_i} (1 - p)^{1-x_i} = \sum_{i=1}^N x_i \log p + (1-x_i) \log (1 - p).</span>
<span id="cb39-79"><a href="#cb39-79" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb39-80"><a href="#cb39-80" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-81"><a href="#cb39-81" aria-hidden="true" tabindex="-1"></a>Recordando que el máximo de $\ell(\mathcal p)$ se obtiene cuando $\frac{d}{dp} \ell(\mathcal p) = 0$, entonces estimar $p$ corresponde a resolver lo siguiente:</span>
<span id="cb39-82"><a href="#cb39-82" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-83"><a href="#cb39-83" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb39-84"><a href="#cb39-84" aria-hidden="true" tabindex="-1"></a>\begin{split}</span>
<span id="cb39-85"><a href="#cb39-85" aria-hidden="true" tabindex="-1"></a>\frac{d}{dp} \ell(\mathcal p) &amp;= 0 <span class="sc">\\</span></span>
<span id="cb39-86"><a href="#cb39-86" aria-hidden="true" tabindex="-1"></a>\frac{d}{dp} <span class="co">[</span><span class="ot"> \sum_{i=1}^N x_i \log p + (1-x_i) \log (1 - p)</span><span class="co">]</span> &amp;= 0 <span class="sc">\\</span> </span>
<span id="cb39-87"><a href="#cb39-87" aria-hidden="true" tabindex="-1"></a>\frac{d}{d p} <span class="co">[</span><span class="ot"> \sum_{i=1}^N x_i \log p + \log (1 - p) (N - \sum_{i=1}^N x_i) </span><span class="co">]</span> &amp;= 0<span class="sc">\\</span></span>
<span id="cb39-88"><a href="#cb39-88" aria-hidden="true" tabindex="-1"></a>\sum_{i=1}^N x_i \frac{d}{d p} \log \mathcal p + (N - \sum_{i=1}^N x_i) \frac{d}{d p} \log (1 - \mathcal p) &amp;= 0<span class="sc">\\</span> </span>
<span id="cb39-89"><a href="#cb39-89" aria-hidden="true" tabindex="-1"></a>\sum_{i=1}^N x_i \frac{1}{p} + (N - \sum_{i=1}^N x_i) \frac{-1}{(1 - p)} &amp;= 0<span class="sc">\\</span> </span>
<span id="cb39-90"><a href="#cb39-90" aria-hidden="true" tabindex="-1"></a>\end{split}</span>
<span id="cb39-91"><a href="#cb39-91" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb39-92"><a href="#cb39-92" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-93"><a href="#cb39-93" aria-hidden="true" tabindex="-1"></a>Realizando algunas operaciones algebraicas se obtiene:</span>
<span id="cb39-94"><a href="#cb39-94" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-95"><a href="#cb39-95" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb39-96"><a href="#cb39-96" aria-hidden="true" tabindex="-1"></a>\hat p = \frac{1}{N}\sum_{i=1}^N x_i.</span>
<span id="cb39-97"><a href="#cb39-97" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb39-98"><a href="#cb39-98" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-99"><a href="#cb39-99" aria-hidden="true" tabindex="-1"></a><span class="fu">### Ejemplo: Distribución Gausiana {#sec-estimacion-distribucion-gausiana}</span></span>
<span id="cb39-100"><a href="#cb39-100" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-101"><a href="#cb39-101" aria-hidden="true" tabindex="-1"></a>Esta sección sigue un camino práctico presentando el código para estimar los parámetros de una distribución Gausiana donde se conocen todos los parámetros. La distribución se usa para generar 1000 muestras y después de esta población se estiman los parámetros; de estas manera se tienen todos los elementos para comparar los parámetros reales $\theta$ de los parámetros estimados $\hat \theta.$</span>
<span id="cb39-102"><a href="#cb39-102" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-103"><a href="#cb39-103" aria-hidden="true" tabindex="-1"></a>La distribución que se usará se utilizó para generar un problema sintético (ver @sec-tres-normales) de tres clases. Los parámetros de la distribución son: $\mathbf \mu = <span class="co">[</span><span class="ot">5, 5</span><span class="co">]</span>^\intercal$ y  $\Sigma = \begin{pmatrix} 4 &amp; 0 <span class="sc">\\</span> 0 &amp; 2 <span class="sc">\\</span> \end{pmatrix}.$ La siguiente instrucción se puede utilizar para generar 1000 muestras de esa distribución. </span>
<span id="cb39-104"><a href="#cb39-104" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-107"><a href="#cb39-107" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb39-108"><a href="#cb39-108" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: true</span></span>
<span id="cb39-109"><a href="#cb39-109" aria-hidden="true" tabindex="-1"></a>D <span class="op">=</span> multivariate_normal(mean<span class="op">=</span>[<span class="dv">5</span>, <span class="dv">5</span>], </span>
<span id="cb39-110"><a href="#cb39-110" aria-hidden="true" tabindex="-1"></a>                        cov<span class="op">=</span>[[<span class="dv">4</span>, <span class="dv">0</span>], </span>
<span id="cb39-111"><a href="#cb39-111" aria-hidden="true" tabindex="-1"></a>                             [<span class="dv">0</span>, <span class="dv">2</span>]]).rvs(size<span class="op">=</span><span class="dv">1000</span>)</span>
<span id="cb39-112"><a href="#cb39-112" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb39-113"><a href="#cb39-113" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-114"><a href="#cb39-114" aria-hidden="true" tabindex="-1"></a>La media estimada de los datos en <span class="in">`D`</span> se calcula usando la función <span class="in">`np.mean`</span> de la siguiente manera</span>
<span id="cb39-115"><a href="#cb39-115" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-118"><a href="#cb39-118" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb39-119"><a href="#cb39-119" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: true</span></span>
<span id="cb39-120"><a href="#cb39-120" aria-hidden="true" tabindex="-1"></a>mu <span class="op">=</span> np.mean(D, axis<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb39-121"><a href="#cb39-121" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb39-122"><a href="#cb39-122" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-125"><a href="#cb39-125" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb39-126"><a href="#cb39-126" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: false</span></span>
<span id="cb39-127"><a href="#cb39-127" aria-hidden="true" tabindex="-1"></a>mu <span class="op">=</span> <span class="st">', '</span>.join([<span class="ss">f'</span><span class="sc">{</span>x<span class="sc">:0.4f}</span><span class="ss">'</span> <span class="cf">for</span> x <span class="kw">in</span> mu])</span>
<span id="cb39-128"><a href="#cb39-128" aria-hidden="true" tabindex="-1"></a>se <span class="op">=</span> np.std(D, axis<span class="op">=</span><span class="dv">0</span>) <span class="op">/</span> np.sqrt(<span class="dv">1000</span>)</span>
<span id="cb39-129"><a href="#cb39-129" aria-hidden="true" tabindex="-1"></a>se <span class="op">=</span> <span class="st">', '</span>.join([<span class="ss">f'</span><span class="sc">{</span>x<span class="sc">:0.4f}</span><span class="ss">'</span> <span class="cf">for</span> x <span class="kw">in</span> se])</span>
<span id="cb39-130"><a href="#cb39-130" aria-hidden="true" tabindex="-1"></a>Markdown(<span class="ss">f"donde el eje de operación es el primero que corresponde al índice $0.$ La media estimada es: $\hat \mu = [</span><span class="sc">{</span>mu<span class="sc">}</span><span class="ss">]^\intercal$ con un [error estándar](ver @sec-error-estandar-media) (`se`) de $[</span><span class="sc">{</span>se<span class="sc">}</span><span class="ss">]^\intercal$ que se calcula con el siguiente código."</span>)</span>
<span id="cb39-131"><a href="#cb39-131" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb39-132"><a href="#cb39-132" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-135"><a href="#cb39-135" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb39-136"><a href="#cb39-136" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: true</span></span>
<span id="cb39-137"><a href="#cb39-137" aria-hidden="true" tabindex="-1"></a>se <span class="op">=</span> np.std(D, axis<span class="op">=</span><span class="dv">0</span>) <span class="op">/</span> np.sqrt(<span class="dv">1000</span>)</span>
<span id="cb39-138"><a href="#cb39-138" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb39-139"><a href="#cb39-139" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-140"><a href="#cb39-140" aria-hidden="true" tabindex="-1"></a>Hasta el momento se ha estimado $\mu$, falta por estimar $\Sigma$, que se puede realizar con la siguiente instrucción</span>
<span id="cb39-141"><a href="#cb39-141" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-144"><a href="#cb39-144" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb39-145"><a href="#cb39-145" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: true</span></span>
<span id="cb39-146"><a href="#cb39-146" aria-hidden="true" tabindex="-1"></a>cov <span class="op">=</span> np.cov(D, rowvar<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb39-147"><a href="#cb39-147" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb39-148"><a href="#cb39-148" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-151"><a href="#cb39-151" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb39-152"><a href="#cb39-152" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: false</span></span>
<span id="cb39-153"><a href="#cb39-153" aria-hidden="true" tabindex="-1"></a>a, b <span class="op">=</span> [<span class="st">'&amp;'</span>.join([<span class="ss">f'</span><span class="sc">{</span>v<span class="sc">:0.4f}</span><span class="ss">'</span> <span class="cf">for</span> v <span class="kw">in</span> col]) <span class="cf">for</span> col <span class="kw">in</span> cov]</span>
<span id="cb39-154"><a href="#cb39-154" aria-hidden="true" tabindex="-1"></a>Markdown(<span class="ss">f"donde el parámetro `rowvar` indica la forma en que están proporcionados los datos. La estimación da los siguientes valores $</span><span class="ch">\\</span><span class="ss">hat </span><span class="ch">\\</span><span class="ss">Sigma = </span><span class="ch">\\</span><span class="ss">begin</span><span class="ch">{{</span><span class="ss">pmatrix</span><span class="ch">}}</span><span class="ss"> </span><span class="sc">{</span>a<span class="sc">}</span><span class="ss"> </span><span class="ch">\\\\</span><span class="ss"> </span><span class="sc">{</span>b<span class="sc">}</span><span class="ss"> </span><span class="ch">\\\\</span><span class="ss"> </span><span class="ch">\\</span><span class="ss">end</span><span class="ch">{{</span><span class="ss">pmatrix</span><span class="ch">}}</span><span class="ss">;$ se puede observar que son similares al parámetro con que se simularon los datos."</span>)</span>
<span id="cb39-155"><a href="#cb39-155" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb39-156"><a href="#cb39-156" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-157"><a href="#cb39-157" aria-hidden="true" tabindex="-1"></a>Siguiendo con la inercia de presentar el error estándar de cada estimación, en las siguientes instrucciones se presenta el error estándar de $\hat \Sigma$, el cual se calcula utilizando la técnica de bootstrap (ver @sec-bootstrap) implementada en el siguiente código. </span>
<span id="cb39-158"><a href="#cb39-158" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-161"><a href="#cb39-161" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb39-162"><a href="#cb39-162" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: true</span></span>
<span id="cb39-163"><a href="#cb39-163" aria-hidden="true" tabindex="-1"></a>S <span class="op">=</span> np.random.randint(D.shape[<span class="dv">0</span>],</span>
<span id="cb39-164"><a href="#cb39-164" aria-hidden="true" tabindex="-1"></a>                      size<span class="op">=</span>(<span class="dv">500</span>, D.shape[<span class="dv">0</span>]))</span>
<span id="cb39-165"><a href="#cb39-165" aria-hidden="true" tabindex="-1"></a>B <span class="op">=</span> [np.cov(D[s], rowvar<span class="op">=</span><span class="va">False</span>) <span class="cf">for</span> s <span class="kw">in</span> S]</span>
<span id="cb39-166"><a href="#cb39-166" aria-hidden="true" tabindex="-1"></a>se <span class="op">=</span> np.std(B, axis<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb39-167"><a href="#cb39-167" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb39-168"><a href="#cb39-168" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-171"><a href="#cb39-171" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb39-172"><a href="#cb39-172" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: false</span></span>
<span id="cb39-173"><a href="#cb39-173" aria-hidden="true" tabindex="-1"></a>a, b <span class="op">=</span> [<span class="st">'&amp;'</span>.join([<span class="ss">f'</span><span class="sc">{</span>v<span class="sc">:0.4f}</span><span class="ss">'</span> <span class="cf">for</span> v <span class="kw">in</span> col]) <span class="cf">for</span> col <span class="kw">in</span> se]</span>
<span id="cb39-174"><a href="#cb39-174" aria-hidden="true" tabindex="-1"></a>Markdown(<span class="ss">f"Se puede observar que la función `np.cov` se ejecuta utilizando la muestra indicada en la variable `s`. El error estándar (`se`) de $</span><span class="ch">\\</span><span class="ss">hat </span><span class="ch">\\</span><span class="ss">Sigma$ corresponde a $</span><span class="ch">\\</span><span class="ss">begin</span><span class="ch">{{</span><span class="ss">pmatrix</span><span class="ch">}}</span><span class="ss"> </span><span class="sc">{</span>a<span class="sc">}</span><span class="ss"> </span><span class="ch">\\\\</span><span class="ss"> </span><span class="sc">{</span>b<span class="sc">}</span><span class="ss"> </span><span class="ch">\\\\</span><span class="ss"> </span><span class="ch">\\</span><span class="ss">end</span><span class="ch">{{</span><span class="ss">pmatrix</span><span class="ch">}}</span><span class="ss">.$ Se observa que los elementos fuera de la diagonal tienen un error estándar tal que el cero se encuentra en el intervalo $</span><span class="ch">\\</span><span class="ss">hat </span><span class="ch">\\</span><span class="ss">Sigma </span><span class="ch">\\</span><span class="ss">pm se;$ lo cual indica que el cero es un valor factible. Lo anterior se puede verificar tomando en cuenta que se conoce $</span><span class="ch">\\</span><span class="ss">Sigma$ y que el parámetro real es $0$ para aquellos elementos fuera de la diagonal."</span>)</span>
<span id="cb39-175"><a href="#cb39-175" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb39-176"><a href="#cb39-176" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-177"><a href="#cb39-177" aria-hidden="true" tabindex="-1"></a><span class="fu">## Metodología de Clasificación</span></span>
<span id="cb39-178"><a href="#cb39-178" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-179"><a href="#cb39-179" aria-hidden="true" tabindex="-1"></a>Habiendo descrito el proceso para estimar los parámetros de una distribución, por un lado se presentó de manera teórica con la distribución Bernoulli (ver @sec-distribucción-de-bernoulli) y de manera práctica con una distribución Gausiana (ver @sec-estimacion-distribucion-gausiana), se está en la posición de usar todos estos elementos para presentar el proceso completo de clasificación. La metodología general de aprendizaje supervisado (ver @sec-metodologia-general) está definida por cinco pasos, estos pasos se especializan para el problema de clasificación y regresión, utilizando modelos paramétricos, de la siguiente manera. </span>
<span id="cb39-180"><a href="#cb39-180" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-181"><a href="#cb39-181" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>Todo empieza con un conjunto de datos $\mathcal D$ que tiene la información del fenómeno de interés.</span>
<span id="cb39-182"><a href="#cb39-182" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>Se selecciona el conjunto $\mathcal T \subset \mathcal D,$ el procedimiento se describe en la @sec-conjunto-entre-prueba. </span>
<span id="cb39-183"><a href="#cb39-183" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>Se diseña un algoritmo, $f$, el cual se basa en un modelo (ver @sec-model-clasificacion) y la estimación de sus parámetros (ver @sec-estimacion-parametros) utilizando $\mathcal T.$</span>
<span id="cb39-184"><a href="#cb39-184" aria-hidden="true" tabindex="-1"></a><span class="ss">4. </span>En la @sec-prediccion se describe el uso de $f$ para predecir.</span>
<span id="cb39-185"><a href="#cb39-185" aria-hidden="true" tabindex="-1"></a><span class="ss">5. </span>La @sec-rendimiento muestra el procedimiento para medir el rendimiento utilizando un conjunto de prueba (ver @sec-conjunto-entre-prueba).</span>
<span id="cb39-186"><a href="#cb39-186" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-187"><a href="#cb39-187" aria-hidden="true" tabindex="-1"></a>La metodología de clasificación se ilustra utilizando el problema sintético (ver @sec-tres-normales) de tres clases que se presentó en el @sec-teoria-decision-bayesianas. Específicamente las entradas que definían a cada clase estaban en la variables <span class="in">`X_1`</span>, <span class="in">`X_2`</span> y <span class="in">`X_3`</span>. Entonces las clases se pueden colocar en la variable <span class="in">`y`</span> tal como se indica a continuación. </span>
<span id="cb39-188"><a href="#cb39-188" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-191"><a href="#cb39-191" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb39-192"><a href="#cb39-192" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: false</span></span>
<span id="cb39-193"><a href="#cb39-193" aria-hidden="true" tabindex="-1"></a>p1 <span class="op">=</span> multivariate_normal(mean<span class="op">=</span>[<span class="dv">5</span>, <span class="dv">5</span>],</span>
<span id="cb39-194"><a href="#cb39-194" aria-hidden="true" tabindex="-1"></a>                         cov<span class="op">=</span>[[<span class="dv">4</span>, <span class="dv">0</span>], [<span class="dv">0</span>, <span class="dv">2</span>]])</span>
<span id="cb39-195"><a href="#cb39-195" aria-hidden="true" tabindex="-1"></a>p2 <span class="op">=</span> multivariate_normal(mean<span class="op">=</span>[<span class="fl">1.5</span>, <span class="op">-</span><span class="fl">1.5</span>],</span>
<span id="cb39-196"><a href="#cb39-196" aria-hidden="true" tabindex="-1"></a>                         cov<span class="op">=</span>[[<span class="dv">2</span>, <span class="dv">1</span>], [<span class="dv">1</span>, <span class="dv">3</span>]])</span>
<span id="cb39-197"><a href="#cb39-197" aria-hidden="true" tabindex="-1"></a>p3 <span class="op">=</span> multivariate_normal(mean<span class="op">=</span>[<span class="fl">12.5</span>, <span class="op">-</span><span class="fl">3.5</span>], </span>
<span id="cb39-198"><a href="#cb39-198" aria-hidden="true" tabindex="-1"></a>                         cov<span class="op">=</span>[[<span class="dv">2</span>, <span class="dv">3</span>], [<span class="dv">3</span>, <span class="dv">7</span>]])</span>
<span id="cb39-199"><a href="#cb39-199" aria-hidden="true" tabindex="-1"></a>X_1 <span class="op">=</span> p1.rvs(size<span class="op">=</span><span class="dv">1000</span>)</span>
<span id="cb39-200"><a href="#cb39-200" aria-hidden="true" tabindex="-1"></a>X_2 <span class="op">=</span> p2.rvs(size<span class="op">=</span><span class="dv">1000</span>)</span>
<span id="cb39-201"><a href="#cb39-201" aria-hidden="true" tabindex="-1"></a>X_3 <span class="op">=</span> p3.rvs(size<span class="op">=</span><span class="dv">1000</span>)                         </span>
<span id="cb39-202"><a href="#cb39-202" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb39-203"><a href="#cb39-203" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-206"><a href="#cb39-206" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb39-207"><a href="#cb39-207" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: true</span></span>
<span id="cb39-208"><a href="#cb39-208" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> np.concatenate((X_1, X_2, X_3))</span>
<span id="cb39-209"><a href="#cb39-209" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> np.array([<span class="dv">1</span>] <span class="op">*</span> <span class="dv">1000</span> <span class="op">+</span> [<span class="dv">2</span>] <span class="op">*</span> <span class="dv">1000</span> <span class="op">+</span> [<span class="dv">3</span>] <span class="op">*</span> <span class="dv">1000</span>)</span>
<span id="cb39-210"><a href="#cb39-210" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb39-211"><a href="#cb39-211" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-212"><a href="#cb39-212" aria-hidden="true" tabindex="-1"></a>Las variables <span class="in">`X`</span> y <span class="in">`y`</span> contiene la información del conjunto $\mathcal D = (\mathcal X, \mathcal Y)$ donde cada renglón de <span class="in">`X`</span> es una realización de la variable aleatoria $\mathcal X$ y equivalentemente cada elemento en <span class="in">`y`</span> es una realización de $\mathcal Y.$</span>
<span id="cb39-213"><a href="#cb39-213" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-214"><a href="#cb39-214" aria-hidden="true" tabindex="-1"></a><span class="fu">## Conjunto de Entrenamiento y Prueba {#sec-conjunto-entre-prueba}</span></span>
<span id="cb39-215"><a href="#cb39-215" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-216"><a href="#cb39-216" aria-hidden="true" tabindex="-1"></a>En la @sec-estimacion-distribucion-gausiana se había utilizado a $\mathcal D$ en el procedimiento de maximizar la verosimilitud, esto porque el objetivo en ese procedimiento era estimar los parámetros de la distribución. Pero el objetivo en aprendizaje supervisado es diseñar un algoritmo (función en este caso) que modele la relación entre $\mathcal X$ y $\mathcal Y$. Para conocer esto es necesario medir el rendimiento del algoritmo en instancias que no han sido vistas en el entrenamiento.</span>
<span id="cb39-217"><a href="#cb39-217" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-218"><a href="#cb39-218" aria-hidden="true" tabindex="-1"></a>En consecuencia, se requieren contar con datos para medir el rendimiento, a este conjunto de datos se le conoce como el conjunto de prueba, $\mathcal G$. $\mathcal G$ se crea a partir de $\mathcal D$ de tal manera que $\mathcal G \cap \mathcal T = \emptyset$ y $\mathcal D =  \mathcal G \cup \mathcal T.$ La siguiente instrucción se puede utilizar para dividir la generación de estos conjuntos a partir de $\mathcal D.$</span>
<span id="cb39-219"><a href="#cb39-219" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-222"><a href="#cb39-222" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb39-223"><a href="#cb39-223" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: true</span></span>
<span id="cb39-224"><a href="#cb39-224" aria-hidden="true" tabindex="-1"></a>T, G, y_t, y_g <span class="op">=</span> train_test_split(X, y, test_size<span class="op">=</span><span class="fl">0.2</span>)</span>
<span id="cb39-225"><a href="#cb39-225" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb39-226"><a href="#cb39-226" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-227"><a href="#cb39-227" aria-hidden="true" tabindex="-1"></a>El parámetro <span class="in">`test_size`</span> indica la proporción del tamaño de conjunto $\mathcal G$ en relación con el conjunto $\mathcal D.$</span>
<span id="cb39-228"><a href="#cb39-228" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-229"><a href="#cb39-229" aria-hidden="true" tabindex="-1"></a><span class="fu">### Modelo {#sec-model-clasificacion}</span></span>
<span id="cb39-230"><a href="#cb39-230" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-231"><a href="#cb39-231" aria-hidden="true" tabindex="-1"></a>El inicio de métodos paramétricos es el Teorema de Bayes (@eq-teorema-bayes) $\mathbb P(\mathcal Y \mid \mathcal X) = \frac{ \mathbb P(\mathcal X \mid \mathcal Y) \mathbb P(\mathcal Y)}{\mathbb P(\mathcal X)}$ donde se usa la verosimilitud $\mathbb P(\mathcal X \mid \mathcal Y)$ y el prior $\mathbb P(\mathcal Y)$ para definir la probabilidad a posteriori $\mathbb P(\mathcal Y \mid \mathcal X)$. En métodos paramétricos se asume que se puede modelar la verosimilitud con una distribución particular, que por lo generar es una distribución Gausiana multivariada. Es decir, la variable aleatoria $\mathcal X$ dado $\mathcal Y$ ($\mathcal X_{\mid \mathcal Y}$) es $\mathcal X_{\mid \mathcal Y} \sim \mathcal N(\mu_{\mathcal Y}, \Sigma_{\mathcal Y}),$ donde se observa que los parámetros de la distribución Gausiana dependen de la variable aleatoria $\mathcal Y$ y estos pueden ser identificados cuando $\mathcal Y$ tiene un valor específico. </span>
<span id="cb39-232"><a href="#cb39-232" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-233"><a href="#cb39-233" aria-hidden="true" tabindex="-1"></a><span class="fu">### Estimación de Parámetros {#sec-estimacion-parametros}</span></span>
<span id="cb39-234"><a href="#cb39-234" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-235"><a href="#cb39-235" aria-hidden="true" tabindex="-1"></a>Dado que por definición del problema (ver @sec-tres-normales) se conoce que la verosimilitud para cada clase proviene de una Gausiana, i.e., $\mathcal X_{\mid \mathcal Y} \sim \mathcal N(\mu_{\mathcal Y}, \Sigma_{\mathcal Y}),$ en esta sección se estimarán los parámetros utilizando este conocimiento. </span>
<span id="cb39-236"><a href="#cb39-236" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-237"><a href="#cb39-237" aria-hidden="true" tabindex="-1"></a>El primer paso en la estimación de parámetros es calcular el prior $\mathbb P(\mathcal Y)$, el cual corresponde a clasificar el evento sin observar el valor de $\mathcal X.$ Esto se puede modelar mediante una distribución Categórica con parámetros $p_i$ donde $\sum_i^K p_i = 1$. Estos parámetros se pueden estimar utilizando la función <span class="in">`np.unique`</span> de la siguiente manera</span>
<span id="cb39-238"><a href="#cb39-238" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-241"><a href="#cb39-241" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb39-242"><a href="#cb39-242" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: true</span></span>
<span id="cb39-243"><a href="#cb39-243" aria-hidden="true" tabindex="-1"></a>labels, counts <span class="op">=</span> np.unique(y_t, return_counts<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb39-244"><a href="#cb39-244" aria-hidden="true" tabindex="-1"></a>prior <span class="op">=</span> counts <span class="op">/</span> counts.<span class="bu">sum</span>()</span>
<span id="cb39-245"><a href="#cb39-245" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb39-246"><a href="#cb39-246" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-247"><a href="#cb39-247" aria-hidden="true" tabindex="-1"></a>La variable <span class="in">`prior`</span> contiene en el primer elemento $\mathbb P(\mathcal Y=1) = 0.3292,$ en el segundo $\mathbb P(\mathcal Y=2) = 0.3412$ y en el tercero $\mathbb P(\mathcal Y=3) = 0.3296$ que es aproximadamente $\frac{1}{3}$ el cual es el valor real del prior. </span>
<span id="cb39-248"><a href="#cb39-248" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-249"><a href="#cb39-249" aria-hidden="true" tabindex="-1"></a>Siguiendo los pasos en estimación de parámetros de una Gausiana(@sec-estimacion-distribucion-gausiana) se pueden estimar los parámetros para cada Gausiana dada la clase. Es decir, se tiene que estimar los parámetros $\mu$ y $\Sigma$ para la clase $1$, $2$ y $3$. Esto se puede realizar iterando por las etiquetas contenidas en la variable <span class="in">`labels`</span> y seleccionando los datos en <span class="in">`T`</span> que corresponden a la clase analizada, ver el uso de la variable <span class="in">`mask`</span> en el slice de la línea 4 y 5. Después se inicializa una instancia de la clase <span class="in">`multivariate_normal`</span> para ser utilizada en el cómputo de la función de densidad de probabilidad. El paso final es guardar las instancias de las distribuciones en la lista <span class="in">`likelihood`</span>.</span>
<span id="cb39-250"><a href="#cb39-250" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-251"><a href="#cb39-251" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-254"><a href="#cb39-254" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb39-255"><a href="#cb39-255" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: true</span></span>
<span id="cb39-256"><a href="#cb39-256" aria-hidden="true" tabindex="-1"></a>likelihood <span class="op">=</span> []</span>
<span id="cb39-257"><a href="#cb39-257" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> k <span class="kw">in</span> labels:</span>
<span id="cb39-258"><a href="#cb39-258" aria-hidden="true" tabindex="-1"></a>    mask <span class="op">=</span> y_t <span class="op">==</span> k</span>
<span id="cb39-259"><a href="#cb39-259" aria-hidden="true" tabindex="-1"></a>    mu <span class="op">=</span> np.mean(T[mask], axis<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb39-260"><a href="#cb39-260" aria-hidden="true" tabindex="-1"></a>    cov <span class="op">=</span> np.cov(T[mask], rowvar<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb39-261"><a href="#cb39-261" aria-hidden="true" tabindex="-1"></a>    likelihood_k <span class="op">=</span> multivariate_normal(mean<span class="op">=</span>mu, cov<span class="op">=</span>cov)</span>
<span id="cb39-262"><a href="#cb39-262" aria-hidden="true" tabindex="-1"></a>    likelihood.append(likelihood_k)</span>
<span id="cb39-263"><a href="#cb39-263" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb39-264"><a href="#cb39-264" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-267"><a href="#cb39-267" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb39-268"><a href="#cb39-268" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: false</span></span>
<span id="cb39-269"><a href="#cb39-269" aria-hidden="true" tabindex="-1"></a>mus <span class="op">=</span> [<span class="st">', '</span>.join([<span class="ss">f'</span><span class="sc">{</span>v<span class="sc">:0.4f}</span><span class="ss">'</span> <span class="cf">for</span> v <span class="kw">in</span> li.mean]) <span class="cf">for</span> li <span class="kw">in</span> likelihood]</span>
<span id="cb39-270"><a href="#cb39-270" aria-hidden="true" tabindex="-1"></a>mm <span class="op">=</span> []</span>
<span id="cb39-271"><a href="#cb39-271" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> cov <span class="kw">in</span> likelihood:</span>
<span id="cb39-272"><a href="#cb39-272" aria-hidden="true" tabindex="-1"></a>    _ <span class="op">=</span> [<span class="st">' &amp; '</span>.join([<span class="ss">f'</span><span class="sc">{</span>v<span class="sc">:0.4f}</span><span class="ss">'</span> <span class="cf">for</span> v <span class="kw">in</span> reg]) <span class="cf">for</span> reg <span class="kw">in</span> cov.cov]</span>
<span id="cb39-273"><a href="#cb39-273" aria-hidden="true" tabindex="-1"></a>    _ <span class="op">=</span> <span class="st">'</span><span class="ch">\\\\</span><span class="st">'</span>.join(_)</span>
<span id="cb39-274"><a href="#cb39-274" aria-hidden="true" tabindex="-1"></a>    mm.append(_)</span>
<span id="cb39-275"><a href="#cb39-275" aria-hidden="true" tabindex="-1"></a>Markdown(<span class="ss">f"Los valores estimados para la media, en cada clase son: $</span><span class="ch">\\</span><span class="ss">hat </span><span class="ch">\\</span><span class="ss">mu_1 = [</span><span class="sc">{</span>mus[<span class="dv">0</span>]<span class="sc">}</span><span class="ss">]^</span><span class="ch">\\</span><span class="ss">intercal,$ $</span><span class="ch">\\</span><span class="ss">hat </span><span class="ch">\\</span><span class="ss">mu_2 = [</span><span class="sc">{</span>mus[<span class="dv">1</span>]<span class="sc">}</span><span class="ss">]^</span><span class="ch">\\</span><span class="ss">intercal$ y $</span><span class="ch">\\</span><span class="ss">hat </span><span class="ch">\\</span><span class="ss">mu_3 = [</span><span class="sc">{</span>mus[<span class="dv">2</span>]<span class="sc">}</span><span class="ss">]^</span><span class="ch">\\</span><span class="ss">intercal$. Para las matrices de covarianza, los valores estimados corresponden a $</span><span class="ch">\\</span><span class="ss">hat </span><span class="ch">\\</span><span class="ss">Sigma_1 = </span><span class="ch">\\</span><span class="ss">begin</span><span class="ch">{{</span><span class="ss">pmatrix</span><span class="ch">}}</span><span class="ss"> </span><span class="sc">{</span>mm[<span class="dv">0</span>]<span class="sc">}</span><span class="ss"> </span><span class="ch">\\\\</span><span class="ss"> </span><span class="ch">\\</span><span class="ss">end</span><span class="ch">{{</span><span class="ss">pmatrix</span><span class="ch">}}</span><span class="ss">,$ $</span><span class="ch">\\</span><span class="ss">hat </span><span class="ch">\\</span><span class="ss">Sigma_2 = </span><span class="ch">\\</span><span class="ss">begin</span><span class="ch">{{</span><span class="ss">pmatrix</span><span class="ch">}}</span><span class="ss"> </span><span class="sc">{</span>mm[<span class="dv">1</span>]<span class="sc">}</span><span class="ss"> </span><span class="ch">\\\\</span><span class="ss"> </span><span class="ch">\\</span><span class="ss">end</span><span class="ch">{{</span><span class="ss">pmatrix</span><span class="ch">}}</span><span class="ss">$ y $</span><span class="ch">\\</span><span class="ss">hat </span><span class="ch">\\</span><span class="ss">Sigma_3 = </span><span class="ch">\\</span><span class="ss">begin</span><span class="ch">{{</span><span class="ss">pmatrix</span><span class="ch">}}</span><span class="ss"> </span><span class="sc">{</span>mm[<span class="dv">2</span>]<span class="sc">}</span><span class="ss"> </span><span class="ch">\\\\</span><span class="ss"> </span><span class="ch">\\</span><span class="ss">end</span><span class="ch">{{</span><span class="ss">pmatrix</span><span class="ch">}}</span><span class="ss">.$"</span>)</span>
<span id="cb39-276"><a href="#cb39-276" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb39-277"><a href="#cb39-277" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-278"><a href="#cb39-278" aria-hidden="true" tabindex="-1"></a>Estas estimaciones se pueden comparar con los parámetros reales (@sec-tres-normales). También se puede calcular su error estándar para identificar si el parámetro real, $\theta$, se encuentra en el intervalo definido por $\hat \theta - 2\hat{se} \leq \hat \theta \leq \hat \theta + 2 \hat{se}$ que corresponde aproximadamente al 95% de confianza asumiendo que la distribución de la estimación del parámetro es Gausiana.</span>
<span id="cb39-279"><a href="#cb39-279" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-280"><a href="#cb39-280" aria-hidden="true" tabindex="-1"></a><span class="fu">### Predicción {#sec-prediccion}</span></span>
<span id="cb39-281"><a href="#cb39-281" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-282"><a href="#cb39-282" aria-hidden="true" tabindex="-1"></a>Una vez que se tiene la función que modela los datos, se está en condiciones de utilizarla para predecir(ver @sec-prediccion-normal) nuevos datos. </span>
<span id="cb39-283"><a href="#cb39-283" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-284"><a href="#cb39-284" aria-hidden="true" tabindex="-1"></a>En esta ocasión se organiza el procedimiento de predicción en diferentes funciones, la primera función recibe los datos a predecir <span class="in">`X`</span> y los componentes del modelo, que son la verosimilitud (<span class="in">`likelihood`</span>) y el <span class="in">`prior`</span>. La función calcula $\mathbb P(\mathcal Y=y \mid \mathcal X=x)$ que es la probabilidad de cada clase dada la entrada $x$. Se puede observar en la primera línea que se usa la función de densidad de probabilidad (<span class="in">`pdf`</span>) para cada clase y esta se multiplica por el <span class="in">`prior`</span> y en la tercera línea se calcula la evidencia. Finalmente, se regresa el a posteriori.  </span>
<span id="cb39-285"><a href="#cb39-285" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-288"><a href="#cb39-288" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb39-289"><a href="#cb39-289" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: true</span></span>
<span id="cb39-290"><a href="#cb39-290" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> predict_prob(X, likelihood, prior):</span>
<span id="cb39-291"><a href="#cb39-291" aria-hidden="true" tabindex="-1"></a>    likelihood <span class="op">=</span> [m.pdf(X) <span class="cf">for</span> m <span class="kw">in</span> likelihood]</span>
<span id="cb39-292"><a href="#cb39-292" aria-hidden="true" tabindex="-1"></a>    posterior <span class="op">=</span> np.vstack(likelihood).T <span class="op">*</span> prior</span>
<span id="cb39-293"><a href="#cb39-293" aria-hidden="true" tabindex="-1"></a>    evidence <span class="op">=</span> posterior.<span class="bu">sum</span>(axis<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb39-294"><a href="#cb39-294" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> posterior <span class="op">/</span> np.atleast_2d(evidence).T</span>
<span id="cb39-295"><a href="#cb39-295" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb39-296"><a href="#cb39-296" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-297"><a href="#cb39-297" aria-hidden="true" tabindex="-1"></a>La función <span class="in">`predict_proba`</span> se utiliza como base para predecir la clase, para la cual se requiere el mapa entre índices y clases que se encuentra en la variable <span class="in">`labels`</span>. Se observa que se llama a la función <span class="in">`predict_proba`</span> y después se calcula el argumento que tiene la máxima probabilidad regresando la etiqueta asociada. </span>
<span id="cb39-298"><a href="#cb39-298" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-301"><a href="#cb39-301" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb39-302"><a href="#cb39-302" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: true</span></span>
<span id="cb39-303"><a href="#cb39-303" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> predict(X, likelihood, prior, labels):</span>
<span id="cb39-304"><a href="#cb39-304" aria-hidden="true" tabindex="-1"></a>    _ <span class="op">=</span> predict_prob(X, likelihood, prior)</span>
<span id="cb39-305"><a href="#cb39-305" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> labels[np.argmax(_, axis<span class="op">=</span><span class="dv">1</span>)]</span>
<span id="cb39-306"><a href="#cb39-306" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb39-307"><a href="#cb39-307" aria-hidden="true" tabindex="-1"></a><span class="fu">### Rendimiento {#sec-rendimiento}</span></span>
<span id="cb39-308"><a href="#cb39-308" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-309"><a href="#cb39-309" aria-hidden="true" tabindex="-1"></a>El rendimiento del algoritmo se mide en el conjunto de prueba <span class="in">`G`</span>, utilizando como medida el error de clasificación (@sec-error-clasificacion). El primer paso es predecir las clases de los elementos en <span class="in">`G`</span>, utilizando la función <span class="in">`predict`</span> que fue diseñada anteriormente. Después se mide el error, con la instrucción de la segunda línea.  </span>
<span id="cb39-310"><a href="#cb39-310" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-313"><a href="#cb39-313" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb39-314"><a href="#cb39-314" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: true</span></span>
<span id="cb39-315"><a href="#cb39-315" aria-hidden="true" tabindex="-1"></a>hy <span class="op">=</span> predict(G, likelihood, prior, labels)</span>
<span id="cb39-316"><a href="#cb39-316" aria-hidden="true" tabindex="-1"></a>error <span class="op">=</span> (y_g <span class="op">!=</span> hy).mean()</span>
<span id="cb39-317"><a href="#cb39-317" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb39-318"><a href="#cb39-318" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-321"><a href="#cb39-321" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb39-322"><a href="#cb39-322" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: false</span></span>
<span id="cb39-323"><a href="#cb39-323" aria-hidden="true" tabindex="-1"></a>Markdown(<span class="ss">f"El error que tiene el algoritmo en el conjunto de prueba es $</span><span class="sc">{</span>error<span class="sc">:0.2f}</span><span class="ss">,$ el cual es ligeramente superior al encontrado con el modelo ideal."</span>)</span>
<span id="cb39-324"><a href="#cb39-324" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb39-325"><a href="#cb39-325" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-328"><a href="#cb39-328" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb39-329"><a href="#cb39-329" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: false</span></span>
<span id="cb39-330"><a href="#cb39-330" aria-hidden="true" tabindex="-1"></a>Markdown(<span class="ss">f"El error estándar se calcula con la siguiente instrucción el cual tiene un valor de $</span><span class="sc">{</span>np<span class="sc">.</span>sqrt(error <span class="op">*</span> (<span class="dv">1</span> <span class="op">-</span> error) <span class="op">/</span> y_g.shape[<span class="dv">0</span>])<span class="sc">:0.4f}</span><span class="ss">.$"</span>)</span>
<span id="cb39-331"><a href="#cb39-331" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb39-332"><a href="#cb39-332" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-335"><a href="#cb39-335" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb39-336"><a href="#cb39-336" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: true</span></span>
<span id="cb39-337"><a href="#cb39-337" aria-hidden="true" tabindex="-1"></a>se_formula <span class="op">=</span> np.sqrt(error <span class="op">*</span> (<span class="dv">1</span> <span class="op">-</span> error) <span class="op">/</span> y_g.shape[<span class="dv">0</span>])</span>
<span id="cb39-338"><a href="#cb39-338" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb39-339"><a href="#cb39-339" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-340"><a href="#cb39-340" aria-hidden="true" tabindex="-1"></a><span class="fu">## Clasificador Bayesiano Ingenuo</span></span>
<span id="cb39-341"><a href="#cb39-341" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-342"><a href="#cb39-342" aria-hidden="true" tabindex="-1"></a>Uno de los clasificadores mas utilizados, sencillo de implementar y competitivo, es el clasificador Bayesiano Ingenuo. En la @sec-model-clasificacion se asumió que la variable aleatoria $\mathcal X = (\mathcal X_1, \mathcal X_2, \ldots, \mathcal X_d)$ dado $\mathcal Y$ ($\mathcal X_{\mid \mathcal Y}$) es $\mathcal X_{\mid \mathcal Y} \sim \mathcal N(\mu_{\mathcal Y}, \Sigma_{\mathcal Y}),$ donde $\mu_{\mathcal Y} \in \mathbb R^d$, $\Sigma_{\mathcal Y} \in \mathbb R^{d \times d}$ y $f(\mathcal X_1, \mathcal X_2, \ldots, \mathcal X_d)$ es la función de densidad de probabilidad conjunta.</span>
<span id="cb39-343"><a href="#cb39-343" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-344"><a href="#cb39-344" aria-hidden="true" tabindex="-1"></a>En el clasificador Bayesiano Ingenuo se asume que las variables $\mathcal X_i$ y $\mathcal X_j$ para $i \neq j$ son independientes, esto trae como consecuencia que $f(\mathcal X_1, \mathcal X_2, \ldots, \mathcal X_d) = \prod_i^d f(\mathcal X_i).$ Esto quiere decir que cada variable está definida como una Gausina donde se tiene que identificar $\mu$ y $\sigma^2.$</span>
<span id="cb39-345"><a href="#cb39-345" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-346"><a href="#cb39-346" aria-hidden="true" tabindex="-1"></a>La estimación de los parámetros de estas distribuciones se puede realizar utilizando un código similar siendo la única diferencia que en se calcula $\sigma^2$ de cada variable en lugar de la covarianza $\Sigma$, esto se puede observar en la quinta línea donde se usa la función <span class="in">`np.var`</span> en el primer eje. El resto del código es equivalente al usado en la @sec-estimacion-parametros.</span>
<span id="cb39-347"><a href="#cb39-347" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-350"><a href="#cb39-350" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb39-351"><a href="#cb39-351" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: true</span></span>
<span id="cb39-352"><a href="#cb39-352" aria-hidden="true" tabindex="-1"></a>likelihood <span class="op">=</span> []</span>
<span id="cb39-353"><a href="#cb39-353" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> k <span class="kw">in</span> labels:</span>
<span id="cb39-354"><a href="#cb39-354" aria-hidden="true" tabindex="-1"></a>    mask <span class="op">=</span> y_t <span class="op">==</span> k</span>
<span id="cb39-355"><a href="#cb39-355" aria-hidden="true" tabindex="-1"></a>    mu <span class="op">=</span> np.mean(T[mask], axis<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb39-356"><a href="#cb39-356" aria-hidden="true" tabindex="-1"></a>    var <span class="op">=</span> np.var(T[mask], axis<span class="op">=</span><span class="dv">0</span>, ddof<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb39-357"><a href="#cb39-357" aria-hidden="true" tabindex="-1"></a>    likelihood_k <span class="op">=</span> multivariate_normal(mean<span class="op">=</span>mu, cov<span class="op">=</span>var)</span>
<span id="cb39-358"><a href="#cb39-358" aria-hidden="true" tabindex="-1"></a>    likelihood.append(likelihood_k)</span>
<span id="cb39-359"><a href="#cb39-359" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb39-360"><a href="#cb39-360" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-361"><a href="#cb39-361" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-364"><a href="#cb39-364" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb39-365"><a href="#cb39-365" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: false</span></span>
<span id="cb39-366"><a href="#cb39-366" aria-hidden="true" tabindex="-1"></a>mus <span class="op">=</span> [<span class="st">', '</span>.join([<span class="ss">f'</span><span class="sc">{</span>v<span class="sc">:0.4f}</span><span class="ss">'</span> <span class="cf">for</span> v <span class="kw">in</span> li.mean]) <span class="cf">for</span> li <span class="kw">in</span> likelihood]</span>
<span id="cb39-367"><a href="#cb39-367" aria-hidden="true" tabindex="-1"></a>mm <span class="op">=</span> []</span>
<span id="cb39-368"><a href="#cb39-368" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> cov <span class="kw">in</span> likelihood:</span>
<span id="cb39-369"><a href="#cb39-369" aria-hidden="true" tabindex="-1"></a>    _ <span class="op">=</span> [<span class="st">' &amp; '</span>.join([<span class="ss">f'</span><span class="sc">{</span>v<span class="sc">:0.4f}</span><span class="ss">'</span> <span class="cf">for</span> v <span class="kw">in</span> reg]) <span class="cf">for</span> reg <span class="kw">in</span> cov.cov]</span>
<span id="cb39-370"><a href="#cb39-370" aria-hidden="true" tabindex="-1"></a>    _ <span class="op">=</span> <span class="st">'</span><span class="ch">\\\\</span><span class="st">'</span>.join(_)</span>
<span id="cb39-371"><a href="#cb39-371" aria-hidden="true" tabindex="-1"></a>    mm.append(_)</span>
<span id="cb39-372"><a href="#cb39-372" aria-hidden="true" tabindex="-1"></a>Markdown(<span class="ss">f"Los parámetros estimados en la versión ingenua son equivalentes con respecto a las medias, i.e., $</span><span class="ch">\\</span><span class="ss">hat </span><span class="ch">\\</span><span class="ss">mu_1 = [</span><span class="sc">{</span>mus[<span class="dv">0</span>]<span class="sc">}</span><span class="ss">]^</span><span class="ch">\\</span><span class="ss">intercal$, $</span><span class="ch">\\</span><span class="ss">hat </span><span class="ch">\\</span><span class="ss">mu_2 = [</span><span class="sc">{</span>mus[<span class="dv">1</span>]<span class="sc">}</span><span class="ss">] ^</span><span class="ch">\\</span><span class="ss">intercal$ y $</span><span class="ch">\\</span><span class="ss">hat </span><span class="ch">\\</span><span class="ss">mu_3 = [</span><span class="sc">{</span>mus[<span class="dv">2</span>]<span class="sc">}</span><span class="ss">]^</span><span class="ch">\\</span><span class="ss">intercal$. La diferencia se puede observar en las varianzas, que a continuación se muestran como matrices de covarianza para resaltar la diferencia, i.e., $</span><span class="ch">\\</span><span class="ss">hat </span><span class="ch">\\</span><span class="ss">Sigma_1 = </span><span class="ch">\\</span><span class="ss">begin</span><span class="ch">{{</span><span class="ss">pmatrix</span><span class="ch">}}</span><span class="ss"> </span><span class="sc">{</span>mm[<span class="dv">0</span>]<span class="sc">}</span><span class="ss"> </span><span class="ch">\\\\</span><span class="ss"> </span><span class="ch">\\</span><span class="ss">end</span><span class="ch">{{</span><span class="ss">pmatrix</span><span class="ch">}}</span><span class="ss">$, $</span><span class="ch">\\</span><span class="ss">hat </span><span class="ch">\\</span><span class="ss">Sigma_2 = </span><span class="ch">\\</span><span class="ss">begin</span><span class="ch">{{</span><span class="ss">pmatrix</span><span class="ch">}}</span><span class="ss"> </span><span class="sc">{</span>mm[<span class="dv">1</span>]<span class="sc">}</span><span class="ss"> </span><span class="ch">\\\\</span><span class="ss"> </span><span class="ch">\\</span><span class="ss">end</span><span class="ch">{{</span><span class="ss">pmatrix</span><span class="ch">}}</span><span class="ss">$ y $</span><span class="ch">\\</span><span class="ss">hat </span><span class="ch">\\</span><span class="ss">Sigma_3 = </span><span class="ch">\\</span><span class="ss">begin</span><span class="ch">{{</span><span class="ss">pmatrix</span><span class="ch">}}</span><span class="ss"> </span><span class="sc">{</span>mm[<span class="dv">2</span>]<span class="sc">}</span><span class="ss"> </span><span class="ch">\\\\</span><span class="ss"> \end</span><span class="ch">{{</span><span class="ss">pmatrix</span><span class="ch">}}</span><span class="ss">$ se observa como los elementos fuera de la diagonal son ceros, lo cual indica la independencia entra las variables de entrada."</span>)</span>
<span id="cb39-373"><a href="#cb39-373" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb39-374"><a href="#cb39-374" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-375"><a href="#cb39-375" aria-hidden="true" tabindex="-1"></a>Finalmente, el código para predecir se utiliza el código descrito en la @sec-prediccion dado que el modelo está dado en las variables <span class="in">`likelihood`</span> y <span class="in">`prior`</span>. </span>
<span id="cb39-376"><a href="#cb39-376" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-379"><a href="#cb39-379" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb39-380"><a href="#cb39-380" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: false</span></span>
<span id="cb39-381"><a href="#cb39-381" aria-hidden="true" tabindex="-1"></a>hy_ingenuo <span class="op">=</span> predict(G, likelihood, prior, labels)</span>
<span id="cb39-382"><a href="#cb39-382" aria-hidden="true" tabindex="-1"></a>error <span class="op">=</span> (y_g <span class="op">!=</span> hy_ingenuo).mean()</span>
<span id="cb39-383"><a href="#cb39-383" aria-hidden="true" tabindex="-1"></a>Markdown(<span class="ss">f"El `error` del clasificador Bayesiano Ingenuo, en el conjunto de prueba, es de $</span><span class="sc">{</span>(y_g <span class="op">!=</span> hy_ingenuo)<span class="sc">.</span>mean()<span class="sc">:0.2f}</span><span class="ss">$ y su error estándar (`se_formula`) es $</span><span class="sc">{</span>np<span class="sc">.</span>sqrt(error <span class="op">*</span> (<span class="dv">1</span> <span class="op">-</span> error) <span class="op">/</span> y_g.shape[<span class="dv">0</span>])<span class="sc">:0.4f}</span><span class="ss">.$"</span>)</span>
<span id="cb39-384"><a href="#cb39-384" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb39-385"><a href="#cb39-385" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-386"><a href="#cb39-386" aria-hidden="true" tabindex="-1"></a><span class="fu">## Ejemplo: Breast Cancer Wisconsin {#sec-ejemplo-breast-cancer-wisconsin}</span></span>
<span id="cb39-387"><a href="#cb39-387" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-388"><a href="#cb39-388" aria-hidden="true" tabindex="-1"></a>Esta sección ilustra el uso del clasificador Bayesiano al generar dos modelos (Clasificador Bayesiano y Bayesiano Ingenuo) del conjunto de datos de *Breast Cancer Wisconsin.* Estos datos se pueden obtener utilizando la función <span class="in">`load_breast_cancer`</span> tal y como se muestra a continuación.</span>
<span id="cb39-389"><a href="#cb39-389" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-392"><a href="#cb39-392" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb39-393"><a href="#cb39-393" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: true</span></span>
<span id="cb39-394"><a href="#cb39-394" aria-hidden="true" tabindex="-1"></a>X, y <span class="op">=</span> load_breast_cancer(return_X_y<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb39-395"><a href="#cb39-395" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb39-396"><a href="#cb39-396" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-397"><a href="#cb39-397" aria-hidden="true" tabindex="-1"></a>El primer paso es contar con los conjuntos de <span class="co">[</span><span class="ot">entrenamiento y prueba</span><span class="co">](/AprendizajeComputacional/capitulos/03Parametricos/#conjunto-de-entrenamiento-y-prueba)</span> para poder realizar de manera completa la evaluación del proceso de clasificación. Esto se realiza ejecutando la siguiente instrucción.</span>
<span id="cb39-398"><a href="#cb39-398" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-401"><a href="#cb39-401" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb39-402"><a href="#cb39-402" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: true</span></span>
<span id="cb39-403"><a href="#cb39-403" aria-hidden="true" tabindex="-1"></a>T, G, y_t, y_g <span class="op">=</span> train_test_split(X, y, test_size<span class="op">=</span><span class="fl">0.2</span>)</span>
<span id="cb39-404"><a href="#cb39-404" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb39-405"><a href="#cb39-405" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-406"><a href="#cb39-406" aria-hidden="true" tabindex="-1"></a><span class="fu">### Entrenamiento</span></span>
<span id="cb39-407"><a href="#cb39-407" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-408"><a href="#cb39-408" aria-hidden="true" tabindex="-1"></a>Los dos modelos que se utilizarán será el clasificador Bayesiano Gausiano y Bayesiano Ingenuo, utilizando la clase <span class="in">`GaussianBayes`</span> que se explica en el @sec-clasificador-bayesiano-gausiano. Las siguientes dos instrucciones inicializan estos dos clasificadores, la única diferencia es el parámetro <span class="in">`naive`</span> que indica si el clasificador es ingenuo. </span>
<span id="cb39-409"><a href="#cb39-409" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-412"><a href="#cb39-412" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb39-413"><a href="#cb39-413" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: true</span></span>
<span id="cb39-414"><a href="#cb39-414" aria-hidden="true" tabindex="-1"></a>gaussian <span class="op">=</span> GaussianBayes().fit(T, y_t)</span>
<span id="cb39-415"><a href="#cb39-415" aria-hidden="true" tabindex="-1"></a>naive <span class="op">=</span> GaussianBayes(naive<span class="op">=</span><span class="va">True</span>).fit(T, y_t)</span>
<span id="cb39-416"><a href="#cb39-416" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb39-417"><a href="#cb39-417" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-418"><a href="#cb39-418" aria-hidden="true" tabindex="-1"></a><span class="fu">### Predicción</span></span>
<span id="cb39-419"><a href="#cb39-419" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-420"><a href="#cb39-420" aria-hidden="true" tabindex="-1"></a>Habiendo definido los dos clasificadores, las predicciones del conjunto de prueba se realiza de la siguiente manera. </span>
<span id="cb39-421"><a href="#cb39-421" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-424"><a href="#cb39-424" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb39-425"><a href="#cb39-425" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: true</span></span>
<span id="cb39-426"><a href="#cb39-426" aria-hidden="true" tabindex="-1"></a>hy_gaussian <span class="op">=</span> gaussian.predict(G)</span>
<span id="cb39-427"><a href="#cb39-427" aria-hidden="true" tabindex="-1"></a>hy_naive <span class="op">=</span> naive.predict(G)</span>
<span id="cb39-428"><a href="#cb39-428" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb39-429"><a href="#cb39-429" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-430"><a href="#cb39-430" aria-hidden="true" tabindex="-1"></a><span class="fu">### Rendimiento {#sec-gaussina-perf-breast_cancer}</span></span>
<span id="cb39-431"><a href="#cb39-431" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-432"><a href="#cb39-432" aria-hidden="true" tabindex="-1"></a>El rendimiento de ambos clasificadores se calcula de la siguiente manera </span>
<span id="cb39-433"><a href="#cb39-433" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-436"><a href="#cb39-436" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb39-437"><a href="#cb39-437" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: true</span></span>
<span id="cb39-438"><a href="#cb39-438" aria-hidden="true" tabindex="-1"></a>error_gaussian <span class="op">=</span> (y_g <span class="op">!=</span> hy_gaussian).mean()</span>
<span id="cb39-439"><a href="#cb39-439" aria-hidden="true" tabindex="-1"></a>error_naive <span class="op">=</span> (y_g <span class="op">!=</span> hy_naive).mean()</span>
<span id="cb39-440"><a href="#cb39-440" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb39-441"><a href="#cb39-441" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-442"><a href="#cb39-442" aria-hidden="true" tabindex="-1"></a>El clasificador Bayesiano Gausiano tiene un error de $0.0614$ y el error de Bayesiano Ingenuo es $0.0877.$ Se ha visto que el error es una variable aleatoria, entonces la pregunta es saber si esta diferencia en rendimiento es significativa o es una diferencia que proviene de la aleatoriedad de los datos. </span>
<span id="cb39-443"><a href="#cb39-443" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-444"><a href="#cb39-444" aria-hidden="true" tabindex="-1"></a><span class="fu">## Diferencias en Rendimiento </span></span>
<span id="cb39-445"><a href="#cb39-445" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-446"><a href="#cb39-446" aria-hidden="true" tabindex="-1"></a>Una manera de ver si existe una diferencia en rendimiento es calcular la diferencia entre los dos errores de clasificación, esto es </span>
<span id="cb39-447"><a href="#cb39-447" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-450"><a href="#cb39-450" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb39-451"><a href="#cb39-451" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: true</span></span>
<span id="cb39-452"><a href="#cb39-452" aria-hidden="true" tabindex="-1"></a>naive <span class="op">=</span> (y_g <span class="op">!=</span> hy_naive).mean()</span>
<span id="cb39-453"><a href="#cb39-453" aria-hidden="true" tabindex="-1"></a>completo <span class="op">=</span> (y_g <span class="op">!=</span> hy_gaussian).mean()</span>
<span id="cb39-454"><a href="#cb39-454" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span> naive <span class="op">&gt;</span> completo:</span>
<span id="cb39-455"><a href="#cb39-455" aria-hidden="true" tabindex="-1"></a>    diff <span class="op">=</span> naive <span class="op">-</span> completo</span>
<span id="cb39-456"><a href="#cb39-456" aria-hidden="true" tabindex="-1"></a><span class="cf">else</span>:</span>
<span id="cb39-457"><a href="#cb39-457" aria-hidden="true" tabindex="-1"></a>    diff <span class="op">=</span> completo <span class="op">-</span> naive</span>
<span id="cb39-458"><a href="#cb39-458" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb39-459"><a href="#cb39-459" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-462"><a href="#cb39-462" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb39-463"><a href="#cb39-463" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: false</span></span>
<span id="cb39-464"><a href="#cb39-464" aria-hidden="true" tabindex="-1"></a>diff_f <span class="op">=</span> Markdown(<span class="ss">f"$</span><span class="sc">{</span>diff<span class="sc">:0.4f}</span><span class="ss">$"</span>)</span>
<span id="cb39-465"><a href="#cb39-465" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb39-466"><a href="#cb39-466" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-467"><a href="#cb39-467" aria-hidden="true" tabindex="-1"></a>que tiene un valor de <span class="in">`{python} diff_f`</span>. De la misma manera que se ha utilizado la técnica de bootstrap (@sec-bootstrap) para calcular el error estándar de la media, se puede usar para estimar el error estándar de la diferencia en rendimiento. El siguiente código muestra el procedimiento para estimar este error estándar. </span>
<span id="cb39-468"><a href="#cb39-468" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-471"><a href="#cb39-471" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb39-472"><a href="#cb39-472" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: true</span></span>
<span id="cb39-473"><a href="#cb39-473" aria-hidden="true" tabindex="-1"></a>S <span class="op">=</span> np.random.randint(y_g.shape[<span class="dv">0</span>],</span>
<span id="cb39-474"><a href="#cb39-474" aria-hidden="true" tabindex="-1"></a>                      size<span class="op">=</span>(<span class="dv">500</span>, y_g.shape[<span class="dv">0</span>]))</span>
<span id="cb39-475"><a href="#cb39-475" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span> naive <span class="op">&gt;</span> completo:</span>
<span id="cb39-476"><a href="#cb39-476" aria-hidden="true" tabindex="-1"></a>    diff_f <span class="op">=</span> <span class="kw">lambda</span> s: (y_g[s] <span class="op">!=</span> hy_naive[s]).mean() <span class="op">-\</span></span>
<span id="cb39-477"><a href="#cb39-477" aria-hidden="true" tabindex="-1"></a>                       (y_g[s] <span class="op">!=</span> hy_gaussian[s]).mean()</span>
<span id="cb39-478"><a href="#cb39-478" aria-hidden="true" tabindex="-1"></a><span class="cf">else</span>:</span>
<span id="cb39-479"><a href="#cb39-479" aria-hidden="true" tabindex="-1"></a>    diff_f <span class="op">=</span> <span class="kw">lambda</span> s: (y_g[s] <span class="op">!=</span> hy_gaussian[s]).mean() <span class="op">-\</span></span>
<span id="cb39-480"><a href="#cb39-480" aria-hidden="true" tabindex="-1"></a>                       (y_g[s] <span class="op">!=</span> hy_naive[s]).mean()</span>
<span id="cb39-481"><a href="#cb39-481" aria-hidden="true" tabindex="-1"></a>B <span class="op">=</span> [diff_f(s) <span class="cf">for</span> s <span class="kw">in</span> S]</span>
<span id="cb39-482"><a href="#cb39-482" aria-hidden="true" tabindex="-1"></a>se <span class="op">=</span> np.std(B, axis<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb39-483"><a href="#cb39-483" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb39-484"><a href="#cb39-484" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-487"><a href="#cb39-487" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb39-488"><a href="#cb39-488" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: false</span></span>
<span id="cb39-489"><a href="#cb39-489" aria-hidden="true" tabindex="-1"></a>se_f <span class="op">=</span> Markdown(<span class="ss">f"$</span><span class="sc">{</span>se<span class="sc">:0.4f}</span><span class="ss">$"</span>)</span>
<span id="cb39-490"><a href="#cb39-490" aria-hidden="true" tabindex="-1"></a>ratio_f <span class="op">=</span> Markdown(<span class="ss">f"$</span><span class="sc">{</span>diff <span class="op">/</span> se<span class="sc">:0.4f}</span><span class="ss">$"</span>)</span>
<span id="cb39-491"><a href="#cb39-491" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb39-492"><a href="#cb39-492" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-493"><a href="#cb39-493" aria-hidden="true" tabindex="-1"></a>El error estándar de la diferencia de rendimiento es de <span class="in">`{python} se_f`</span>, una procedimiento simple para saber si la diferencia observada es significativa, es dividir la diferencia entre su error estándar dando un valor de <span class="in">`{python} ratio_f`</span>. En el caso que el valor absoluto fuera igual o superior a 2 se sabría que la diferencia es significativa con una confianza de al menos 95%, esto asumiendo que la diferencia se comporta como una distribución Gausiana. </span>
<span id="cb39-494"><a href="#cb39-494" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-495"><a href="#cb39-495" aria-hidden="true" tabindex="-1"></a>El histograma de los datos que se tienen en la variable <span class="in">`B`</span> se observa en la @fig-diff-cl-bayesianos. Se puede ver que la forma del histograma asemeja una distribución Gausiana y que el cero esta en el cuerpo de la Gausiana, tal y como lo confirmó el cociente que se calculado.</span>
<span id="cb39-496"><a href="#cb39-496" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-499"><a href="#cb39-499" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb39-500"><a href="#cb39-500" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-diff-cl-bayesianos</span></span>
<span id="cb39-501"><a href="#cb39-501" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: Diferencia entre Clasificadores Bayesianos</span></span>
<span id="cb39-502"><a href="#cb39-502" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-fold: true</span></span>
<span id="cb39-503"><a href="#cb39-503" aria-hidden="true" tabindex="-1"></a>sns.set_style(<span class="st">'whitegrid'</span>)</span>
<span id="cb39-504"><a href="#cb39-504" aria-hidden="true" tabindex="-1"></a>fig <span class="op">=</span> sns.displot(B, kde<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb39-505"><a href="#cb39-505" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb39-506"><a href="#cb39-506" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-507"><a href="#cb39-507" aria-hidden="true" tabindex="-1"></a>Se puede conocer la probabilidad de manera exacta calculando el área bajo la curva a la izquierda del cero, este sería el valor $p$, si este es menor a 0.05 quiere decir que se tiene una confianza mayor del 95% de que los rendimientos son diferentes. Para este ejemplo, el área se calcula con el siguiente código</span>
<span id="cb39-508"><a href="#cb39-508" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-511"><a href="#cb39-511" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb39-512"><a href="#cb39-512" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: true</span></span>
<span id="cb39-513"><a href="#cb39-513" aria-hidden="true" tabindex="-1"></a>dist <span class="op">=</span> norm(loc<span class="op">=</span>diff, scale<span class="op">=</span>se)</span>
<span id="cb39-514"><a href="#cb39-514" aria-hidden="true" tabindex="-1"></a>p_value <span class="op">=</span> dist.cdf(<span class="dv">0</span>)</span>
<span id="cb39-515"><a href="#cb39-515" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb39-516"><a href="#cb39-516" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-519"><a href="#cb39-519" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb39-520"><a href="#cb39-520" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: false</span></span>
<span id="cb39-521"><a href="#cb39-521" aria-hidden="true" tabindex="-1"></a>p_value_f <span class="op">=</span> Markdown(<span class="ss">f'$</span><span class="sc">{</span>p_value<span class="sc">:0.4f}</span><span class="ss">$'</span>)</span>
<span id="cb39-522"><a href="#cb39-522" aria-hidden="true" tabindex="-1"></a>conf <span class="op">=</span> Markdown(<span class="ss">f'$</span><span class="sc">{</span><span class="bu">int</span>((<span class="dv">1</span> <span class="op">-</span> p_value) <span class="op">*</span> <span class="dv">100</span>)<span class="sc">}</span><span class="ss">$%'</span>)</span>
<span id="cb39-523"><a href="#cb39-523" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb39-524"><a href="#cb39-524" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-525"><a href="#cb39-525" aria-hidden="true" tabindex="-1"></a>teniendo el valor de <span class="in">`{python} p_value_f`</span>, lo que significa que se tiene una confianza del <span class="in">`{python} conf`</span> de que los dos algoritmos son diferentes considerando el error de clasificación como medida de rendimiento. </span>
<span id="cb39-526"><a href="#cb39-526" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-527"><a href="#cb39-527" aria-hidden="true" tabindex="-1"></a><span class="fu">## Regresión {#sec-regresion-ols}</span></span>
<span id="cb39-528"><a href="#cb39-528" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-529"><a href="#cb39-529" aria-hidden="true" tabindex="-1"></a>Hasta este momento se han revisado métodos paramétricos en clasificación, ahora es el turno de abordar el problema de regresión. La diferencia entre clasificación y regresión como se describió en la @sec-aprendizaje-supervisado es que en regresión $\mathcal Y \in \mathbb R.$</span>
<span id="cb39-530"><a href="#cb39-530" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-531"><a href="#cb39-531" aria-hidden="true" tabindex="-1"></a>El procedimiento de regresión que se describe en esta sección es regresión de **Mínimos Cuadrados Ordinaria** (OLS -*Ordinary Least Squares*-), en el cual se asume que $\mathcal Y \sim \mathcal N(\mathbf w \cdot \mathbf x + \epsilon, \sigma^2)$, de tal manera que $y = \mathbb E<span class="co">[</span><span class="ot">\mathcal N(\mathbf w \cdot \mathbf x + \epsilon, \sigma^2)</span><span class="co">]</span>.$</span>
<span id="cb39-532"><a href="#cb39-532" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-533"><a href="#cb39-533" aria-hidden="true" tabindex="-1"></a>Trabajando con $y = \mathbb E<span class="co">[</span><span class="ot">\mathcal N(\mathbf w \cdot \mathbf x + \epsilon, \sigma^2)</span><span class="co">]</span>,$ se considera lo siguiente $y = \mathbb E<span class="co">[</span><span class="ot">\mathcal N(\mathbf w \cdot \mathbf x, 0) + \mathcal N(0, \sigma^2)</span><span class="co">]</span>$ que implica que el error $\epsilon$ es independiente de $\mathbf x$, lo cual se transforma en $y = \mathbf w \cdot \mathbf x + \mathbb E<span class="co">[</span><span class="ot">\epsilon</span><span class="co">]</span>,$ donde $\mathbb E<span class="co">[</span><span class="ot">\epsilon</span><span class="co">]</span>=0.$ Por lo tanto $y = \mathbf w \cdot \mathbf x.$</span>
<span id="cb39-534"><a href="#cb39-534" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-535"><a href="#cb39-535" aria-hidden="true" tabindex="-1"></a>La función de densidad de probabilidad de una Gausiana corresponde a</span>
<span id="cb39-536"><a href="#cb39-536" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-537"><a href="#cb39-537" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb39-538"><a href="#cb39-538" aria-hidden="true" tabindex="-1"></a>f(\alpha) = \frac{1}{\sigma \sqrt{2 \pi}} \exp{-\frac{1}{2} (\frac{\alpha -  \mu}{\sigma})^2},</span>
<span id="cb39-539"><a href="#cb39-539" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb39-540"><a href="#cb39-540" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-541"><a href="#cb39-541" aria-hidden="true" tabindex="-1"></a>donde $\alpha$, en el caso de regresión, corresponde a $\mathbf w \cdot \mathbf x$ (i.e., $\alpha = \mathbf w \cdot \mathbf x$).</span>
<span id="cb39-542"><a href="#cb39-542" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-543"><a href="#cb39-543" aria-hidden="true" tabindex="-1"></a>Utilizando el método de verosimilitud el cual corresponde a maximizar </span>
<span id="cb39-544"><a href="#cb39-544" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-545"><a href="#cb39-545" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb39-546"><a href="#cb39-546" aria-hidden="true" tabindex="-1"></a>\begin{split}</span>
<span id="cb39-547"><a href="#cb39-547" aria-hidden="true" tabindex="-1"></a>\mathcal L(\mathbf w, \sigma) &amp;= \prod_{(\mathbf x, y) \in \mathcal D} f(\mathbf w \cdot \mathbf x) <span class="sc">\\</span></span>
<span id="cb39-548"><a href="#cb39-548" aria-hidden="true" tabindex="-1"></a>&amp;= \prod_{(\mathbf x, y) \in \mathcal D} \frac{1}{\sigma \sqrt{2\pi}} \exp{(-\frac{1}{2} (\frac{\mathbf w \cdot \mathbf x -  y}{\sigma})^2)} <span class="sc">\\</span></span>
<span id="cb39-549"><a href="#cb39-549" aria-hidden="true" tabindex="-1"></a>\ell(\mathbf w, \sigma) &amp;= \sum_{(\mathbf x, y) \in \mathcal D}\log \frac{1}{\sigma \sqrt{2\pi}}  -\frac{1}{2} (\frac{\mathbf w \cdot \mathbf x -  y}{\sigma})^2 <span class="sc">\\</span></span>
<span id="cb39-550"><a href="#cb39-550" aria-hidden="true" tabindex="-1"></a>&amp;= - \frac{1}{2\sigma^2}  \sum_{(\mathbf x, y) \in \mathcal D} (\mathbf w \cdot \mathbf x -  y)^2 - N \log \frac{1}{\sigma \sqrt{2\pi}}.</span>
<span id="cb39-551"><a href="#cb39-551" aria-hidden="true" tabindex="-1"></a>\end{split}</span>
<span id="cb39-552"><a href="#cb39-552" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb39-553"><a href="#cb39-553" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-554"><a href="#cb39-554" aria-hidden="true" tabindex="-1"></a>El valor de cada parámetro se obtiene al calcular la derivada parcial con respecto al parámetro de interés, entonces se resuelven $d$ derivadas parciales para cada uno de los coeficientes $\mathbf w$. En este proceso se observar que el término $N \log \frac{1}{\sigma \sqrt{2\pi}}$ no depende de $\mathbf w$ entonces no afecta el máximo siendo una constante en el proceso de derivación y por lo tanto se desprecia. Lo mismo pasa para la constante $\frac{1}{2\sigma^2}$. Una vez obtenidos los parámetros $\mathcal w$ se obtiene el valor $\sigma.$ </span>
<span id="cb39-555"><a href="#cb39-555" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-556"><a href="#cb39-556" aria-hidden="true" tabindex="-1"></a>Una manera equivalente de plantear este problema es como un problema de algebra lineal, donde se tiene una matriz de observaciones $X$ que se construyen con las variables $\mathbf x$ de $\mathcal X,$ donde cada renglón de $X$ es una observación, y el vector dependiente $\mathbf y$ donde cada elemento es la respuesta correspondiente a la observación.</span>
<span id="cb39-557"><a href="#cb39-557" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-558"><a href="#cb39-558" aria-hidden="true" tabindex="-1"></a>Viéndolo como un problema de algebra lineal lo que se tiene es </span>
<span id="cb39-559"><a href="#cb39-559" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-560"><a href="#cb39-560" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb39-561"><a href="#cb39-561" aria-hidden="true" tabindex="-1"></a>X \mathbf w = \mathbf y,</span>
<span id="cb39-562"><a href="#cb39-562" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb39-563"><a href="#cb39-563" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-564"><a href="#cb39-564" aria-hidden="true" tabindex="-1"></a>donde para identificar $\mathbf w$ se pueden realizar lo siguiente</span>
<span id="cb39-565"><a href="#cb39-565" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-566"><a href="#cb39-566" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb39-567"><a href="#cb39-567" aria-hidden="true" tabindex="-1"></a>X^\intercal X \mathbf w = X^\intercal \mathbf y.</span>
<span id="cb39-568"><a href="#cb39-568" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb39-569"><a href="#cb39-569" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-570"><a href="#cb39-570" aria-hidden="true" tabindex="-1"></a>Despejando $\mathbf w$ se tiene</span>
<span id="cb39-571"><a href="#cb39-571" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-572"><a href="#cb39-572" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb39-573"><a href="#cb39-573" aria-hidden="true" tabindex="-1"></a>\mathbf w = (X^\intercal X)^{-1} X^\intercal \mathbf y.</span>
<span id="cb39-574"><a href="#cb39-574" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb39-575"><a href="#cb39-575" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-576"><a href="#cb39-576" aria-hidden="true" tabindex="-1"></a>Previamente se ha presentado el error estándar de cada parámetro que se ha estimado, en caso de la regresión el error estándar (@sec-error-estandar-ols) de $\mathcal w_j$ es $\sigma \sqrt{(X^\intercal X)^{-1}_{jj}}.$</span>
<span id="cb39-577"><a href="#cb39-577" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-578"><a href="#cb39-578" aria-hidden="true" tabindex="-1"></a><span class="fu">### Ejemplo: Diabetes {#sec-diabetes}</span></span>
<span id="cb39-579"><a href="#cb39-579" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-580"><a href="#cb39-580" aria-hidden="true" tabindex="-1"></a>Esta sección ilustra el proceso de resolver un problema de regresión utilizando OLS. El problema a resolver se obtiene mediante la función <span class="in">`load_diabetes`</span> de la siguiente manera</span>
<span id="cb39-581"><a href="#cb39-581" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-584"><a href="#cb39-584" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb39-585"><a href="#cb39-585" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: True</span></span>
<span id="cb39-586"><a href="#cb39-586" aria-hidden="true" tabindex="-1"></a>X, y <span class="op">=</span> load_diabetes(return_X_y<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb39-587"><a href="#cb39-587" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb39-588"><a href="#cb39-588" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-589"><a href="#cb39-589" aria-hidden="true" tabindex="-1"></a>El siguiente paso es generar los conjuntos de entrenamiento y prueba (@sec-conjunto-entre-prueba)</span>
<span id="cb39-590"><a href="#cb39-590" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-593"><a href="#cb39-593" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb39-594"><a href="#cb39-594" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: true</span></span>
<span id="cb39-595"><a href="#cb39-595" aria-hidden="true" tabindex="-1"></a>T, G, y_t, y_g <span class="op">=</span> train_test_split(X, y, test_size<span class="op">=</span><span class="fl">0.2</span>)</span>
<span id="cb39-596"><a href="#cb39-596" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb39-597"><a href="#cb39-597" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-598"><a href="#cb39-598" aria-hidden="true" tabindex="-1"></a>Con el conjunto de entrenamiento <span class="in">`T`</span> y <span class="in">`y_t`</span> se estiman los parámetros de la regresión lineal tal y como se muestra a continuación</span>
<span id="cb39-599"><a href="#cb39-599" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-602"><a href="#cb39-602" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb39-603"><a href="#cb39-603" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: true</span></span>
<span id="cb39-604"><a href="#cb39-604" aria-hidden="true" tabindex="-1"></a>m <span class="op">=</span> LinearRegression().fit(T, y_t)</span>
<span id="cb39-605"><a href="#cb39-605" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb39-606"><a href="#cb39-606" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-609"><a href="#cb39-609" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb39-610"><a href="#cb39-610" aria-hidden="true" tabindex="-1"></a>w_f <span class="op">=</span> <span class="st">', '</span>.join([<span class="ss">f'</span><span class="sc">{</span>v<span class="sc">:0.2f}</span><span class="ss">'</span> <span class="cf">for</span> v <span class="kw">in</span> m.coef_[:<span class="dv">3</span>]])</span>
<span id="cb39-611"><a href="#cb39-611" aria-hidden="true" tabindex="-1"></a>w_f <span class="op">=</span> Markdown(<span class="ss">f'$</span><span class="ch">\\</span><span class="ss">mathbf w=[</span><span class="sc">{</span>w_f<span class="sc">}</span><span class="ss">, \ldots]$'</span>)</span>
<span id="cb39-612"><a href="#cb39-612" aria-hidden="true" tabindex="-1"></a>w_0 <span class="op">=</span> Markdown(<span class="ss">f'$w_0=</span><span class="sc">{</span>m<span class="sc">.</span>intercept_<span class="sc">:0.2f}</span><span class="ss">$'</span>)</span>
<span id="cb39-613"><a href="#cb39-613" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb39-614"><a href="#cb39-614" aria-hidden="true" tabindex="-1"></a>Los primeros tres coeficientes de la regresión lineal son <span class="in">`{python} w_f`</span> y <span class="in">`{python} w_0`</span> lo cual se encuentran en las siguientes variables</span>
<span id="cb39-615"><a href="#cb39-615" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-618"><a href="#cb39-618" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb39-619"><a href="#cb39-619" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: true</span></span>
<span id="cb39-620"><a href="#cb39-620" aria-hidden="true" tabindex="-1"></a>w <span class="op">=</span> m.coef_</span>
<span id="cb39-621"><a href="#cb39-621" aria-hidden="true" tabindex="-1"></a>w_0 <span class="op">=</span> m.intercept_</span>
<span id="cb39-622"><a href="#cb39-622" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb39-623"><a href="#cb39-623" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-624"><a href="#cb39-624" aria-hidden="true" tabindex="-1"></a>La pregunta es si estos coeficientes son estadísticamente diferentes de cero, esto se puede conocer calculando el error estándar de cada coeficiente. Para lo cual se requiere estimar $\sigma$ que corresponde a la desviación estándar del error tal y como se muestra en las siguientes instrucciones.</span>
<span id="cb39-625"><a href="#cb39-625" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-628"><a href="#cb39-628" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb39-629"><a href="#cb39-629" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: true</span></span>
<span id="cb39-630"><a href="#cb39-630" aria-hidden="true" tabindex="-1"></a>error <span class="op">=</span> y_t <span class="op">-</span> m.predict(T)</span>
<span id="cb39-631"><a href="#cb39-631" aria-hidden="true" tabindex="-1"></a>std_error <span class="op">=</span> np.std(error)</span>
<span id="cb39-632"><a href="#cb39-632" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb39-633"><a href="#cb39-633" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-634"><a href="#cb39-634" aria-hidden="true" tabindex="-1"></a>El error estándar de $\mathbf w$ es </span>
<span id="cb39-635"><a href="#cb39-635" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-638"><a href="#cb39-638" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb39-639"><a href="#cb39-639" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: true</span></span>
<span id="cb39-640"><a href="#cb39-640" aria-hidden="true" tabindex="-1"></a>diag <span class="op">=</span> np.arange(T.shape[<span class="dv">1</span>])</span>
<span id="cb39-641"><a href="#cb39-641" aria-hidden="true" tabindex="-1"></a>_ <span class="op">=</span> np.sqrt((np.dot(T.T, T)<span class="op">**</span>(<span class="op">-</span><span class="dv">1</span>))[diag, diag])</span>
<span id="cb39-642"><a href="#cb39-642" aria-hidden="true" tabindex="-1"></a>se <span class="op">=</span> std_error <span class="op">*</span> _</span>
<span id="cb39-643"><a href="#cb39-643" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb39-644"><a href="#cb39-644" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-647"><a href="#cb39-647" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb39-648"><a href="#cb39-648" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: false</span></span>
<span id="cb39-649"><a href="#cb39-649" aria-hidden="true" tabindex="-1"></a>se_f <span class="op">=</span> <span class="st">', '</span>.join([<span class="ss">f'</span><span class="sc">{</span>v<span class="sc">:0.2f}</span><span class="ss">'</span> <span class="cf">for</span> v <span class="kw">in</span> (w <span class="op">/</span> se)[:<span class="dv">3</span>]])</span>
<span id="cb39-650"><a href="#cb39-650" aria-hidden="true" tabindex="-1"></a>se_f <span class="op">=</span> Markdown(<span class="ss">f'$[</span><span class="sc">{</span>se_f<span class="sc">}</span><span class="ss">, \ldots]$'</span>)</span>
<span id="cb39-651"><a href="#cb39-651" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb39-652"><a href="#cb39-652" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-653"><a href="#cb39-653" aria-hidden="true" tabindex="-1"></a>y para saber si los coeficientes son significativamente diferente de cero se calcula el cociente <span class="in">`m.coef_`</span> entre <span class="in">`se`</span>; teniendo los siguientes valores <span class="in">`{python} se_f`</span>, para las tres primeras componentes. Se observa que hay varios coeficientes con valor absoluto menor que 2, lo cual significa que esas variables tiene un coeficiente que estadísticamente no es diferente de cero. </span>
<span id="cb39-654"><a href="#cb39-654" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-655"><a href="#cb39-655" aria-hidden="true" tabindex="-1"></a>La predicción del conjunto de prueba se puede realizar con la siguiente instrucción</span>
<span id="cb39-656"><a href="#cb39-656" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-659"><a href="#cb39-659" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb39-660"><a href="#cb39-660" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: true</span></span>
<span id="cb39-661"><a href="#cb39-661" aria-hidden="true" tabindex="-1"></a>hy <span class="op">=</span> m.predict(G)</span>
<span id="cb39-662"><a href="#cb39-662" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb39-663"><a href="#cb39-663" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-664"><a href="#cb39-664" aria-hidden="true" tabindex="-1"></a>Finalmente, la @fig-regresion-lineal-scatter muestra las predicciones contra las mediciones reales. También se incluye la línea que ilustra el modelo ideal. </span>
<span id="cb39-665"><a href="#cb39-665" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-668"><a href="#cb39-668" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb39-669"><a href="#cb39-669" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-regresion-lineal-scatter</span></span>
<span id="cb39-670"><a href="#cb39-670" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: Regresión Lineal</span></span>
<span id="cb39-671"><a href="#cb39-671" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-fold: true</span></span>
<span id="cb39-672"><a href="#cb39-672" aria-hidden="true" tabindex="-1"></a>sns.scatterplot(x<span class="op">=</span>hy, y<span class="op">=</span>y_g)</span>
<span id="cb39-673"><a href="#cb39-673" aria-hidden="true" tabindex="-1"></a>_min <span class="op">=</span> <span class="bu">min</span>(y_g.<span class="bu">min</span>(), hy.<span class="bu">min</span>())</span>
<span id="cb39-674"><a href="#cb39-674" aria-hidden="true" tabindex="-1"></a>_max <span class="op">=</span> <span class="bu">max</span>(y_g.<span class="bu">max</span>(), hy.<span class="bu">max</span>())</span>
<span id="cb39-675"><a href="#cb39-675" aria-hidden="true" tabindex="-1"></a>sns.set_style(<span class="st">'whitegrid'</span>)</span>
<span id="cb39-676"><a href="#cb39-676" aria-hidden="true" tabindex="-1"></a>fig <span class="op">=</span> sns.lineplot(x<span class="op">=</span>[_min, _max], y<span class="op">=</span>[_min, _max])</span>
<span id="cb39-677"><a href="#cb39-677" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb39-678"><a href="#cb39-678" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-679"><a href="#cb39-679" aria-hidden="true" tabindex="-1"></a>Complementando el ejemplo anterior, se realiza un modelo que primero elimina las variables que no son estadísticamente diferentes de cero (primera línea) y después crea nuevas variables al incluir el cuadrado, ver las líneas dos y tres del siguiente código. </span>
<span id="cb39-680"><a href="#cb39-680" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-683"><a href="#cb39-683" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb39-684"><a href="#cb39-684" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: true</span></span>
<span id="cb39-685"><a href="#cb39-685" aria-hidden="true" tabindex="-1"></a>mask <span class="op">=</span> np.fabs(m.coef_ <span class="op">/</span> se) <span class="op">&gt;=</span> <span class="dv">2</span></span>
<span id="cb39-686"><a href="#cb39-686" aria-hidden="true" tabindex="-1"></a>T <span class="op">=</span> np.concatenate((T[:, mask], T[:, mask]<span class="op">**</span><span class="dv">2</span>), axis<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb39-687"><a href="#cb39-687" aria-hidden="true" tabindex="-1"></a>G <span class="op">=</span> np.concatenate((G[:, mask], G[:, mask]<span class="op">**</span><span class="dv">2</span>), axis<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb39-688"><a href="#cb39-688" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb39-689"><a href="#cb39-689" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-690"><a href="#cb39-690" aria-hidden="true" tabindex="-1"></a>Se observa que la identificación de los coeficientes $\mathbf w$ sigue siendo lineal aun y cuando la representación ya no es lineal por incluir el cuadrado. Siguiendo los pasos descritos previamente, se inicializa el modelo y después se realiza la predicción.</span>
<span id="cb39-691"><a href="#cb39-691" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-694"><a href="#cb39-694" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb39-695"><a href="#cb39-695" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: true</span></span>
<span id="cb39-696"><a href="#cb39-696" aria-hidden="true" tabindex="-1"></a>m2 <span class="op">=</span> LinearRegression().fit(T, y_t)</span>
<span id="cb39-697"><a href="#cb39-697" aria-hidden="true" tabindex="-1"></a>hy2 <span class="op">=</span> m2.predict(G)</span>
<span id="cb39-698"><a href="#cb39-698" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb39-699"><a href="#cb39-699" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-702"><a href="#cb39-702" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb39-703"><a href="#cb39-703" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: false</span></span>
<span id="cb39-704"><a href="#cb39-704" aria-hidden="true" tabindex="-1"></a>diff_f <span class="op">=</span> Markdown(<span class="ss">f'$</span><span class="sc">{</span>((y_g <span class="op">-</span> hy2)<span class="op">**</span><span class="dv">2</span>)<span class="sc">.</span>mean() <span class="op">-</span>  ((y_g <span class="op">-</span> hy)<span class="op">**</span><span class="dv">2</span>)<span class="sc">.</span>mean()<span class="sc">:0.4f}</span><span class="ss">$'</span>)</span>
<span id="cb39-705"><a href="#cb39-705" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb39-706"><a href="#cb39-706" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-707"><a href="#cb39-707" aria-hidden="true" tabindex="-1"></a>En este momento se compara si la diferencia entre el error cuadrático medio, del primer y segundo modelo, la diferencia es <span class="in">`{python} diff_f`</span> indicando que el primer modelo es mejor. </span>
<span id="cb39-708"><a href="#cb39-708" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-709"><a href="#cb39-709" aria-hidden="true" tabindex="-1"></a><span class="in">```python</span></span>
<span id="cb39-710"><a href="#cb39-710" aria-hidden="true" tabindex="-1"></a>diff <span class="op">=</span> ((y_g <span class="op">-</span> hy2)<span class="op">**</span><span class="dv">2</span>).mean() <span class="op">-</span>  ((y_g <span class="op">-</span> hy)<span class="op">**</span><span class="dv">2</span>).mean()</span>
<span id="cb39-711"><a href="#cb39-711" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb39-712"><a href="#cb39-712" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-713"><a href="#cb39-713" aria-hidden="true" tabindex="-1"></a>Para comprobar si esta diferencia es significativa se calcula el error estándar, utilizando bootstrap (@sec-bootstrap) tal y como se muestra a continuación. </span>
<span id="cb39-714"><a href="#cb39-714" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-717"><a href="#cb39-717" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb39-718"><a href="#cb39-718" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: true</span></span>
<span id="cb39-719"><a href="#cb39-719" aria-hidden="true" tabindex="-1"></a>S <span class="op">=</span> np.random.randint(y_g.shape[<span class="dv">0</span>],</span>
<span id="cb39-720"><a href="#cb39-720" aria-hidden="true" tabindex="-1"></a>                      size<span class="op">=</span>(<span class="dv">500</span>, y_g.shape[<span class="dv">0</span>]))</span>
<span id="cb39-721"><a href="#cb39-721" aria-hidden="true" tabindex="-1"></a>B <span class="op">=</span> [((y_g[s] <span class="op">-</span> hy2[s])<span class="op">**</span><span class="dv">2</span>).mean() <span class="op">-</span></span>
<span id="cb39-722"><a href="#cb39-722" aria-hidden="true" tabindex="-1"></a>      ((y_g[s] <span class="op">-</span> hy[s])<span class="op">**</span><span class="dv">2</span>).mean()</span>
<span id="cb39-723"><a href="#cb39-723" aria-hidden="true" tabindex="-1"></a>     <span class="cf">for</span> s <span class="kw">in</span> S]</span>
<span id="cb39-724"><a href="#cb39-724" aria-hidden="true" tabindex="-1"></a>se <span class="op">=</span> np.std(B, axis<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb39-725"><a href="#cb39-725" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb39-726"><a href="#cb39-726" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-729"><a href="#cb39-729" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb39-730"><a href="#cb39-730" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: false</span></span>
<span id="cb39-731"><a href="#cb39-731" aria-hidden="true" tabindex="-1"></a>dist <span class="op">=</span> norm(loc<span class="op">=</span>diff, scale<span class="op">=</span>se)</span>
<span id="cb39-732"><a href="#cb39-732" aria-hidden="true" tabindex="-1"></a>p_value <span class="op">=</span> dist.cdf(<span class="dv">0</span>)</span>
<span id="cb39-733"><a href="#cb39-733" aria-hidden="true" tabindex="-1"></a>p_value <span class="op">=</span> Markdown(<span class="ss">f'$</span><span class="sc">{</span>p_value<span class="sc">:0.4f}</span><span class="ss">$'</span>)</span>
<span id="cb39-734"><a href="#cb39-734" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb39-735"><a href="#cb39-735" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-736"><a href="#cb39-736" aria-hidden="true" tabindex="-1"></a>Finalmente, se calcula el área bajo la curva a la izquierda del cero, teniendo un valor de <span class="in">`{python} p_value`</span> lo cual indica que los dos modelos son similares. En este caso se prefiere el modelo más simple porque se observar que incluir el cuadrado de las variables no contribuye a generar un mejor model. El área bajo la curva se calcula con el siguiente código. </span>
<span id="cb39-737"><a href="#cb39-737" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-740"><a href="#cb39-740" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb39-741"><a href="#cb39-741" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: true</span></span>
<span id="cb39-742"><a href="#cb39-742" aria-hidden="true" tabindex="-1"></a>dist <span class="op">=</span> norm(loc<span class="op">=</span>diff, scale<span class="op">=</span>se)</span>
<span id="cb39-743"><a href="#cb39-743" aria-hidden="true" tabindex="-1"></a>p_value <span class="op">=</span> dist.cdf(<span class="dv">0</span>)</span>
<span id="cb39-744"><a href="#cb39-744" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
</code><button title="Copiar al portapapeles" class="code-copy-button" data-in-quarto-modal=""><i class="bi"></i></button></pre></div>
</div></div></div></div></div>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
      &nbsp;
    </div>   
    <div class="nav-footer-center">
      &nbsp;
    </div>
    <div class="nav-footer-right">
<p><a href="http://creativecommons.org/licenses/by-sa/4.0/"><img src="https://i.creativecommons.org/l/by-sa/4.0/80x15.png" class="img-fluid"></a> <br> Esta obra está bajo una <a href="http://creativecommons.org/licenses/by-sa/4.0/">Licencia Creative Commons Atribución-CompartirIgual 4.0 Internacional</a></p>
</div>
  </div>
</footer>




</body></html>