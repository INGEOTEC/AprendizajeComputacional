<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="es" xml:lang="es"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.5.37">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Aprendizaje Computacional – 5&nbsp; Reducción de Dimensión</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<link href="../capitulos/06Agrupamiento.html" rel="next">
<link href="../capitulos/04Rendimiento.html" rel="prev">
<script src="../site_libs/quarto-html/quarto.js"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "Sin resultados",
    "search-matching-documents-text": "documentos encontrados",
    "search-copy-link-title": "Copiar el enlace en la búsqueda",
    "search-hide-matches-text": "Ocultar resultados adicionales",
    "search-more-match-text": "resultado adicional en este documento",
    "search-more-matches-text": "resultados adicionales en este documento",
    "search-clear-button-title": "Borrar",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancelar",
    "search-submit-button-title": "Enviar",
    "search-label": "Buscar"
  }
}</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="nav-sidebar floating">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Alternar barra lateral" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../capitulos/05ReduccionDim.html"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Reducción de Dimensión</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Alternar barra lateral" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Buscar" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="../">Aprendizaje Computacional</a> 
        <div class="sidebar-tools-main">
    <a href="https://github.com/INGEOTEC/AprendizajeComputacional" title="Ejecutar el código" class="quarto-navigation-tool px-1" aria-label="Ejecutar el código"><i class="bi bi-github"></i></a>
    <a href="../Aprendizaje-Computacional.pdf" title="Descargar PDF" class="quarto-navigation-tool px-1" aria-label="Descargar PDF"><i class="bi bi-file-pdf"></i></a>
</div>
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Buscar"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Prefacio</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../capitulos/01Introduccion.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Introducción</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../capitulos/02Teoria_Decision.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Teoría de Decisión Bayesiana</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../capitulos/03Parametricos.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Métodos Paramétricos</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../capitulos/04Rendimiento.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Rendimiento</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../capitulos/05ReduccionDim.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Reducción de Dimensión</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../capitulos/06Agrupamiento.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Agrupamiento</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../capitulos/07NoParametricos.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Métodos No Paramétricos</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../capitulos/08Arboles.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Árboles de Decisión</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../capitulos/09Lineal.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Discriminantes Lineales</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../capitulos/10Optimizacion.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Optimización</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../capitulos/11RedesNeuronales.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Redes Neuronales</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../capitulos/12Ensambles.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">Ensambles</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../capitulos/13Comparacion.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">Comparación de Algoritmos</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../capitulos/17Referencias.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Referencias</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true">
 <span class="menu-text">Apéndices</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true" aria-label="Alternar sección">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../capitulos/14Estadistica.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">A</span>&nbsp; <span class="chapter-title">Estadística</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../capitulos/15Codigo.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">B</span>&nbsp; <span class="chapter-title">Código</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../capitulos/16ConjuntosDatos.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">C</span>&nbsp; <span class="chapter-title">Conjunto de Datos</span></span></a>
  </div>
</li>
      </ul>
  </li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Tabla de contenidos</h2>
   
  <ul>
  <li><a href="#paquetes-usados" id="toc-paquetes-usados" class="nav-link active" data-scroll-target="#paquetes-usados">Paquetes usados</a></li>
  <li><a href="#introducción" id="toc-introducción" class="nav-link" data-scroll-target="#introducción"><span class="header-section-number">5.1</span> Introducción</a></li>
  <li><a href="#sec-seleccion-var-estadistica" id="toc-sec-seleccion-var-estadistica" class="nav-link" data-scroll-target="#sec-seleccion-var-estadistica"><span class="header-section-number">5.2</span> Selección de Variables basadas en Estadísticas</a>
  <ul class="collapse">
  <li><a href="#ventajas-y-limitaciones" id="toc-ventajas-y-limitaciones" class="nav-link" data-scroll-target="#ventajas-y-limitaciones"><span class="header-section-number">5.2.1</span> Ventajas y Limitaciones</a></li>
  </ul></li>
  <li><a href="#selección-hacia-adelante" id="toc-selección-hacia-adelante" class="nav-link" data-scroll-target="#selección-hacia-adelante"><span class="header-section-number">5.3</span> Selección hacia Adelante</a>
  <ul class="collapse">
  <li><a href="#ventajas-y-limitaciones-1" id="toc-ventajas-y-limitaciones-1" class="nav-link" data-scroll-target="#ventajas-y-limitaciones-1"><span class="header-section-number">5.3.1</span> Ventajas y Limitaciones</a></li>
  </ul></li>
  <li><a href="#sec-seleccion-por-modelo" id="toc-sec-seleccion-por-modelo" class="nav-link" data-scroll-target="#sec-seleccion-por-modelo"><span class="header-section-number">5.4</span> Selección mediante Modelo</a></li>
  <li><a href="#sec-pca" id="toc-sec-pca" class="nav-link" data-scroll-target="#sec-pca"><span class="header-section-number">5.5</span> Análisis de Componentes Principales</a>
  <ul class="collapse">
  <li><a href="#sec-visualizacion-iris" id="toc-sec-visualizacion-iris" class="nav-link" data-scroll-target="#sec-visualizacion-iris"><span class="header-section-number">5.5.1</span> Ejemplo - Visualización</a></li>
  </ul></li>
  <li><a href="#sec-umap-visualizacion" id="toc-sec-umap-visualizacion" class="nav-link" data-scroll-target="#sec-umap-visualizacion"><span class="header-section-number">5.6</span> UMAP: Uniform Manifold Approximation and Projection for Dimension Reduction</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<div class="quarto-title-block"><div><h1 class="title"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Reducción de Dimensión</span></h1><button type="button" class="btn code-tools-button dropdown-toggle" id="quarto-code-tools-menu" data-bs-toggle="dropdown" aria-expanded="false"><i class="bi"></i> Código</button><ul class="dropdown-menu dropdown-menu-end" aria-labelelledby="quarto-code-tools-menu"><li><a id="quarto-show-all-code" class="dropdown-item" href="javascript:void(0)" role="button">Mostrar todo el código</a></li><li><a id="quarto-hide-all-code" class="dropdown-item" href="javascript:void(0)" role="button">Ocultar todo el código</a></li><li><hr class="dropdown-divider"></li><li><a id="quarto-view-source" class="dropdown-item" href="javascript:void(0)" role="button">Ver el código fuente</a></li></ul></div></div>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<p>El <strong>objetivo</strong> de la unidad es aplicar técnicas de reducción de dimensionalidad, para mejorar el aprendizaje y para visualizar los datos</p>
<section id="paquetes-usados" class="level2 unnumbered">
<h2 class="unnumbered anchored" data-anchor-id="paquetes-usados">Paquetes usados</h2>
<div id="a59c3066" class="cell" data-execution_count="1">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> scipy.stats <span class="im">import</span> multivariate_normal, norm, kruskal</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.datasets <span class="im">import</span> load_diabetes,<span class="op">\</span></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a>                             load_breast_cancer,<span class="op">\</span></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a>                             load_iris,<span class="op">\</span></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a>                             load_wine,<span class="op">\</span></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a>                             load_digits</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.feature_selection <span class="im">import</span> f_regression,<span class="op">\</span></span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a>                                      SelectKBest,<span class="op">\</span></span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a>                                      SelectFromModel,<span class="op">\</span></span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a>                                      SequentialFeatureSelector</span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> KFold, train_test_split</span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> recall_score, make_scorer, r2_score</span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.linear_model <span class="im">import</span> LinearRegression</span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.naive_bayes <span class="im">import</span> GaussianNB</span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.preprocessing <span class="im">import</span> StandardScaler</span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn <span class="im">import</span> decomposition</span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> EvoMSA.model <span class="im">import</span> GaussianBayes</span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> umap</span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> matplotlib <span class="im">import</span> pylab <span class="im">as</span> plt</span>
<span id="cb1-22"><a href="#cb1-22" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib <span class="im">as</span> mpl</span>
<span id="cb1-23"><a href="#cb1-23" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> seaborn <span class="im">as</span> sns</span></code><button title="Copiar al portapapeles" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<hr>
<div class="quarto-video"><iframe data-external="1" src="https://www.youtube.com/embed/mNrDb9YSK8A" width="560" height="315" title="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div>
<hr>
</section>
<section id="introducción" class="level2" data-number="5.1">
<h2 data-number="5.1" class="anchored" data-anchor-id="introducción"><span class="header-section-number">5.1</span> Introducción</h2>
<p>Habiendo descrito problemas de clasificación y regresión, podemos imaginar que existen ocasiones donde las variables que describen al problema no contribuyen dentro de la solución, o que su aporte está dado por otras componentes dentro de la descripción. Esto trae como consecuencia, en el mejor de los caso, que el algoritmo tenga un mayor costo computacional o en un caso menos afortunado que el algoritmo tenga un rendimiento menor al que se hubiera obtenido seleccionado las variables. Es pertinente mencionar que el caso contrario correspondiente al incremento del número de variables es también un escenario factible y se abordará en otra ocasión.</p>
<p>Existen diferentes maneras para reducir la dimensión de un problema, es decir, transformar la representación original <span class="math inline">\(x \in \mathbb R^d\)</span> a una representación <span class="math inline">\(\hat x \in \mathbb R^m\)</span> donde <span class="math inline">\(m &lt; d\)</span>. El objetivo es que la nueva representación <span class="math inline">\(\hat x\)</span> contenga la información necesaria para realizar la tarea de clasificación o regresión. También otro objetivo sería reducir a <span class="math inline">\(\mathbb R^2\)</span> o <span class="math inline">\(\mathbb R^3\)</span> de tal manera que se pueda visualizar el problema. En este último caso el objetivo es que se mantengan las características de los datos en <span class="math inline">\(\mathbb R^d\)</span> en la reducción.</p>
<p>Esta descripción inicia con una metodología de selección basada en calcular estadísticas de los datos y descartar aquellas que no proporcionan información de acuerdo a la estadística.</p>
</section>
<section id="sec-seleccion-var-estadistica" class="level2" data-number="5.2">
<h2 data-number="5.2" class="anchored" data-anchor-id="sec-seleccion-var-estadistica"><span class="header-section-number">5.2</span> Selección de Variables basadas en Estadísticas</h2>
<p>Se utilizará el problema sintético (<a href="02Teoria_Decision.html#sec-tres-normales" class="quarto-xref"><span>Sección 2.3.1</span></a>) de tres clases para describir el algoritmo de selección. Este problema está definido por tres Distribuciones Gausianas donde se generan tres muestras de 1000 elementos cada una utilizando el siguiente código.</p>
<div id="d8e19828" class="cell" data-execution_count="3">
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a>p1 <span class="op">=</span> multivariate_normal(mean<span class="op">=</span>[<span class="dv">5</span>, <span class="dv">5</span>],</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a>                         cov<span class="op">=</span>[[<span class="dv">4</span>, <span class="dv">0</span>], [<span class="dv">0</span>, <span class="dv">2</span>]])</span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a>p2 <span class="op">=</span> multivariate_normal(mean<span class="op">=</span>[<span class="fl">1.5</span>, <span class="op">-</span><span class="fl">1.5</span>],</span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a>                         cov<span class="op">=</span>[[<span class="dv">2</span>, <span class="dv">1</span>], [<span class="dv">1</span>, <span class="dv">3</span>]])</span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a>p3 <span class="op">=</span> multivariate_normal(mean<span class="op">=</span>[<span class="fl">12.5</span>, <span class="op">-</span><span class="fl">3.5</span>],</span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a>                         cov<span class="op">=</span>[[<span class="dv">2</span>, <span class="dv">3</span>], [<span class="dv">3</span>, <span class="dv">7</span>]])</span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a>X_1 <span class="op">=</span> p1.rvs(size<span class="op">=</span><span class="dv">1000</span>)</span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a>X_2 <span class="op">=</span> p2.rvs(size<span class="op">=</span><span class="dv">1000</span>)</span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a>X_3 <span class="op">=</span> p3.rvs(size<span class="op">=</span><span class="dv">1000</span>)</span></code><button title="Copiar al portapapeles" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Estas tres distribuciones representan el problema de clasificación para tres clases. El siguiente código une las tres matrices <code>X_1</code>, <code>X_2</code> y <code>X_3</code> y genera un arreglo <code>y</code> que representa la clase.</p>
<div id="1204ddac" class="cell" data-execution_count="4">
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a>D <span class="op">=</span> np.concatenate((X_1, X_2, X_3), axis<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> np.array([<span class="st">'a'</span>] <span class="op">*</span> X_1.shape[<span class="dv">0</span>] <span class="op">+</span></span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a>             [<span class="st">'b'</span>] <span class="op">*</span> X_2.shape[<span class="dv">0</span>] <span class="op">+</span></span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a>             [<span class="st">'c'</span>] <span class="op">*</span> X_3.shape[<span class="dv">0</span>])</span></code><button title="Copiar al portapapeles" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Por construcción el problema está en <span class="math inline">\(\mathbb R^2\)</span> y se sabe que las dos componentes contribuyen a la solución del mismo, es decir, imagine que una de las variables se pierde, con la información restante se desarrollaría un algoritmo de clasificación con un rendimiento mucho menor a aquel que tenga toda la información.</p>
<p>Continuando con el problema sintético, en está ocasión lo que se realiza es incluir en el problema una variable que no tiene relación con la clase, para esto se añade una variable aleatoria con una distribución Gausiana con <span class="math inline">\(\mu=2\)</span> y <span class="math inline">\(\sigma=3\)</span> tal como se muestra en el siguiente código.</p>
<div id="174de752" class="cell" data-execution_count="5">
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a>N <span class="op">=</span> norm.rvs(loc<span class="op">=</span><span class="dv">2</span>, scale<span class="op">=</span><span class="dv">3</span>, size<span class="op">=</span><span class="dv">3000</span>)</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a>D <span class="op">=</span> np.concatenate((D, np.atleast_2d(N).T), axis<span class="op">=</span><span class="dv">1</span>)</span></code><button title="Copiar al portapapeles" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>El objetivo es encontrar la variable que no está relacionada con la salida. Una manera de realizar esto es imaginar que si la media en las diferentes variables es la misma en todas las clases entonces esa variable no contribuye a discriminar la clase. En <a href="03Parametricos.html#sec-estimacion-parametros" class="quarto-xref"><span>Sección 3.5.2</span></a> se presentó el procedimiento para obtener las medias que definen <span class="math inline">\(\mathbb P(\mathcal X \mid \mathcal Y)\)</span> para cada clase. El siguiente código muestra el procedimiento para calcular las medías que son <span class="math inline">\(\mu_1=[5.0494, 4.9877, 1.9303]^\intercal\)</span>, <span class="math inline">\(\mu_2=[1.5272, -1.5190, 2.0854]^\intercal\)</span> y <span class="math inline">\(\mu_3=[12.5464, -3.4195, 1.8253]^\intercal\)</span>.</p>
<div id="5c4cba34" class="cell" data-execution_count="7">
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a>labels <span class="op">=</span> np.unique(y)</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a>mus <span class="op">=</span> [np.mean(D[y<span class="op">==</span>i], axis<span class="op">=</span><span class="dv">0</span>) <span class="cf">for</span> i <span class="kw">in</span> labels]</span></code><button title="Copiar al portapapeles" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Se observa que la media de la tercera variable es aproximadamente igual para las tres clases, teniendo un valor cercano a <span class="math inline">\(2\)</span> tal y como fue generada. Entonces lo que se busca es un procedimiento que permita identificar que las muestras en cada grupo (clase) hayan sido originadas por la misma distribución. Es decir se busca una prueba que indique que las primeras dos variables provienen de diferentes distribuciones y que la tercera provienen de la misma distribución. Es pertinente comentar que este procedimiento no es aplicable para problemas de regresión.</p>
<p>Si se puede suponer que los datos provienen de una Distribución Gausiana entonces la prueba a realizar es ANOVA, en caso contrario se puede utilizar su equivalente método no paramétrico como es la prueba Kruskal-Wallis. Considerando que de manera general se desconoce la distribución que genera los datos, entonces se presenta el uso de la segunda prueba.</p>
<p>La prueba Kruskal-Wallis identifica si un conjunto de muestras independientes provienen de la misma distribución. La hipótesis nula es que las muestras provienen de la misma distribución. La función <code>kruskal</code> implementa esta prueba y recibe tantas muestras como argumentos. En el siguiente código ilustra su uso, se observa que se llama a la función <code>kruskal</code> para cada columna en <code>D</code> y se calcula su valor <span class="math inline">\(p\)</span>. Los valores <span class="math inline">\(p\)</span> obtenidos son: <span class="math inline">\([0.0000, 0.0000, 0.3511]\)</span> lo cual indica que para las primeras dos variables la hipótesis nula se puede rechazar y por el otro lado la hipótesis nula es factible para la tercera variable con un valor <span class="math inline">\(p=0.3511\)</span>.</p>
<div id="b6ab4828" class="cell" data-execution_count="9">
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a>res <span class="op">=</span> [kruskal(<span class="op">*</span>[D[y<span class="op">==</span>l, i] <span class="cf">for</span> l <span class="kw">in</span> labels]).pvalue</span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a>       <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(D.shape[<span class="dv">1</span>])]</span></code><button title="Copiar al portapapeles" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>En lugar de discriminar aquellas características que no aportan a modelar los datos, es más común seleccionar las mejores características. Este procedimiento se puede realizar utilizando los valores <span class="math inline">\(p\)</span> de la prueba estadística o cualquier otra función que ordene la importancia de las características.</p>
<p>El procedimiento equivalente a la estadística de Kruskal-Wallis en regresión es calcular la estadística F cuya hipótesis nula es asumir que el coeficiente obtenido en una regresión lineal entre las variables independientes y la dependiente es zero. Esta estadística se encuentra implementada en la función <code>f_regression</code>. El siguiente código muestra su uso en el conjunto de datos de diabetes; el cual tiene <span class="math inline">\(10\)</span> variables independientes. En la variable <code>p_values</code> se tienen los valores <span class="math inline">\(p\)</span> se puede observar que el valor <span class="math inline">\(p\)</span> correspondiente a la segunda variable tiene un valor de <span class="math inline">\(0.3664\)</span>, lo cual hace que esa variable no sea representativa para el problema que se está resolviendo.</p>
<div id="7e9d3f0a" class="cell" data-execution_count="11">
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a>X, y <span class="op">=</span> load_diabetes(return_X_y<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a>f_statistics, p_values <span class="op">=</span> f_regression(X, y)</span></code><button title="Copiar al portapapeles" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Un ejemplo que involucra la selección de las variables más representativas mediante una calificación que ordenan la importancia de las mismas se muestra en el siguiente código. Se puede observar que las nueve variables seleccionadas son: [0, 2, 3, 4, 5, 6, 7, 8, 9] descartando la segunda variable que tiene el máximo valor <span class="math inline">\(p.\)</span></p>
<div id="15b37fca" class="cell" data-execution_count="13">
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a>sel <span class="op">=</span> SelectKBest(score_func<span class="op">=</span>f_regression, k<span class="op">=</span><span class="dv">9</span>).fit(X, y)</span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a><span class="bu">vars</span> <span class="op">=</span> sel.get_support(indices<span class="op">=</span><span class="va">True</span>)</span></code><button title="Copiar al portapapeles" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<section id="ventajas-y-limitaciones" class="level3" data-number="5.2.1">
<h3 data-number="5.2.1" class="anchored" data-anchor-id="ventajas-y-limitaciones"><span class="header-section-number">5.2.1</span> Ventajas y Limitaciones</h3>
<p>Las técnicas vistas hasta este momento requieren de pocos recursos computacionales para su cálculo. Además están basadas en estadísticas que permite saber cuales son las razones de funcionamiento. Estas fortalezas también son origen a sus debilidades, estas técnicas observan en cada paso las variables independientes de manera aislada y no consideran que estas variables pueden interactuar. Por otro lado, la selección es agnóstica del algoritmo de aprendizaje utilizado.</p>
</section>
</section>
<section id="selección-hacia-adelante" class="level2" data-number="5.3">
<h2 data-number="5.3" class="anchored" data-anchor-id="selección-hacia-adelante"><span class="header-section-number">5.3</span> Selección hacia Adelante</h2>
<p>Un procedimiento que pone en el centro del proceso de selección al algoritmo de aprendizaje utilizado es <strong>Selección hacia Adelante</strong> y su complemento que sería <strong>Selección hacia Atrás.</strong> El algoritmo de selección hacia adelante es un procedimiento iterativo que selecciona una variable a la vez guiada por el rendimiento de esta variable cuando es usada en el algoritmo de aprendizaje. Al igual que los procedimientos anteriores este no modifica las características del problema, solamente selecciona las que se consideran relevantes.</p>
<p>En selección hacia adelante y hacia atrás se inicia con el conjunto de entrenamiento <span class="math inline">\(\mathcal T = \{(x_i, y_i)\},\)</span> con una función <span class="math inline">\(L\)</span> que mide el rendimiento del algoritmo de aprendizaje <span class="math inline">\(\mathcal H.\)</span> La idea es ir seleccionando de manera iterativa aquellas variables que generan un modelo con mejores capacidades de generalización. Para medir la generalización del algoritmo se pueden realizar de diferentes maneras, una es mediante la división de <span class="math inline">\(\mathcal X\)</span> en dos conjuntos: entrenamiento y validación; y la segunda manera corresponde a utilizar <span class="math inline">\(k\)</span>-iteraciones de validación cruzada.</p>
<p>Suponga un conjunto <span class="math inline">\(\pi \subseteq \{1, 2, \ldots, d\}\)</span> de tal manera que <span class="math inline">\(\mathcal T_\pi\)</span> solamente cuenta con las variables identificadas en el conjunto <span class="math inline">\(\pi\)</span>. Utilizando está notación el algoritmo se puede definir de la siguiente manera. Inicialmente $^0 = $, en el siguiente paso <span class="math inline">\(\pi^{j + 1} \leftarrow \pi^j \cup \textsf{arg max}_{\{i \mid i \in \{1, 2, \ldots, d\}, i \notin \pi^j\}} \mathcal P(\mathcal H, \mathcal T_{\pi \cup i}, L)\)</span>, donde <span class="math inline">\(\mathcal P\)</span> representa el rendimiento del algoritmo <span class="math inline">\(\mathcal H\)</span> en el subconjunto <span class="math inline">\({\pi \cup i}\)</span> usando la función de rendimiento <span class="math inline">\(L\)</span>. Este proceso continua si <span class="math inline">\(\mathcal P^{j+1} &gt; \mathcal P^j\)</span> donde <span class="math inline">\(\mathcal P^0 = 0\)</span>.</p>
<p>Es importante mencionar que el algoritmo antes descrito es un algoritmo voraz y que el encontrar el óptimo de este problema de optimización no se garantiza con este tipo de algoritmos. Lo que quiere decir es que el algoritmo encontrará un óptimo local.</p>
<p>Ilustrando estos pasos en el conjunto de Breast Cancer Wisconsin (<a href="03Parametricos.html#sec-ejemplo-breast-cancer-wisconsin" class="quarto-xref"><span>Sección 3.7</span></a>).</p>
<div id="a2fe4b68" class="cell" data-execution_count="14">
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a>D, y <span class="op">=</span> load_breast_cancer(return_X_y<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a>T, G, y_t, y_g <span class="op">=</span> train_test_split(D, y, test_size<span class="op">=</span><span class="fl">0.2</span>)</span></code><button title="Copiar al portapapeles" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>El primer paso es medir el rendimiento cuando solamente una variable interviene en el proceso. Como inicialmente $^0 = $ entonces solo es necesario generar un clasificador cuando sola una variable está involucrada. El rendimiento de cada variable se guarda en la variable <code>perf</code>, se puede observar que el primer ciclo (línea 3) itera por todas las variables en la representación, para cada una se seleccionan los datos solo con esa variable <code>T1 = T[:, np.array([var])]</code> después se hacen <span class="math inline">\(k\)</span>-iteraciones de validación cruzada y finalmente se guarda el rendimiento. El rendimiento que se está calculando es macro-recall.</p>
<div id="efd7b007" class="cell" data-execution_count="15">
<div class="sourceCode cell-code" id="cb10"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a>perf <span class="op">=</span> []</span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a>kfold <span class="op">=</span> KFold(shuffle<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> var <span class="kw">in</span> <span class="bu">range</span>(T.shape[<span class="dv">1</span>]):</span>
<span id="cb10-4"><a href="#cb10-4" aria-hidden="true" tabindex="-1"></a>    T1 <span class="op">=</span> T[:, np.array([var])]</span>
<span id="cb10-5"><a href="#cb10-5" aria-hidden="true" tabindex="-1"></a>    perf_inner <span class="op">=</span> []</span>
<span id="cb10-6"><a href="#cb10-6" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> ts, vs <span class="kw">in</span> kfold.split(T1):</span>
<span id="cb10-7"><a href="#cb10-7" aria-hidden="true" tabindex="-1"></a>        gaussian <span class="op">=</span> GaussianNB().fit(T1[ts], y_t[ts])</span>
<span id="cb10-8"><a href="#cb10-8" aria-hidden="true" tabindex="-1"></a>        hy_gaussian <span class="op">=</span> gaussian.predict(T1[vs])</span>
<span id="cb10-9"><a href="#cb10-9" aria-hidden="true" tabindex="-1"></a>        _ <span class="op">=</span> recall_score(y_t[vs], hy_gaussian,</span>
<span id="cb10-10"><a href="#cb10-10" aria-hidden="true" tabindex="-1"></a>                         average<span class="op">=</span><span class="st">'macro'</span>)    </span>
<span id="cb10-11"><a href="#cb10-11" aria-hidden="true" tabindex="-1"></a>        perf_inner.append(_)</span>
<span id="cb10-12"><a href="#cb10-12" aria-hidden="true" tabindex="-1"></a>    perf.append(np.mean(perf_inner))</span></code><button title="Copiar al portapapeles" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>La <a href="#fig-rendimiento-sel" class="quarto-xref">Figura&nbsp;<span>5.1</span></a> muestra el rendimiento de las 30 variables, se observa como una gran parte de las variables proporcionan un rendimiento superior al <span class="math inline">\(0.8\)</span> y la variable que tiene el mejor rendimiento es la que corresponde al índice <span class="math inline">\(27\)</span> y valor <span class="math inline">\(0.9037\)</span>.</p>
<div id="cell-fig-rendimiento-sel" class="cell" data-execution_count="17">
<details class="code-fold">
<summary>Código</summary>
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a>sns.barplot(x<span class="op">=</span><span class="bu">list</span>(<span class="bu">range</span>(<span class="bu">len</span>(perf))), y<span class="op">=</span>perf)</span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'Variable'</span>)</span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a>fig <span class="op">=</span> plt.ylabel(<span class="st">'Macro-Recall'</span>)</span></code><button title="Copiar al portapapeles" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div id="fig-rendimiento-sel" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-rendimiento-sel-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="05ReduccionDim_files/figure-html/fig-rendimiento-sel-output-1.png" width="585" height="427" class="figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-rendimiento-sel-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figura&nbsp;5.1: Rendimiento de las variables en la primera iteración del selección hacia adelante
</figcaption>
</figure>
</div>
</div>
</div>
<p>El algoritmo de selección hacia atrás y adelante se implementa en la clase <code>SequentialFeatureSelector</code> y su uso se observa en las siguientes instrucciones.</p>
<div id="0928d254" class="cell" data-execution_count="18">
<div class="sourceCode cell-code" id="cb12"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a>kfolds <span class="op">=</span> <span class="bu">list</span>(KFold(shuffle<span class="op">=</span><span class="va">True</span>).split(T))</span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a>scoring <span class="op">=</span> make_scorer(<span class="kw">lambda</span> y, hy: recall_score(y, hy,</span>
<span id="cb12-3"><a href="#cb12-3" aria-hidden="true" tabindex="-1"></a>                                         average<span class="op">=</span><span class="st">'macro'</span>))</span>
<span id="cb12-4"><a href="#cb12-4" aria-hidden="true" tabindex="-1"></a>seq <span class="op">=</span> SequentialFeatureSelector(estimator<span class="op">=</span>GaussianNB(),</span>
<span id="cb12-5"><a href="#cb12-5" aria-hidden="true" tabindex="-1"></a>                                scoring<span class="op">=</span>scoring,</span>
<span id="cb12-6"><a href="#cb12-6" aria-hidden="true" tabindex="-1"></a>                                n_features_to_select<span class="op">=</span><span class="st">'auto'</span>,</span>
<span id="cb12-7"><a href="#cb12-7" aria-hidden="true" tabindex="-1"></a>                                cv<span class="op">=</span>kfolds).fit(T, y_t)</span></code><button title="Copiar al portapapeles" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Al igual que en el algoritmo de <code>SelectKBest</code> las variables seleccionadas se pueden observar con la función <code>get_support</code>. En este caso las variables seleccionadas son: <span class="math inline">\([1, 4, 7, 9, 11, 14, 16, 17, 18, 19, 21, 22, 23, 24, 26]\)</span>.</p>
<p>Utilizando el parámetro <code>direction='backward'</code> para utilizar selección hacia atrás en el mismo conjunto de datos, da como resultado la siguiente selección <span class="math inline">\([1, 4, 6, 7, 8, 11, 15, 17, 20, 21, 22, 23, 24, 27, 28]\)</span>. Se puede observar que las variables seleccionadas por los métodos son diferentes. Esto es factible porque los algoritmos solo aseguran llegar a un máximo local y no está garantizado que el máximo local corresponda al máximo global.</p>
<section id="ventajas-y-limitaciones-1" class="level3" data-number="5.3.1">
<h3 data-number="5.3.1" class="anchored" data-anchor-id="ventajas-y-limitaciones-1"><span class="header-section-number">5.3.1</span> Ventajas y Limitaciones</h3>
<p>Una de las ventajas de la selección hacia atrás y adelante es que el algoritmo termina a lo más cuando se han analizado todas las variables, esto eso para un problema en <span class="math inline">\(\mathbb R^d\)</span> se analizarán un máximo de <span class="math inline">\(d\)</span> variables. La principal desventaja es que estos algoritmos son voraces, es decir, toman la mejor decisión en el momento, lo cual tiene como consecuencia que sean capaces de garantizar llegar a un máximo local y en ocasiones este máximo local no corresponde al máximo global. Con el fin de complementar esta idea, en un problema <span class="math inline">\(\mathbb R^d\)</span> se tiene un espacio de búsqueda de <span class="math inline">\(2^d - 1\)</span>, es decir, se tiene esa cantidad de diferentes configuraciones que se pueden explorar. En los algoritmos de vistos se observa un máximo de <span class="math inline">\(d\)</span> elementos de ese total.</p>
</section>
</section>
<section id="sec-seleccion-por-modelo" class="level2" data-number="5.4">
<h2 data-number="5.4" class="anchored" data-anchor-id="sec-seleccion-por-modelo"><span class="header-section-number">5.4</span> Selección mediante Modelo</h2>
<p>Existen algoritmos de aprendizaje supervisado donde sus parámetros indican la importancia que tiene cada característica en el problema. Para ejemplificar esto se utilizará el problema de Diabetes que fue utilizado en la <a href="03Parametricos.html#sec-diabetes" class="quarto-xref"><span>Sección 3.9.1</span></a>. Los datos se obtienen con la siguiente instrucción, adicionalmente se normalizan las características para tener una media <span class="math inline">\(0\)</span> y desviación estándar <span class="math inline">\(1\)</span>.</p>
<div id="3263b793" class="cell" data-execution_count="20">
<div class="sourceCode cell-code" id="cb13"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a>D, y <span class="op">=</span> load_diabetes(return_X_y<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a>D <span class="op">=</span> StandardScaler().fit_transform(D)</span></code><button title="Copiar al portapapeles" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>El siguiente paso se estiman los parámetros de una regresión lineal (<a href="03Parametricos.html#sec-regresion-ols" class="quarto-xref"><span>Sección 3.9</span></a>), también se muestran los valores de los primeros tres coeficientes de la regresión. Se puede observar que el valor absoluto de estos coeficientes está en diferentes rangos, el primer coeficiente es cercano a cero, el segundo está alrededor de 10 y el tercero es superior a 20. Considerando que los datos están normalizados, entonces el valor absoluto indica el efecto que tiene esa variable en el resultado final, en estos tres datos es puede concluir que el tercer coeficiente tiene una mayor contribución que los otros dos coeficientes.</p>
<div id="2985a3c7" class="cell" data-execution_count="21">
<div class="sourceCode cell-code" id="cb14"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a>reg <span class="op">=</span> LinearRegression().fit(D, y)</span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a>reg.coef_[:<span class="dv">3</span>]</span></code><button title="Copiar al portapapeles" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="21">
<pre><code>array([ -0.47612079, -11.40686692,  24.72654886])</code></pre>
</div>
</div>
<p>La clase <code>SelectFromModel</code> permite seleccionar las <span class="math inline">\(d\)</span> características que el modelo considera más importante. En el siguiente ejemplo se selecciona las <span class="math inline">\(d=4\)</span> características más importantes; dentro de estas características se encuentra la tercera (i.e., índice 2) cuyo coeficiente se había presentado previamente.</p>
<div id="e1f171af" class="cell" data-execution_count="22">
<div class="sourceCode cell-code" id="cb16"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a>sel <span class="op">=</span> SelectFromModel(reg, max_features<span class="op">=</span><span class="dv">4</span>,</span>
<span id="cb16-2"><a href="#cb16-2" aria-hidden="true" tabindex="-1"></a>                      threshold<span class="op">=-</span>np.inf, prefit<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb16-3"><a href="#cb16-3" aria-hidden="true" tabindex="-1"></a>np.where(sel.get_support())[<span class="dv">0</span>]</span></code><button title="Copiar al portapapeles" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="22">
<pre><code>array([2, 4, 5, 8])</code></pre>
</div>
</div>
<p>Habiendo descrito el procedimiento para seleccionar aquellas características más importantes, es necesario encontrar cuál es el número de características que producen el mejor rendimiento. Para realizar esto, se realiza <span class="math inline">\(k\)</span>-iteraciones de validación cruzada (<a href="04Rendimiento.html#sec-kfold-cross-validation" class="quarto-xref"><span>Sección 4.4.1</span></a>), donde se varía el número de características y en cada caso se mide el rendimiento utilizando la medida <span class="math inline">\(R^2\)</span> (<a href="04Rendimiento.html#sec-rendimiento-regresion" class="quarto-xref"><span>Sección 4.3</span></a>).</p>
<div id="3cbbb59d" class="cell" data-execution_count="23">
<div class="sourceCode cell-code" id="cb18"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a>folds <span class="op">=</span> [x <span class="cf">for</span> x <span class="kw">in</span> KFold(shuffle<span class="op">=</span><span class="va">True</span>).split(D, y)]</span>
<span id="cb18-2"><a href="#cb18-2" aria-hidden="true" tabindex="-1"></a>perf <span class="op">=</span> []</span>
<span id="cb18-3"><a href="#cb18-3" aria-hidden="true" tabindex="-1"></a>dims <span class="op">=</span> <span class="bu">range</span>(<span class="dv">2</span>, D.shape[<span class="dv">1</span>] <span class="op">+</span> <span class="dv">1</span>)</span>
<span id="cb18-4"><a href="#cb18-4" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> dim <span class="kw">in</span> dims:</span>
<span id="cb18-5"><a href="#cb18-5" aria-hidden="true" tabindex="-1"></a>    model <span class="op">=</span> SelectFromModel(reg, max_features<span class="op">=</span>dim,</span>
<span id="cb18-6"><a href="#cb18-6" aria-hidden="true" tabindex="-1"></a>                            threshold<span class="op">=-</span>np.inf, prefit<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb18-7"><a href="#cb18-7" aria-hidden="true" tabindex="-1"></a>    hy <span class="op">=</span> np.empty_like(y)</span>
<span id="cb18-8"><a href="#cb18-8" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> tr, vs <span class="kw">in</span> folds:</span>
<span id="cb18-9"><a href="#cb18-9" aria-hidden="true" tabindex="-1"></a>        Dt <span class="op">=</span> model.transform(D)</span>
<span id="cb18-10"><a href="#cb18-10" aria-hidden="true" tabindex="-1"></a>        m <span class="op">=</span> LinearRegression().fit(Dt[tr], y[tr])</span>
<span id="cb18-11"><a href="#cb18-11" aria-hidden="true" tabindex="-1"></a>        hy[vs] <span class="op">=</span> m.predict(Dt[vs])</span>
<span id="cb18-12"><a href="#cb18-12" aria-hidden="true" tabindex="-1"></a>    perf.append(<span class="bu">dict</span>(r2<span class="op">=</span>r2_score(y, hy), numero<span class="op">=</span>dim))</span></code><button title="Copiar al portapapeles" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>La <a href="#fig-seleccion-por-modelo" class="quarto-xref">Figura&nbsp;<span>5.2</span></a> muestra el rendimiento (<span class="math inline">\(R^2\)</span>) para cada modelo generado, empezando cuando se usan las <span class="math inline">\(2\)</span> características más importantes y terminando con el modelo que tiene todas las características. En la figura se puede observar que el rendimiento mejora considerablemente de <span class="math inline">\(2\)</span> a <span class="math inline">\(3\)</span> características, este sigue mejorando y llega un punto donde incluir más características tiene un impacto negativo en el rendimiento.</p>
<div id="cell-fig-seleccion-por-modelo" class="cell" data-execution_count="24">
<details class="code-fold">
<summary>Código</summary>
<div class="sourceCode cell-code" id="cb19"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a>df <span class="op">=</span> pd.DataFrame(perf)</span>
<span id="cb19-2"><a href="#cb19-2" aria-hidden="true" tabindex="-1"></a>ax <span class="op">=</span> sns.relplot(df, x<span class="op">=</span><span class="st">'numero'</span>, y<span class="op">=</span><span class="st">'r2'</span>, kind<span class="op">=</span><span class="st">'line'</span>)</span></code><button title="Copiar al portapapeles" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div id="fig-seleccion-por-modelo" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-seleccion-por-modelo-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="05ReduccionDim_files/figure-html/fig-seleccion-por-modelo-output-1.png" width="471" height="471" class="figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-seleccion-por-modelo-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figura&nbsp;5.2: Rendimiento variando el número de características
</figcaption>
</figure>
</div>
</div>
</div>
</section>
<section id="sec-pca" class="level2" data-number="5.5">
<h2 data-number="5.5" class="anchored" data-anchor-id="sec-pca"><span class="header-section-number">5.5</span> Análisis de Componentes Principales</h2>
<p>Los algoritmos de selección hacia atrás y adelante tiene la característica de requerir un conjunto de entrenamiento de aprendizaje supervisado, por lo que no podrían ser utilizados en problemas de aprendizaje no-supervisado. En esta sección se revisará el uso de Análisis de Componentes Principales (Principal Components Analysis - PCA) para la reducción de dimensión. PCA tiene la firma: <span class="math inline">\(f: \mathbb R^d \rightarrow \mathbb R^m\)</span> donde <span class="math inline">\(m &lt; d\)</span></p>
<p>La idea de PCA es buscar una matriz de proyección <span class="math inline">\(W^\intercal \in \mathbb R^{m \times d}\)</span> tal que los elementos de <span class="math inline">\(\mathcal D = \{x_i\}\)</span> sea transformados utilizando <span class="math inline">\(z = W^\intercal x\)</span> donde <span class="math inline">\(z \in \mathbb R^m\)</span>. El objetivo es que la muestra <span class="math inline">\(z_1\)</span> tenga la mayor variación posible. Es decir, se quiere observar en la primera característica de los elementos transformados la mayor variación; esto se puede lograr de la siguiente manera.</p>
<p>Si suponemos que <span class="math inline">\(\mathbf x \sim \mathcal N_d(\mu, \Sigma)\)</span> y <span class="math inline">\(\mathbf w \in \mathbb R^d\)</span> entonces <span class="math inline">\(\mathbf w \cdot \mathbf x \sim \mathcal N(\mathbf w \cdot \mu, \mathbf w^\intercal \Sigma \mathbf w)\)</span> y por lo tanto <span class="math inline">\(\textsf{Var} (\mathbf w \cdot \mathbf x) = \mathbf w^\intercal \Sigma \mathbf w.\)</span></p>
<p>Utilizando esta información se puede describir el problema como encontrar <span class="math inline">\(\mathbf w_1\)</span> tal que <span class="math inline">\(\textsf{Var}(\mathbf z_1)\)</span> sea máxima, donde <span class="math inline">\(\textsf{Var}(\mathbf z_1) = \mathbf w_1^\intercal \Sigma \mathbf w_1.\)</span> Dado que en este problema de optimización tiene multiples soluciones, se busca además maximizar bajo la restricción de <span class="math inline">\(\mid\mid \mathbf w_1 \mid\mid = 1.\)</span> Escribiéndolo como un problema de Lagrange quedaría como: <span class="math inline">\(\max_{\mathbf w_1} \mathbf w_1^\intercal \Sigma \mathbf w_1 - \alpha (\mathbf w_1 \cdot \mathbf w_1 - 1).\)</span> Derivando con respecto a <span class="math inline">\(\mathbf w_1\)</span> se tiene que la solución es: <span class="math inline">\(\Sigma \mathbf w_i = \alpha \mathbf w_i\)</span> donde esto se cumple solo si <span class="math inline">\(\mathbf w_1\)</span> es un eigenvector de <span class="math inline">\(\Sigma\)</span> y <span class="math inline">\(\alpha\)</span> es el eigenvalor correspondiente. Para encontrar <span class="math inline">\(\mathbf w_2\)</span> se requiere <span class="math inline">\(\mid\mid \mathbf w_2 \mid\mid = 1\)</span> y que los vectores sean ortogonales, es decir, <span class="math inline">\(\mathbf w_2 \cdot \mathbf w_1 = 0.\)</span> Realizando las operaciones necesarias se encuentra que <span class="math inline">\(\mathbf w_2\)</span> corresponde al segundo eigenvector y así sucesivamente.</p>
<section id="sec-visualizacion-iris" class="level3" data-number="5.5.1">
<h3 data-number="5.5.1" class="anchored" data-anchor-id="sec-visualizacion-iris"><span class="header-section-number">5.5.1</span> Ejemplo - Visualización</h3>
<p>Supongamos que deseamos visualizar los ejemplos del problema del iris. Los ejemplos se encuetran en <span class="math inline">\(\mathbb R^4\)</span> entonces para poderlos graficar en <span class="math inline">\(\mathbb R^2\)</span> se requiere realizar una transformación como podría ser Análisis de Componentes Principales.</p>
<p>Empezamos por cargar los datos del problema tal y como se muestra en la siguiente instrucción, en la segunda linea se normalizan los datos para tener una media <span class="math inline">\(0\)</span> y desviación estándar <span class="math inline">\(1\)</span>, este procedimiento es necesario cuando los valores de las características se encuentran en diferentes escalas.</p>
<div id="b57dd691" class="cell" data-execution_count="25">
<div class="sourceCode cell-code" id="cb20"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a>D, y <span class="op">=</span> load_iris(return_X_y<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb20-2"><a href="#cb20-2" aria-hidden="true" tabindex="-1"></a>D <span class="op">=</span> StandardScaler().fit_transform(D)</span></code><button title="Copiar al portapapeles" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Habiendo importado los datos el siguiente paso es inicializar la clase de PCA, para esto requerimos especificar el parámetro que indica el número de componentes deseado, dado que el objetivo es representar en <span class="math inline">\(\mathbb R^2\)</span> los datos, entonces el ocupamos dos componentes. La primera línea inicializa la clase de PCA, después, en la segunda línea se hace la proyección.</p>
<div id="bb9815cf" class="cell" data-execution_count="26">
<div class="sourceCode cell-code" id="cb21"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a>pca <span class="op">=</span> decomposition.PCA(n_components<span class="op">=</span><span class="dv">2</span>).fit(D)</span>
<span id="cb21-2"><a href="#cb21-2" aria-hidden="true" tabindex="-1"></a>Xn <span class="op">=</span> pca.transform(D)</span></code><button title="Copiar al portapapeles" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>El siguiente código se utiliza para visualizar los datos. El resultado se muestra en la <a href="#fig-pca-iris" class="quarto-xref">Figura&nbsp;<span>5.3</span></a>, donde se observa en diferente color cada una de las clases.</p>
<div id="cell-fig-pca-iris" class="cell" data-execution_count="27">
<details class="code-fold">
<summary>Código</summary>
<div class="sourceCode cell-code" id="cb22"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb22-1"><a href="#cb22-1" aria-hidden="true" tabindex="-1"></a>data <span class="op">=</span> pd.DataFrame([<span class="bu">dict</span>(x<span class="op">=</span>x, y<span class="op">=</span>y, clase<span class="op">=</span>c)</span>
<span id="cb22-2"><a href="#cb22-2" aria-hidden="true" tabindex="-1"></a>                     <span class="cf">for</span> (x, y), c <span class="kw">in</span> <span class="bu">zip</span>(Xn, y)])</span>
<span id="cb22-3"><a href="#cb22-3" aria-hidden="true" tabindex="-1"></a>fig <span class="op">=</span> sns.relplot(data, kind<span class="op">=</span><span class="st">'scatter'</span>,</span>
<span id="cb22-4"><a href="#cb22-4" aria-hidden="true" tabindex="-1"></a>                  x<span class="op">=</span><span class="st">'x'</span>, y<span class="op">=</span><span class="st">'y'</span>, hue<span class="op">=</span><span class="st">'clase'</span>)</span>
<span id="cb22-5"><a href="#cb22-5" aria-hidden="true" tabindex="-1"></a>fig.tick_params(bottom<span class="op">=</span><span class="va">False</span>, top<span class="op">=</span><span class="va">False</span>, </span>
<span id="cb22-6"><a href="#cb22-6" aria-hidden="true" tabindex="-1"></a>                left<span class="op">=</span><span class="va">False</span>, right<span class="op">=</span><span class="va">False</span>,</span>
<span id="cb22-7"><a href="#cb22-7" aria-hidden="true" tabindex="-1"></a>                labelbottom<span class="op">=</span><span class="va">False</span>, labelleft<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb22-8"><a href="#cb22-8" aria-hidden="true" tabindex="-1"></a>fig.<span class="bu">set</span>(xlabel<span class="op">=</span><span class="va">None</span>, ylabel<span class="op">=</span><span class="va">None</span>)</span></code><button title="Copiar al portapapeles" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div id="fig-pca-iris" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-pca-iris-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="05ReduccionDim_files/figure-html/fig-pca-iris-output-1.png" width="485" height="432" class="figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-pca-iris-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figura&nbsp;5.3: Proyección mediante PCA del problema del Iris
</figcaption>
</figure>
</div>
</div>
</div>
</section>
</section>
<section id="sec-umap-visualizacion" class="level2" data-number="5.6">
<h2 data-number="5.6" class="anchored" data-anchor-id="sec-umap-visualizacion"><span class="header-section-number">5.6</span> UMAP: Uniform Manifold Approximation and Projection for Dimension Reduction</h2>
<p>En esta sección se presenta el uso de UMAP propuesto por <span class="citation" data-cites="UMAP">McInnes, Healy, y Melville (<a href="17Referencias.html#ref-UMAP" role="doc-biblioref">2020</a>)</span>. UMAP es una técnica no lineal para reducir la dimensión y que con dimensiones bajas (<span class="math inline">\(d=2\)</span>) permite tener una visualización de los datos. El ejemplo que se realizará es visualizar mediante UMAP los datos del problema de Dígitos, estos datos se pueden obtener con la siguiente instrucción; en estas instrucciones también se normalizan los datos para tener una media <span class="math inline">\(0\)</span> y desviación estándar <span class="math inline">\(1.\)</span></p>
<div id="8708c63e" class="cell" data-execution_count="28">
<div class="sourceCode cell-code" id="cb23"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb23-1"><a href="#cb23-1" aria-hidden="true" tabindex="-1"></a>D, y <span class="op">=</span> load_digits(return_X_y<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb23-2"><a href="#cb23-2" aria-hidden="true" tabindex="-1"></a>D <span class="op">=</span> StandardScaler().fit_transform(D)</span></code><button title="Copiar al portapapeles" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Con el objetivo de ilustrar la diferencia entre una técnica de proyección lineal como PCA con respecto a una técnica no lineal, la <a href="#fig-pca-digits" class="quarto-xref">Figura&nbsp;<span>5.4</span></a> presenta la proyección del conjunto de Dígitos utilizando PCA. Como se puede observar en la figura, esta proyección no da una separación clara entre grupos, aunque si se puede identificar una estructura, donde algunos números se encuentran en la perifería de la figura.</p>
<div id="cell-fig-pca-digits" class="cell" data-execution_count="29">
<details class="code-fold">
<summary>Código</summary>
<div class="sourceCode cell-code" id="cb24"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb24-1"><a href="#cb24-1" aria-hidden="true" tabindex="-1"></a>pal <span class="op">=</span> mpl.cm.Paired</span>
<span id="cb24-2"><a href="#cb24-2" aria-hidden="true" tabindex="-1"></a>pca <span class="op">=</span> decomposition.PCA(n_components<span class="op">=</span><span class="dv">2</span>).fit(D)</span>
<span id="cb24-3"><a href="#cb24-3" aria-hidden="true" tabindex="-1"></a>Xn <span class="op">=</span> pca.transform(D)</span>
<span id="cb24-4"><a href="#cb24-4" aria-hidden="true" tabindex="-1"></a>df <span class="op">=</span> pd.DataFrame(Xn, columns<span class="op">=</span>[<span class="st">'x'</span>, <span class="st">'y'</span>])</span>
<span id="cb24-5"><a href="#cb24-5" aria-hidden="true" tabindex="-1"></a>df[<span class="st">'Clase'</span>] <span class="op">=</span> y</span>
<span id="cb24-6"><a href="#cb24-6" aria-hidden="true" tabindex="-1"></a>fig <span class="op">=</span> sns.relplot(df, kind<span class="op">=</span><span class="st">'scatter'</span>,</span>
<span id="cb24-7"><a href="#cb24-7" aria-hidden="true" tabindex="-1"></a>                  legend<span class="op">=</span><span class="st">'full'</span>, palette<span class="op">=</span>pal,</span>
<span id="cb24-8"><a href="#cb24-8" aria-hidden="true" tabindex="-1"></a>                  x<span class="op">=</span><span class="st">'x'</span>, y<span class="op">=</span><span class="st">'y'</span>, hue<span class="op">=</span><span class="st">'Clase'</span>)</span>
<span id="cb24-9"><a href="#cb24-9" aria-hidden="true" tabindex="-1"></a>fig.tick_params(bottom<span class="op">=</span><span class="va">False</span>, top<span class="op">=</span><span class="va">False</span>, </span>
<span id="cb24-10"><a href="#cb24-10" aria-hidden="true" tabindex="-1"></a>                left<span class="op">=</span><span class="va">False</span>, right<span class="op">=</span><span class="va">False</span>,</span>
<span id="cb24-11"><a href="#cb24-11" aria-hidden="true" tabindex="-1"></a>                labelbottom<span class="op">=</span><span class="va">False</span>, labelleft<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb24-12"><a href="#cb24-12" aria-hidden="true" tabindex="-1"></a>fig.<span class="bu">set</span>(xlabel<span class="op">=</span><span class="va">None</span>, ylabel<span class="op">=</span><span class="va">None</span>)</span></code><button title="Copiar al portapapeles" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div id="fig-pca-digits" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-pca-digits-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="05ReduccionDim_files/figure-html/fig-pca-digits-output-1.png" width="473" height="432" class="figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-pca-digits-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figura&nbsp;5.4: Proyección mediante PCA del problema de Dígitos
</figcaption>
</figure>
</div>
</div>
</div>
<p>UMAP genera una proyección que intuitivamente trata de preservar la distancia que existe entre los <span class="math inline">\(K\)</span> vecinos cercanos en la dimensión original como en la dimensión destino. Por este motivo un parámetro importante de UMAP es la cantidad de vecinos, que se tiene el efecto de priorizar la estructura global o local de los datos. En el siguiente código se realiza una exploración de este parámetro generando una visualización para <span class="math inline">\(K=[2, 4, 8, 16].\)</span> El código que realiza está exploración se muestra en las siguientes instrucciones.</p>
<div id="de9cea0d" class="cell" data-execution_count="30">
<div class="sourceCode cell-code" id="cb25"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb25-1"><a href="#cb25-1" aria-hidden="true" tabindex="-1"></a>df <span class="op">=</span> pd.DataFrame()</span>
<span id="cb25-2"><a href="#cb25-2" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> K <span class="kw">in</span> [<span class="dv">2</span>, <span class="dv">4</span>, <span class="dv">8</span>, <span class="dv">16</span>]:</span>
<span id="cb25-3"><a href="#cb25-3" aria-hidden="true" tabindex="-1"></a>    reducer <span class="op">=</span> umap.UMAP(n_neighbors<span class="op">=</span>K)</span>
<span id="cb25-4"><a href="#cb25-4" aria-hidden="true" tabindex="-1"></a>    low_dim <span class="op">=</span> reducer.fit_transform(D)</span>
<span id="cb25-5"><a href="#cb25-5" aria-hidden="true" tabindex="-1"></a>    _ <span class="op">=</span> pd.DataFrame(low_dim, columns<span class="op">=</span>[<span class="st">'x'</span>, <span class="st">'y'</span>])</span>
<span id="cb25-6"><a href="#cb25-6" aria-hidden="true" tabindex="-1"></a>    _[<span class="st">'Clase'</span>] <span class="op">=</span> y</span>
<span id="cb25-7"><a href="#cb25-7" aria-hidden="true" tabindex="-1"></a>    _[<span class="st">'K'</span>] <span class="op">=</span> K</span>
<span id="cb25-8"><a href="#cb25-8" aria-hidden="true" tabindex="-1"></a>    df <span class="op">=</span> pd.concat((df, _))</span></code><button title="Copiar al portapapeles" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>La <a href="#fig-umap-digits-k" class="quarto-xref">Figura&nbsp;<span>5.5</span></a> muestra las diferentes visualizaciones cuando el número de vecinos de cambia en UMAP, se puede observar como para <span class="math inline">\(K=2\)</span> la visualización no muestra ninguna estructura, pero partiendo de <span class="math inline">\(K=4\)</span> se puede visualizar grupos que corresponden a los diferentes dígitos.</p>
<div id="cell-fig-umap-digits-k" class="cell" data-execution_count="31">
<details class="code-fold">
<summary>Código</summary>
<div class="sourceCode cell-code" id="cb26"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb26-1"><a href="#cb26-1" aria-hidden="true" tabindex="-1"></a>fig <span class="op">=</span> sns.relplot(df, kind<span class="op">=</span><span class="st">'scatter'</span>, x<span class="op">=</span><span class="st">'x'</span>, y<span class="op">=</span><span class="st">'y'</span>,</span>
<span id="cb26-2"><a href="#cb26-2" aria-hidden="true" tabindex="-1"></a>                  legend<span class="op">=</span><span class="st">'full'</span>, palette<span class="op">=</span>pal,</span>
<span id="cb26-3"><a href="#cb26-3" aria-hidden="true" tabindex="-1"></a>                  hue<span class="op">=</span><span class="st">'Clase'</span>, col<span class="op">=</span><span class="st">'K'</span>)</span>
<span id="cb26-4"><a href="#cb26-4" aria-hidden="true" tabindex="-1"></a>fig.tick_params(bottom<span class="op">=</span><span class="va">False</span>, top<span class="op">=</span><span class="va">False</span>, </span>
<span id="cb26-5"><a href="#cb26-5" aria-hidden="true" tabindex="-1"></a>                left<span class="op">=</span><span class="va">False</span>, right<span class="op">=</span><span class="va">False</span>,</span>
<span id="cb26-6"><a href="#cb26-6" aria-hidden="true" tabindex="-1"></a>                labelbottom<span class="op">=</span><span class="va">False</span>, labelleft<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb26-7"><a href="#cb26-7" aria-hidden="true" tabindex="-1"></a>fig.<span class="bu">set</span>(xlabel<span class="op">=</span><span class="va">None</span>, ylabel<span class="op">=</span><span class="va">None</span>)            </span></code><button title="Copiar al portapapeles" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div id="fig-umap-digits-k" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-umap-digits-k-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="05ReduccionDim_files/figure-html/fig-umap-digits-k-output-1.png" width="1923" height="432" class="figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-umap-digits-k-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figura&nbsp;5.5: Proyección mediante UMAP del problema de Dígitos variando el número de vecinos
</figcaption>
</figure>
</div>
</div>
</div>
<p>Con el objetivo de mostrar con mayor detalle los grupos encontrados, la <a href="#fig-umap-digits-k8" class="quarto-xref">Figura&nbsp;<span>5.6</span></a> muestra la visualización de <span class="math inline">\(K=8\)</span> en ella se puede observar como los ejemplos se agrupan de acuerdo al dígito que representan, también se puede ver la similitud entre los diferentes dígitos y como algunos ejemplos están muy cerca a un grupo al cual no pertenecen.</p>
<div id="cell-fig-umap-digits-k8" class="cell" data-execution_count="32">
<details class="code-fold">
<summary>Código</summary>
<div class="sourceCode cell-code" id="cb27"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb27-1"><a href="#cb27-1" aria-hidden="true" tabindex="-1"></a>fig <span class="op">=</span> sns.relplot(df[df.K<span class="op">==</span><span class="dv">8</span>], kind<span class="op">=</span><span class="st">'scatter'</span>, </span>
<span id="cb27-2"><a href="#cb27-2" aria-hidden="true" tabindex="-1"></a>                  legend<span class="op">=</span><span class="st">'full'</span>, palette<span class="op">=</span>pal,</span>
<span id="cb27-3"><a href="#cb27-3" aria-hidden="true" tabindex="-1"></a>                  x<span class="op">=</span><span class="st">'x'</span>, y<span class="op">=</span><span class="st">'y'</span>, hue<span class="op">=</span><span class="st">'Clase'</span>)</span>
<span id="cb27-4"><a href="#cb27-4" aria-hidden="true" tabindex="-1"></a>fig.tick_params(bottom<span class="op">=</span><span class="va">False</span>, top<span class="op">=</span><span class="va">False</span>, </span>
<span id="cb27-5"><a href="#cb27-5" aria-hidden="true" tabindex="-1"></a>                left<span class="op">=</span><span class="va">False</span>, right<span class="op">=</span><span class="va">False</span>,</span>
<span id="cb27-6"><a href="#cb27-6" aria-hidden="true" tabindex="-1"></a>                labelbottom<span class="op">=</span><span class="va">False</span>, labelleft<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb27-7"><a href="#cb27-7" aria-hidden="true" tabindex="-1"></a>fig.<span class="bu">set</span>(xlabel<span class="op">=</span><span class="va">None</span>, ylabel<span class="op">=</span><span class="va">None</span>)</span></code><button title="Copiar al portapapeles" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div id="fig-umap-digits-k8" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-umap-digits-k8-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="05ReduccionDim_files/figure-html/fig-umap-digits-k8-output-1.png" width="477" height="432" class="figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-umap-digits-k8-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figura&nbsp;5.6: Proyección mediante UMAP del problema de dígitos con ocho vecinos
</figcaption>
</figure>
</div>
</div>
</div>


<!-- -->

<div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" role="list" style="display: none">
<div id="ref-UMAP" class="csl-entry" role="listitem">
McInnes, Leland, John Healy, y James Melville. 2020. <span>«UMAP: Uniform Manifold Approximation and Projection for Dimension Reduction»</span>. <a href="https://arxiv.org/abs/1802.03426">https://arxiv.org/abs/1802.03426</a>.
</div>
</div>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copiado");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copiado");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    // For code content inside modals, clipBoardJS needs to be initialized with a container option
    // TODO: Check when it could be a function (https://github.com/zenorocha/clipboard.js/issues/860)
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
  const viewSource = window.document.getElementById('quarto-view-source') ||
                     window.document.getElementById('quarto-code-tools-source');
  if (viewSource) {
    const sourceUrl = viewSource.getAttribute("data-quarto-source-url");
    viewSource.addEventListener("click", function(e) {
      if (sourceUrl) {
        // rstudio viewer pane
        if (/\bcapabilities=\b/.test(window.location)) {
          window.open(sourceUrl);
        } else {
          window.location.href = sourceUrl;
        }
      } else {
        const modal = new bootstrap.Modal(document.getElementById('quarto-embedded-source-code-modal'));
        modal.show();
      }
      return false;
    });
  }
  function toggleCodeHandler(show) {
    return function(e) {
      const detailsSrc = window.document.querySelectorAll(".cell > details > .sourceCode");
      for (let i=0; i<detailsSrc.length; i++) {
        const details = detailsSrc[i].parentElement;
        if (show) {
          details.open = true;
        } else {
          details.removeAttribute("open");
        }
      }
      const cellCodeDivs = window.document.querySelectorAll(".cell > .sourceCode");
      const fromCls = show ? "hidden" : "unhidden";
      const toCls = show ? "unhidden" : "hidden";
      for (let i=0; i<cellCodeDivs.length; i++) {
        const codeDiv = cellCodeDivs[i];
        if (codeDiv.classList.contains(fromCls)) {
          codeDiv.classList.remove(fromCls);
          codeDiv.classList.add(toCls);
        } 
      }
      return false;
    }
  }
  const hideAllCode = window.document.getElementById("quarto-hide-all-code");
  if (hideAllCode) {
    hideAllCode.addEventListener("click", toggleCodeHandler(false));
  }
  const showAllCode = window.document.getElementById("quarto-show-all-code");
  if (showAllCode) {
    showAllCode.addEventListener("click", toggleCodeHandler(true));
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp("https:\/\/ingeotec\.github\.io\/AprendizajeComputacional");
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="../capitulos/04Rendimiento.html" class="pagination-link" aria-label="Rendimiento">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Rendimiento</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="../capitulos/06Agrupamiento.html" class="pagination-link" aria-label="Agrupamiento">
        <span class="nav-page-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Agrupamiento</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav><div class="modal fade" id="quarto-embedded-source-code-modal" tabindex="-1" aria-labelledby="quarto-embedded-source-code-modal-label" aria-hidden="true"><div class="modal-dialog modal-dialog-scrollable"><div class="modal-content"><div class="modal-header"><h5 class="modal-title" id="quarto-embedded-source-code-modal-label">Ejecutar el código</h5><button class="btn-close" data-bs-dismiss="modal"></button></div><div class="modal-body"><div class="">
<div class="sourceCode" id="cb28" data-shortcodes="false"><pre class="sourceCode markdown code-with-copy"><code class="sourceCode markdown"><span id="cb28-1"><a href="#cb28-1" aria-hidden="true" tabindex="-1"></a><span class="fu"># Reducción de Dimensión</span></span>
<span id="cb28-2"><a href="#cb28-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-3"><a href="#cb28-3" aria-hidden="true" tabindex="-1"></a>El **objetivo** de la unidad es aplicar técnicas de reducción de dimensionalidad, para mejorar el aprendizaje y para visualizar los datos</span>
<span id="cb28-4"><a href="#cb28-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-5"><a href="#cb28-5" aria-hidden="true" tabindex="-1"></a><span class="fu">## Paquetes usados {.unnumbered}</span></span>
<span id="cb28-6"><a href="#cb28-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-9"><a href="#cb28-9" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb28-10"><a href="#cb28-10" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: true</span></span>
<span id="cb28-11"><a href="#cb28-11" aria-hidden="true" tabindex="-1"></a><span class="co">#| warning: false</span></span>
<span id="cb28-12"><a href="#cb28-12" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> scipy.stats <span class="im">import</span> multivariate_normal, norm, kruskal</span>
<span id="cb28-13"><a href="#cb28-13" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.datasets <span class="im">import</span> load_diabetes,<span class="op">\</span></span>
<span id="cb28-14"><a href="#cb28-14" aria-hidden="true" tabindex="-1"></a>                             load_breast_cancer,<span class="op">\</span></span>
<span id="cb28-15"><a href="#cb28-15" aria-hidden="true" tabindex="-1"></a>                             load_iris,<span class="op">\</span></span>
<span id="cb28-16"><a href="#cb28-16" aria-hidden="true" tabindex="-1"></a>                             load_wine,<span class="op">\</span></span>
<span id="cb28-17"><a href="#cb28-17" aria-hidden="true" tabindex="-1"></a>                             load_digits</span>
<span id="cb28-18"><a href="#cb28-18" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.feature_selection <span class="im">import</span> f_regression,<span class="op">\</span></span>
<span id="cb28-19"><a href="#cb28-19" aria-hidden="true" tabindex="-1"></a>                                      SelectKBest,<span class="op">\</span></span>
<span id="cb28-20"><a href="#cb28-20" aria-hidden="true" tabindex="-1"></a>                                      SelectFromModel,<span class="op">\</span></span>
<span id="cb28-21"><a href="#cb28-21" aria-hidden="true" tabindex="-1"></a>                                      SequentialFeatureSelector</span>
<span id="cb28-22"><a href="#cb28-22" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> KFold, train_test_split</span>
<span id="cb28-23"><a href="#cb28-23" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> recall_score, make_scorer, r2_score</span>
<span id="cb28-24"><a href="#cb28-24" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.linear_model <span class="im">import</span> LinearRegression</span>
<span id="cb28-25"><a href="#cb28-25" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.naive_bayes <span class="im">import</span> GaussianNB</span>
<span id="cb28-26"><a href="#cb28-26" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.preprocessing <span class="im">import</span> StandardScaler</span>
<span id="cb28-27"><a href="#cb28-27" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn <span class="im">import</span> decomposition</span>
<span id="cb28-28"><a href="#cb28-28" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> EvoMSA.model <span class="im">import</span> GaussianBayes</span>
<span id="cb28-29"><a href="#cb28-29" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> umap</span>
<span id="cb28-30"><a href="#cb28-30" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb28-31"><a href="#cb28-31" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb28-32"><a href="#cb28-32" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> matplotlib <span class="im">import</span> pylab <span class="im">as</span> plt</span>
<span id="cb28-33"><a href="#cb28-33" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib <span class="im">as</span> mpl</span>
<span id="cb28-34"><a href="#cb28-34" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> seaborn <span class="im">as</span> sns</span>
<span id="cb28-35"><a href="#cb28-35" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb28-36"><a href="#cb28-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-39"><a href="#cb28-39" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb28-40"><a href="#cb28-40" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: false</span></span>
<span id="cb28-41"><a href="#cb28-41" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> IPython.display <span class="im">import</span> Markdown</span>
<span id="cb28-42"><a href="#cb28-42" aria-hidden="true" tabindex="-1"></a>sns.set_style(<span class="st">'whitegrid'</span>)</span>
<span id="cb28-43"><a href="#cb28-43" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb28-44"><a href="#cb28-44" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-45"><a href="#cb28-45" aria-hidden="true" tabindex="-1"></a>::: {.content-visible when-format="html"}</span>
<span id="cb28-46"><a href="#cb28-46" aria-hidden="true" tabindex="-1"></a>---</span>
<span id="cb28-47"><a href="#cb28-47" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-48"><a href="#cb28-48" aria-hidden="true" tabindex="-1"></a>{{&lt; video https://www.youtube.com/embed/mNrDb9YSK8A width="560" height="315" &gt;}}</span>
<span id="cb28-49"><a href="#cb28-49" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-50"><a href="#cb28-50" aria-hidden="true" tabindex="-1"></a>---</span>
<span id="cb28-51"><a href="#cb28-51" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb28-52"><a href="#cb28-52" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-53"><a href="#cb28-53" aria-hidden="true" tabindex="-1"></a><span class="fu">## Introducción</span></span>
<span id="cb28-54"><a href="#cb28-54" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-55"><a href="#cb28-55" aria-hidden="true" tabindex="-1"></a>Habiendo descrito problemas de clasificación y regresión, podemos imaginar que existen ocasiones donde las variables que describen al problema no contribuyen dentro de la solución, o que su aporte está dado por otras componentes dentro de la descripción. Esto trae como consecuencia, en el mejor de los caso, que el algoritmo tenga un mayor costo computacional o en un caso menos afortunado que el algoritmo tenga un rendimiento menor al que se hubiera obtenido seleccionado las variables. Es pertinente mencionar que el caso contrario correspondiente al incremento del número de variables es también un escenario factible y se abordará en otra ocasión.</span>
<span id="cb28-56"><a href="#cb28-56" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-57"><a href="#cb28-57" aria-hidden="true" tabindex="-1"></a>Existen diferentes maneras para reducir la dimensión de un problema, es decir, transformar la representación original $x \in \mathbb R^d$ a una representación $\hat x \in \mathbb R^m$ donde $m &lt; d$. El objetivo es que la nueva representación $\hat x$ contenga la información necesaria para realizar la tarea de clasificación o regresión. También otro objetivo sería reducir a $\mathbb R^2$ o $\mathbb R^3$ de tal manera que se pueda visualizar el problema. En este último caso el objetivo es que se mantengan las características de los datos en $\mathbb R^d$ en la reducción.  </span>
<span id="cb28-58"><a href="#cb28-58" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-59"><a href="#cb28-59" aria-hidden="true" tabindex="-1"></a>Esta descripción inicia con una metodología de selección basada en calcular estadísticas de los datos y descartar aquellas que no proporcionan información de acuerdo a la estadística. </span>
<span id="cb28-60"><a href="#cb28-60" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-61"><a href="#cb28-61" aria-hidden="true" tabindex="-1"></a><span class="fu">## Selección de Variables basadas en Estadísticas {#sec-seleccion-var-estadistica}</span></span>
<span id="cb28-62"><a href="#cb28-62" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-63"><a href="#cb28-63" aria-hidden="true" tabindex="-1"></a>Se utilizará el problema sintético (@sec-tres-normales) de tres clases para describir el algoritmo de selección. Este problema está definido por tres Distribuciones Gausianas donde se generan tres muestras de 1000 elementos cada una utilizando el siguiente código. </span>
<span id="cb28-64"><a href="#cb28-64" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-67"><a href="#cb28-67" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb28-68"><a href="#cb28-68" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: true</span></span>
<span id="cb28-69"><a href="#cb28-69" aria-hidden="true" tabindex="-1"></a>p1 <span class="op">=</span> multivariate_normal(mean<span class="op">=</span>[<span class="dv">5</span>, <span class="dv">5</span>],</span>
<span id="cb28-70"><a href="#cb28-70" aria-hidden="true" tabindex="-1"></a>                         cov<span class="op">=</span>[[<span class="dv">4</span>, <span class="dv">0</span>], [<span class="dv">0</span>, <span class="dv">2</span>]])</span>
<span id="cb28-71"><a href="#cb28-71" aria-hidden="true" tabindex="-1"></a>p2 <span class="op">=</span> multivariate_normal(mean<span class="op">=</span>[<span class="fl">1.5</span>, <span class="op">-</span><span class="fl">1.5</span>],</span>
<span id="cb28-72"><a href="#cb28-72" aria-hidden="true" tabindex="-1"></a>                         cov<span class="op">=</span>[[<span class="dv">2</span>, <span class="dv">1</span>], [<span class="dv">1</span>, <span class="dv">3</span>]])</span>
<span id="cb28-73"><a href="#cb28-73" aria-hidden="true" tabindex="-1"></a>p3 <span class="op">=</span> multivariate_normal(mean<span class="op">=</span>[<span class="fl">12.5</span>, <span class="op">-</span><span class="fl">3.5</span>],</span>
<span id="cb28-74"><a href="#cb28-74" aria-hidden="true" tabindex="-1"></a>                         cov<span class="op">=</span>[[<span class="dv">2</span>, <span class="dv">3</span>], [<span class="dv">3</span>, <span class="dv">7</span>]])</span>
<span id="cb28-75"><a href="#cb28-75" aria-hidden="true" tabindex="-1"></a>X_1 <span class="op">=</span> p1.rvs(size<span class="op">=</span><span class="dv">1000</span>)</span>
<span id="cb28-76"><a href="#cb28-76" aria-hidden="true" tabindex="-1"></a>X_2 <span class="op">=</span> p2.rvs(size<span class="op">=</span><span class="dv">1000</span>)</span>
<span id="cb28-77"><a href="#cb28-77" aria-hidden="true" tabindex="-1"></a>X_3 <span class="op">=</span> p3.rvs(size<span class="op">=</span><span class="dv">1000</span>)</span>
<span id="cb28-78"><a href="#cb28-78" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb28-79"><a href="#cb28-79" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-80"><a href="#cb28-80" aria-hidden="true" tabindex="-1"></a>Estas tres distribuciones representan el problema de clasificación para tres clases. El siguiente código une las tres matrices <span class="in">`X_1`</span>, <span class="in">`X_2`</span> y <span class="in">`X_3`</span> y genera un arreglo <span class="in">`y`</span> que representa la clase. </span>
<span id="cb28-81"><a href="#cb28-81" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-82"><a href="#cb28-82" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-85"><a href="#cb28-85" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb28-86"><a href="#cb28-86" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: true</span></span>
<span id="cb28-87"><a href="#cb28-87" aria-hidden="true" tabindex="-1"></a>D <span class="op">=</span> np.concatenate((X_1, X_2, X_3), axis<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb28-88"><a href="#cb28-88" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> np.array([<span class="st">'a'</span>] <span class="op">*</span> X_1.shape[<span class="dv">0</span>] <span class="op">+</span></span>
<span id="cb28-89"><a href="#cb28-89" aria-hidden="true" tabindex="-1"></a>             [<span class="st">'b'</span>] <span class="op">*</span> X_2.shape[<span class="dv">0</span>] <span class="op">+</span></span>
<span id="cb28-90"><a href="#cb28-90" aria-hidden="true" tabindex="-1"></a>             [<span class="st">'c'</span>] <span class="op">*</span> X_3.shape[<span class="dv">0</span>])</span>
<span id="cb28-91"><a href="#cb28-91" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb28-92"><a href="#cb28-92" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-93"><a href="#cb28-93" aria-hidden="true" tabindex="-1"></a>Por construcción el problema está en $\mathbb R^2$ y se sabe que las dos componentes contribuyen a la solución del mismo, es decir, imagine que una de las variables se pierde, con la información restante se desarrollaría un algoritmo de clasificación con un rendimiento mucho menor a aquel que tenga toda la información. </span>
<span id="cb28-94"><a href="#cb28-94" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-95"><a href="#cb28-95" aria-hidden="true" tabindex="-1"></a>Continuando con el problema sintético, en está ocasión lo que se realiza es incluir en el problema una variable que no tiene relación con la clase, para esto se añade una variable aleatoria con una distribución Gausiana con $\mu=2$ y $\sigma=3$ tal como se muestra en el siguiente código.</span>
<span id="cb28-96"><a href="#cb28-96" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-99"><a href="#cb28-99" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb28-100"><a href="#cb28-100" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: true</span></span>
<span id="cb28-101"><a href="#cb28-101" aria-hidden="true" tabindex="-1"></a>N <span class="op">=</span> norm.rvs(loc<span class="op">=</span><span class="dv">2</span>, scale<span class="op">=</span><span class="dv">3</span>, size<span class="op">=</span><span class="dv">3000</span>)</span>
<span id="cb28-102"><a href="#cb28-102" aria-hidden="true" tabindex="-1"></a>D <span class="op">=</span> np.concatenate((D, np.atleast_2d(N).T), axis<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb28-103"><a href="#cb28-103" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb28-104"><a href="#cb28-104" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-107"><a href="#cb28-107" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb28-108"><a href="#cb28-108" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: false</span></span>
<span id="cb28-109"><a href="#cb28-109" aria-hidden="true" tabindex="-1"></a>labels <span class="op">=</span> np.unique(y)</span>
<span id="cb28-110"><a href="#cb28-110" aria-hidden="true" tabindex="-1"></a>_ <span class="op">=</span> [[<span class="ss">f'</span><span class="sc">{</span>j<span class="sc">:0.4f}</span><span class="ss">'</span> <span class="cf">for</span> j <span class="kw">in</span> np.mean(D[y<span class="op">==</span>i], axis<span class="op">=</span><span class="dv">0</span>)] <span class="cf">for</span> i <span class="kw">in</span> labels]</span>
<span id="cb28-111"><a href="#cb28-111" aria-hidden="true" tabindex="-1"></a>_ <span class="op">=</span> [<span class="st">', '</span>.join(i) <span class="cf">for</span> i <span class="kw">in</span> _]</span>
<span id="cb28-112"><a href="#cb28-112" aria-hidden="true" tabindex="-1"></a>mus <span class="op">=</span> [Markdown(<span class="ss">f'$</span><span class="ch">\\</span><span class="ss">mu_</span><span class="sc">{</span>k<span class="op">+</span><span class="dv">1</span><span class="sc">}</span><span class="ss">=[</span><span class="sc">{</span>i<span class="sc">}</span><span class="ss">]^</span><span class="ch">\\</span><span class="ss">intercal$'</span>) <span class="cf">for</span> k, i <span class="kw">in</span> <span class="bu">enumerate</span>(_)]</span>
<span id="cb28-113"><a href="#cb28-113" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb28-114"><a href="#cb28-114" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-115"><a href="#cb28-115" aria-hidden="true" tabindex="-1"></a>El objetivo es encontrar la variable que no está relacionada con la salida. Una manera de realizar esto es imaginar que si la media en las diferentes variables es la misma en todas las clases entonces esa variable no contribuye a discriminar la clase. En @sec-estimacion-parametros se presentó el procedimiento para obtener las medias que definen $\mathbb P(\mathcal X \mid \mathcal Y)$ para cada clase. El siguiente código muestra el procedimiento para calcular las medías que son <span class="in">`{python} mus[0]`</span>, <span class="in">`{python} mus[1]`</span> y <span class="in">`{python} mus[2]`</span>.</span>
<span id="cb28-116"><a href="#cb28-116" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-119"><a href="#cb28-119" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb28-120"><a href="#cb28-120" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: true</span></span>
<span id="cb28-121"><a href="#cb28-121" aria-hidden="true" tabindex="-1"></a>labels <span class="op">=</span> np.unique(y)</span>
<span id="cb28-122"><a href="#cb28-122" aria-hidden="true" tabindex="-1"></a>mus <span class="op">=</span> [np.mean(D[y<span class="op">==</span>i], axis<span class="op">=</span><span class="dv">0</span>) <span class="cf">for</span> i <span class="kw">in</span> labels]</span>
<span id="cb28-123"><a href="#cb28-123" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb28-124"><a href="#cb28-124" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-125"><a href="#cb28-125" aria-hidden="true" tabindex="-1"></a>Se observa que la media de la tercera variable es aproximadamente igual para las tres clases, teniendo un valor cercano a $2$ tal y como fue generada. Entonces lo que se busca es un procedimiento que permita identificar que las muestras en cada grupo (clase) hayan sido originadas por la misma distribución. Es decir se busca una prueba que indique que las primeras dos variables provienen de diferentes distribuciones y que la tercera provienen de la misma distribución. Es pertinente comentar que este procedimiento no es aplicable para problemas de regresión. </span>
<span id="cb28-126"><a href="#cb28-126" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-127"><a href="#cb28-127" aria-hidden="true" tabindex="-1"></a>Si se puede suponer que los datos provienen de una Distribución Gausiana entonces la prueba a realizar es ANOVA, en caso contrario se puede utilizar su equivalente método no paramétrico como es la prueba Kruskal-Wallis. Considerando que de manera general se desconoce la distribución que genera los datos, entonces se presenta el uso de la segunda prueba.</span>
<span id="cb28-128"><a href="#cb28-128" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-131"><a href="#cb28-131" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb28-132"><a href="#cb28-132" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: false</span></span>
<span id="cb28-133"><a href="#cb28-133" aria-hidden="true" tabindex="-1"></a>res <span class="op">=</span> [kruskal(<span class="op">*</span>[D[y<span class="op">==</span>l, i] <span class="cf">for</span> l <span class="kw">in</span> labels]).pvalue</span>
<span id="cb28-134"><a href="#cb28-134" aria-hidden="true" tabindex="-1"></a>       <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(D.shape[<span class="dv">1</span>])]</span>
<span id="cb28-135"><a href="#cb28-135" aria-hidden="true" tabindex="-1"></a>_ <span class="op">=</span> [<span class="ss">f'</span><span class="sc">{</span>j<span class="sc">:0.4f}</span><span class="ss">'</span> <span class="cf">for</span> j <span class="kw">in</span> res]       </span>
<span id="cb28-136"><a href="#cb28-136" aria-hidden="true" tabindex="-1"></a>ultimo <span class="op">=</span> Markdown(<span class="ss">f'$p=</span><span class="sc">{</span>_[<span class="op">-</span><span class="dv">1</span>]<span class="sc">}</span><span class="ss">$'</span>)</span>
<span id="cb28-137"><a href="#cb28-137" aria-hidden="true" tabindex="-1"></a>_ <span class="op">=</span> <span class="st">', '</span>.join(_)</span>
<span id="cb28-138"><a href="#cb28-138" aria-hidden="true" tabindex="-1"></a>p_f <span class="op">=</span> Markdown(<span class="ss">f'$[</span><span class="sc">{</span>_<span class="sc">}</span><span class="ss">]$'</span>)</span>
<span id="cb28-139"><a href="#cb28-139" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb28-140"><a href="#cb28-140" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-141"><a href="#cb28-141" aria-hidden="true" tabindex="-1"></a>La prueba Kruskal-Wallis identifica si un conjunto de muestras independientes provienen de la misma distribución. La hipótesis nula es que las muestras provienen de la misma distribución. La función <span class="in">`kruskal`</span> implementa esta prueba y recibe tantas muestras como argumentos. En el siguiente código ilustra su uso, se observa que se llama a la función <span class="in">`kruskal`</span> para cada columna en <span class="in">`D`</span> y se calcula su valor $p$. Los valores $p$ obtenidos son: <span class="in">`{python} p_f`</span> lo cual indica que para las primeras dos variables la hipótesis nula se puede rechazar y por el otro lado la hipótesis nula es factible para la tercera variable con un valor <span class="in">`{python} ultimo`</span>.</span>
<span id="cb28-142"><a href="#cb28-142" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-145"><a href="#cb28-145" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb28-146"><a href="#cb28-146" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: true</span></span>
<span id="cb28-147"><a href="#cb28-147" aria-hidden="true" tabindex="-1"></a>res <span class="op">=</span> [kruskal(<span class="op">*</span>[D[y<span class="op">==</span>l, i] <span class="cf">for</span> l <span class="kw">in</span> labels]).pvalue</span>
<span id="cb28-148"><a href="#cb28-148" aria-hidden="true" tabindex="-1"></a>       <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(D.shape[<span class="dv">1</span>])]</span>
<span id="cb28-149"><a href="#cb28-149" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb28-150"><a href="#cb28-150" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-151"><a href="#cb28-151" aria-hidden="true" tabindex="-1"></a>En lugar de discriminar aquellas características que no aportan a modelar los datos, es más común seleccionar las mejores características. Este procedimiento se puede realizar utilizando los valores $p$ de la prueba estadística o cualquier otra función que ordene la importancia de las características.</span>
<span id="cb28-152"><a href="#cb28-152" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-155"><a href="#cb28-155" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb28-156"><a href="#cb28-156" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: false</span></span>
<span id="cb28-157"><a href="#cb28-157" aria-hidden="true" tabindex="-1"></a>X, y <span class="op">=</span> load_diabetes(return_X_y<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb28-158"><a href="#cb28-158" aria-hidden="true" tabindex="-1"></a>f_statistics, p_values <span class="op">=</span> f_regression(X, y)</span>
<span id="cb28-159"><a href="#cb28-159" aria-hidden="true" tabindex="-1"></a>p2_f <span class="op">=</span> Markdown(<span class="ss">f'$</span><span class="sc">{</span>p_values[<span class="dv">1</span>]<span class="sc">:0.4f}</span><span class="ss">$'</span>)</span>
<span id="cb28-160"><a href="#cb28-160" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb28-161"><a href="#cb28-161" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-162"><a href="#cb28-162" aria-hidden="true" tabindex="-1"></a>El procedimiento equivalente a la estadística de Kruskal-Wallis en regresión es calcular la estadística F cuya hipótesis nula es asumir que el coeficiente obtenido en una regresión lineal entre las variables independientes y la dependiente es zero. Esta estadística se encuentra implementada en la función <span class="in">`f_regression`</span>. El siguiente código muestra su uso en el conjunto de datos de diabetes; el cual tiene $10$ variables independientes. En la variable <span class="in">`p_values`</span> se tienen los valores $p$ se puede observar que el valor $p$ correspondiente a la segunda variable tiene un valor de <span class="in">`{python} p2_f`</span>, lo cual hace que esa variable no sea representativa para el problema que se está resolviendo. </span>
<span id="cb28-163"><a href="#cb28-163" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-164"><a href="#cb28-164" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-167"><a href="#cb28-167" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb28-168"><a href="#cb28-168" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: true</span></span>
<span id="cb28-169"><a href="#cb28-169" aria-hidden="true" tabindex="-1"></a>X, y <span class="op">=</span> load_diabetes(return_X_y<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb28-170"><a href="#cb28-170" aria-hidden="true" tabindex="-1"></a>f_statistics, p_values <span class="op">=</span> f_regression(X, y)</span>
<span id="cb28-171"><a href="#cb28-171" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb28-172"><a href="#cb28-172" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-175"><a href="#cb28-175" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb28-176"><a href="#cb28-176" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: false</span></span>
<span id="cb28-177"><a href="#cb28-177" aria-hidden="true" tabindex="-1"></a>sel <span class="op">=</span> SelectKBest(score_func<span class="op">=</span>f_regression, k<span class="op">=</span><span class="dv">9</span>).fit(X, y)</span>
<span id="cb28-178"><a href="#cb28-178" aria-hidden="true" tabindex="-1"></a>vars_f <span class="op">=</span> [<span class="bu">str</span>(i) <span class="cf">for</span> i <span class="kw">in</span> sel.get_support(indices<span class="op">=</span><span class="va">True</span>)]</span>
<span id="cb28-179"><a href="#cb28-179" aria-hidden="true" tabindex="-1"></a>vars_f <span class="op">=</span> Markdown(<span class="ss">f'[</span><span class="sc">{</span><span class="st">", "</span><span class="sc">.</span>join(vars_f)<span class="sc">}</span><span class="ss">]'</span>)</span>
<span id="cb28-180"><a href="#cb28-180" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb28-181"><a href="#cb28-181" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-182"><a href="#cb28-182" aria-hidden="true" tabindex="-1"></a>Un ejemplo que involucra la selección de las variables más representativas mediante una calificación que ordenan la importancia de las mismas se muestra en el siguiente código. Se puede observar que las nueve variables seleccionadas son: <span class="in">`{python} vars_f`</span> descartando la segunda variable que tiene el máximo valor $p.$ </span>
<span id="cb28-183"><a href="#cb28-183" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-184"><a href="#cb28-184" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-187"><a href="#cb28-187" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb28-188"><a href="#cb28-188" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: true</span></span>
<span id="cb28-189"><a href="#cb28-189" aria-hidden="true" tabindex="-1"></a>sel <span class="op">=</span> SelectKBest(score_func<span class="op">=</span>f_regression, k<span class="op">=</span><span class="dv">9</span>).fit(X, y)</span>
<span id="cb28-190"><a href="#cb28-190" aria-hidden="true" tabindex="-1"></a><span class="bu">vars</span> <span class="op">=</span> sel.get_support(indices<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb28-191"><a href="#cb28-191" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb28-192"><a href="#cb28-192" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-193"><a href="#cb28-193" aria-hidden="true" tabindex="-1"></a><span class="fu">### Ventajas y Limitaciones</span></span>
<span id="cb28-194"><a href="#cb28-194" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-195"><a href="#cb28-195" aria-hidden="true" tabindex="-1"></a>Las técnicas vistas hasta este momento requieren de pocos recursos computacionales para su cálculo. Además están basadas en estadísticas que permite saber cuales son las razones de funcionamiento. Estas fortalezas también son origen a sus debilidades, estas técnicas observan en cada paso las variables independientes de manera aislada y no consideran que estas variables pueden interactuar. Por otro lado, la selección es agnóstica del algoritmo de aprendizaje utilizado.</span>
<span id="cb28-196"><a href="#cb28-196" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-197"><a href="#cb28-197" aria-hidden="true" tabindex="-1"></a><span class="fu">## Selección hacia Adelante</span></span>
<span id="cb28-198"><a href="#cb28-198" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-199"><a href="#cb28-199" aria-hidden="true" tabindex="-1"></a>Un procedimiento que pone en el centro del proceso de selección al algoritmo de aprendizaje utilizado es **Selección hacia Adelante** y su complemento que sería **Selección hacia Atrás.** El algoritmo de selección hacia adelante es un procedimiento iterativo que selecciona una variable a la vez guiada por el rendimiento de esta variable cuando es usada en el algoritmo de aprendizaje. Al igual que los procedimientos anteriores este no modifica las características del problema, solamente selecciona las que se consideran relevantes.</span>
<span id="cb28-200"><a href="#cb28-200" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-201"><a href="#cb28-201" aria-hidden="true" tabindex="-1"></a>En selección hacia adelante y hacia atrás se inicia con el conjunto de entrenamiento $\mathcal T = <span class="sc">\{</span>(x_i, y_i)<span class="sc">\}</span>,$ con una función $L$ que mide el rendimiento del algoritmo de aprendizaje $\mathcal H.$ La idea es ir seleccionando de manera iterativa aquellas variables que generan un modelo con mejores capacidades de generalización. Para medir la generalización del algoritmo se pueden realizar de diferentes maneras, una es mediante la división de $\mathcal X$ en dos conjuntos: entrenamiento y validación; y la segunda manera corresponde a utilizar $k$-iteraciones de validación cruzada.  </span>
<span id="cb28-202"><a href="#cb28-202" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-203"><a href="#cb28-203" aria-hidden="true" tabindex="-1"></a>Suponga un conjunto $\pi \subseteq <span class="sc">\{</span>1, 2, \ldots, d<span class="sc">\}</span>$ de tal manera que $\mathcal T_\pi$ solamente cuenta con las variables identificadas en el conjunto $\pi$. Utilizando está notación el algoritmo se puede definir de la siguiente manera. Inicialmente $\pi^0 = \emptyset $, en el siguiente paso $\pi^{j + 1} \leftarrow \pi^j \cup \textsf{arg max}_{\{i \mid i \in \{1, 2, \ldots, d\}, i \notin \pi^j\}} \mathcal P(\mathcal H, \mathcal T_{\pi \cup i}, L)$, donde $\mathcal P$ representa el rendimiento del algoritmo $\mathcal H$ en el subconjunto ${\pi \cup i}$ usando la función de rendimiento $L$. Este proceso continua si $\mathcal P^{j+1} &gt; \mathcal P^j$ donde $\mathcal P^0 = 0$.</span>
<span id="cb28-204"><a href="#cb28-204" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-205"><a href="#cb28-205" aria-hidden="true" tabindex="-1"></a>Es importante mencionar que el algoritmo antes descrito es un algoritmo voraz y que el encontrar el óptimo de este problema de optimización no se garantiza con este tipo de algoritmos. Lo que quiere decir es que el algoritmo encontrará un óptimo local.</span>
<span id="cb28-206"><a href="#cb28-206" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-207"><a href="#cb28-207" aria-hidden="true" tabindex="-1"></a>Ilustrando estos pasos en el conjunto de Breast Cancer Wisconsin (@sec-ejemplo-breast-cancer-wisconsin).</span>
<span id="cb28-208"><a href="#cb28-208" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-211"><a href="#cb28-211" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb28-212"><a href="#cb28-212" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: true</span></span>
<span id="cb28-213"><a href="#cb28-213" aria-hidden="true" tabindex="-1"></a>D, y <span class="op">=</span> load_breast_cancer(return_X_y<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb28-214"><a href="#cb28-214" aria-hidden="true" tabindex="-1"></a>T, G, y_t, y_g <span class="op">=</span> train_test_split(D, y, test_size<span class="op">=</span><span class="fl">0.2</span>)</span>
<span id="cb28-215"><a href="#cb28-215" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb28-216"><a href="#cb28-216" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-217"><a href="#cb28-217" aria-hidden="true" tabindex="-1"></a>El primer paso es medir el rendimiento cuando solamente una variable interviene en el proceso. Como inicialmente $\pi^0 = \emptyset $ entonces solo es necesario generar un clasificador cuando sola una variable está involucrada. El rendimiento de cada variable se guarda en la variable <span class="in">`perf`</span>, se puede observar que el primer ciclo (línea 3) itera por todas las variables en la representación, para cada una se seleccionan los datos solo con esa variable <span class="in">`T1 = T[:, np.array([var])]`</span> después se hacen $k$-iteraciones de validación cruzada y finalmente se guarda el rendimiento. El rendimiento que se está calculando es macro-recall. </span>
<span id="cb28-218"><a href="#cb28-218" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-221"><a href="#cb28-221" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb28-222"><a href="#cb28-222" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: true</span></span>
<span id="cb28-223"><a href="#cb28-223" aria-hidden="true" tabindex="-1"></a>perf <span class="op">=</span> []</span>
<span id="cb28-224"><a href="#cb28-224" aria-hidden="true" tabindex="-1"></a>kfold <span class="op">=</span> KFold(shuffle<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb28-225"><a href="#cb28-225" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> var <span class="kw">in</span> <span class="bu">range</span>(T.shape[<span class="dv">1</span>]):</span>
<span id="cb28-226"><a href="#cb28-226" aria-hidden="true" tabindex="-1"></a>    T1 <span class="op">=</span> T[:, np.array([var])]</span>
<span id="cb28-227"><a href="#cb28-227" aria-hidden="true" tabindex="-1"></a>    perf_inner <span class="op">=</span> []</span>
<span id="cb28-228"><a href="#cb28-228" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> ts, vs <span class="kw">in</span> kfold.split(T1):</span>
<span id="cb28-229"><a href="#cb28-229" aria-hidden="true" tabindex="-1"></a>        gaussian <span class="op">=</span> GaussianNB().fit(T1[ts], y_t[ts])</span>
<span id="cb28-230"><a href="#cb28-230" aria-hidden="true" tabindex="-1"></a>        hy_gaussian <span class="op">=</span> gaussian.predict(T1[vs])</span>
<span id="cb28-231"><a href="#cb28-231" aria-hidden="true" tabindex="-1"></a>        _ <span class="op">=</span> recall_score(y_t[vs], hy_gaussian,</span>
<span id="cb28-232"><a href="#cb28-232" aria-hidden="true" tabindex="-1"></a>                         average<span class="op">=</span><span class="st">'macro'</span>)    </span>
<span id="cb28-233"><a href="#cb28-233" aria-hidden="true" tabindex="-1"></a>        perf_inner.append(_)</span>
<span id="cb28-234"><a href="#cb28-234" aria-hidden="true" tabindex="-1"></a>    perf.append(np.mean(perf_inner))</span>
<span id="cb28-235"><a href="#cb28-235" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb28-236"><a href="#cb28-236" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-239"><a href="#cb28-239" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb28-240"><a href="#cb28-240" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: false</span></span>
<span id="cb28-241"><a href="#cb28-241" aria-hidden="true" tabindex="-1"></a>index <span class="op">=</span> np.argmax(perf)</span>
<span id="cb28-242"><a href="#cb28-242" aria-hidden="true" tabindex="-1"></a>perf_f <span class="op">=</span> Markdown(<span class="ss">f'$</span><span class="sc">{</span>perf[index]<span class="sc">:0.4f}</span><span class="ss">$'</span>)</span>
<span id="cb28-243"><a href="#cb28-243" aria-hidden="true" tabindex="-1"></a>index_f <span class="op">=</span> Markdown(<span class="ss">f'$</span><span class="sc">{</span>index<span class="sc">}</span><span class="ss">$'</span>)</span>
<span id="cb28-244"><a href="#cb28-244" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb28-245"><a href="#cb28-245" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-246"><a href="#cb28-246" aria-hidden="true" tabindex="-1"></a>La @fig-rendimiento-sel muestra el rendimiento de las 30 variables, se observa como una gran parte de las variables proporcionan un rendimiento superior al $0.8$ y la variable que tiene el mejor rendimiento es la que corresponde al índice <span class="in">`{python} index_f`</span> y valor <span class="in">`{python} perf_f`</span>.</span>
<span id="cb28-247"><a href="#cb28-247" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-250"><a href="#cb28-250" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb28-251"><a href="#cb28-251" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: Rendimiento de las variables en la primera iteración del selección hacia adelante</span></span>
<span id="cb28-252"><a href="#cb28-252" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-rendimiento-sel</span></span>
<span id="cb28-253"><a href="#cb28-253" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-fold: true</span></span>
<span id="cb28-254"><a href="#cb28-254" aria-hidden="true" tabindex="-1"></a>sns.barplot(x<span class="op">=</span><span class="bu">list</span>(<span class="bu">range</span>(<span class="bu">len</span>(perf))), y<span class="op">=</span>perf)</span>
<span id="cb28-255"><a href="#cb28-255" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'Variable'</span>)</span>
<span id="cb28-256"><a href="#cb28-256" aria-hidden="true" tabindex="-1"></a>fig <span class="op">=</span> plt.ylabel(<span class="st">'Macro-Recall'</span>)</span>
<span id="cb28-257"><a href="#cb28-257" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb28-258"><a href="#cb28-258" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-259"><a href="#cb28-259" aria-hidden="true" tabindex="-1"></a>El algoritmo de selección hacia atrás y adelante se implementa en </span>
<span id="cb28-260"><a href="#cb28-260" aria-hidden="true" tabindex="-1"></a>la clase <span class="in">`SequentialFeatureSelector`</span> y su uso se observa en las siguientes</span>
<span id="cb28-261"><a href="#cb28-261" aria-hidden="true" tabindex="-1"></a>instrucciones. </span>
<span id="cb28-262"><a href="#cb28-262" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-265"><a href="#cb28-265" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb28-266"><a href="#cb28-266" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: true</span></span>
<span id="cb28-267"><a href="#cb28-267" aria-hidden="true" tabindex="-1"></a>kfolds <span class="op">=</span> <span class="bu">list</span>(KFold(shuffle<span class="op">=</span><span class="va">True</span>).split(T))</span>
<span id="cb28-268"><a href="#cb28-268" aria-hidden="true" tabindex="-1"></a>scoring <span class="op">=</span> make_scorer(<span class="kw">lambda</span> y, hy: recall_score(y, hy,</span>
<span id="cb28-269"><a href="#cb28-269" aria-hidden="true" tabindex="-1"></a>                                         average<span class="op">=</span><span class="st">'macro'</span>))</span>
<span id="cb28-270"><a href="#cb28-270" aria-hidden="true" tabindex="-1"></a>seq <span class="op">=</span> SequentialFeatureSelector(estimator<span class="op">=</span>GaussianNB(),</span>
<span id="cb28-271"><a href="#cb28-271" aria-hidden="true" tabindex="-1"></a>                                scoring<span class="op">=</span>scoring,</span>
<span id="cb28-272"><a href="#cb28-272" aria-hidden="true" tabindex="-1"></a>                                n_features_to_select<span class="op">=</span><span class="st">'auto'</span>,</span>
<span id="cb28-273"><a href="#cb28-273" aria-hidden="true" tabindex="-1"></a>                                cv<span class="op">=</span>kfolds).fit(T, y_t)</span>
<span id="cb28-274"><a href="#cb28-274" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb28-275"><a href="#cb28-275" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-278"><a href="#cb28-278" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb28-279"><a href="#cb28-279" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: false</span></span>
<span id="cb28-280"><a href="#cb28-280" aria-hidden="true" tabindex="-1"></a>index <span class="op">=</span> seq.get_support()</span>
<span id="cb28-281"><a href="#cb28-281" aria-hidden="true" tabindex="-1"></a>index <span class="op">=</span> <span class="st">', '</span>.join([<span class="bu">str</span>(x) <span class="cf">for</span> x <span class="kw">in</span> np.where(index)[<span class="dv">0</span>]])</span>
<span id="cb28-282"><a href="#cb28-282" aria-hidden="true" tabindex="-1"></a>index <span class="op">=</span> Markdown(<span class="ss">f'$[</span><span class="sc">{</span>index<span class="sc">}</span><span class="ss">]$'</span>)</span>
<span id="cb28-283"><a href="#cb28-283" aria-hidden="true" tabindex="-1"></a>seq <span class="op">=</span> SequentialFeatureSelector(estimator<span class="op">=</span>GaussianNB(),</span>
<span id="cb28-284"><a href="#cb28-284" aria-hidden="true" tabindex="-1"></a>                                scoring<span class="op">=</span>scoring,</span>
<span id="cb28-285"><a href="#cb28-285" aria-hidden="true" tabindex="-1"></a>                                direction<span class="op">=</span><span class="st">'backward'</span>,</span>
<span id="cb28-286"><a href="#cb28-286" aria-hidden="true" tabindex="-1"></a>                                n_features_to_select<span class="op">=</span><span class="st">'auto'</span>,</span>
<span id="cb28-287"><a href="#cb28-287" aria-hidden="true" tabindex="-1"></a>                                cv<span class="op">=</span>kfolds).fit(T, y_t)</span>
<span id="cb28-288"><a href="#cb28-288" aria-hidden="true" tabindex="-1"></a>index2 <span class="op">=</span> seq.get_support()</span>
<span id="cb28-289"><a href="#cb28-289" aria-hidden="true" tabindex="-1"></a>index2 <span class="op">=</span> <span class="st">', '</span>.join([<span class="bu">str</span>(x) <span class="cf">for</span> x <span class="kw">in</span> np.where(index2)[<span class="dv">0</span>]])</span>
<span id="cb28-290"><a href="#cb28-290" aria-hidden="true" tabindex="-1"></a>index2 <span class="op">=</span> Markdown(<span class="ss">f'$[</span><span class="sc">{</span>index2<span class="sc">}</span><span class="ss">]$'</span>)</span>
<span id="cb28-291"><a href="#cb28-291" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb28-292"><a href="#cb28-292" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-293"><a href="#cb28-293" aria-hidden="true" tabindex="-1"></a>Al igual que en el algoritmo de <span class="in">`SelectKBest`</span> las variables seleccionadas se pueden observar con la función <span class="in">`get_support`</span>. En este caso las variables seleccionadas son: <span class="in">`{python} index`</span>.</span>
<span id="cb28-294"><a href="#cb28-294" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-295"><a href="#cb28-295" aria-hidden="true" tabindex="-1"></a>Utilizando el parámetro <span class="in">`direction='backward'`</span> para utilizar selección hacia atrás en el mismo conjunto de datos, da como resultado la siguiente selección <span class="in">`{python} index2`</span>. Se puede observar que las variables seleccionadas por los métodos son diferentes. Esto es factible porque los algoritmos solo aseguran llegar a un máximo local y no está garantizado que el máximo local corresponda al máximo global. </span>
<span id="cb28-296"><a href="#cb28-296" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-297"><a href="#cb28-297" aria-hidden="true" tabindex="-1"></a><span class="fu">### Ventajas y Limitaciones</span></span>
<span id="cb28-298"><a href="#cb28-298" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-299"><a href="#cb28-299" aria-hidden="true" tabindex="-1"></a>Una de las ventajas de la selección hacia atrás y adelante es que el algoritmo termina a lo más cuando se han analizado todas las variables, esto eso para un problema en $\mathbb R^d$ se analizarán un máximo de $d$ variables. La principal desventaja es que estos algoritmos son voraces, es decir, toman la mejor decisión en el momento, lo cual tiene como consecuencia que sean capaces de garantizar llegar a un máximo local y en ocasiones este máximo local no corresponde al máximo global. Con el fin de complementar esta idea, en un problema $\mathbb R^d$ se tiene un espacio de búsqueda de $2^d - 1$, es decir, se tiene esa cantidad de diferentes configuraciones que se pueden explorar. En los algoritmos de vistos se observa un máximo de $d$ elementos de ese total.</span>
<span id="cb28-300"><a href="#cb28-300" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-301"><a href="#cb28-301" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-302"><a href="#cb28-302" aria-hidden="true" tabindex="-1"></a><span class="fu">## Selección mediante Modelo {#sec-seleccion-por-modelo}</span></span>
<span id="cb28-303"><a href="#cb28-303" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-304"><a href="#cb28-304" aria-hidden="true" tabindex="-1"></a>Existen algoritmos de aprendizaje supervisado donde sus parámetros indican la importancia que tiene cada característica en el problema. Para ejemplificar esto se utilizará el problema de Diabetes que fue utilizado en la @sec-diabetes. Los datos se obtienen con la siguiente instrucción, adicionalmente se normalizan las características para tener una media $0$ y desviación estándar $1$. </span>
<span id="cb28-305"><a href="#cb28-305" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-308"><a href="#cb28-308" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb28-309"><a href="#cb28-309" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: true</span></span>
<span id="cb28-310"><a href="#cb28-310" aria-hidden="true" tabindex="-1"></a>D, y <span class="op">=</span> load_diabetes(return_X_y<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb28-311"><a href="#cb28-311" aria-hidden="true" tabindex="-1"></a>D <span class="op">=</span> StandardScaler().fit_transform(D)</span>
<span id="cb28-312"><a href="#cb28-312" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb28-313"><a href="#cb28-313" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-314"><a href="#cb28-314" aria-hidden="true" tabindex="-1"></a>El siguiente paso se estiman los parámetros de una regresión lineal (@sec-regresion-ols), también se muestran los valores de los primeros tres coeficientes de la regresión. Se puede observar que el valor absoluto de estos coeficientes está en diferentes rangos, el primer coeficiente es cercano a cero, el segundo está alrededor de 10 y el tercero es superior a 20. Considerando que los datos están normalizados, entonces el valor absoluto indica el efecto que tiene esa variable en el resultado final, en estos tres datos es puede concluir que el tercer coeficiente tiene una mayor contribución que los otros dos coeficientes. </span>
<span id="cb28-315"><a href="#cb28-315" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-318"><a href="#cb28-318" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb28-319"><a href="#cb28-319" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: true</span></span>
<span id="cb28-320"><a href="#cb28-320" aria-hidden="true" tabindex="-1"></a>reg <span class="op">=</span> LinearRegression().fit(D, y)</span>
<span id="cb28-321"><a href="#cb28-321" aria-hidden="true" tabindex="-1"></a>reg.coef_[:<span class="dv">3</span>]</span>
<span id="cb28-322"><a href="#cb28-322" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb28-323"><a href="#cb28-323" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-324"><a href="#cb28-324" aria-hidden="true" tabindex="-1"></a>La clase <span class="in">`SelectFromModel`</span> permite seleccionar las $d$ características que el modelo considera más importante. En el siguiente ejemplo se selecciona las $d=4$ características más importantes; dentro de estas características se encuentra la tercera (i.e., índice 2) cuyo coeficiente se había presentado previamente. </span>
<span id="cb28-325"><a href="#cb28-325" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-328"><a href="#cb28-328" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb28-329"><a href="#cb28-329" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: true</span></span>
<span id="cb28-330"><a href="#cb28-330" aria-hidden="true" tabindex="-1"></a>sel <span class="op">=</span> SelectFromModel(reg, max_features<span class="op">=</span><span class="dv">4</span>,</span>
<span id="cb28-331"><a href="#cb28-331" aria-hidden="true" tabindex="-1"></a>                      threshold<span class="op">=-</span>np.inf, prefit<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb28-332"><a href="#cb28-332" aria-hidden="true" tabindex="-1"></a>np.where(sel.get_support())[<span class="dv">0</span>]</span>
<span id="cb28-333"><a href="#cb28-333" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb28-334"><a href="#cb28-334" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-335"><a href="#cb28-335" aria-hidden="true" tabindex="-1"></a>Habiendo descrito el procedimiento para seleccionar aquellas características más importantes, es necesario encontrar cuál es el número de características que producen el mejor rendimiento. Para realizar esto, se realiza $k$-iteraciones de validación cruzada (@sec-kfold-cross-validation), donde se varía el número de características y en cada caso se mide el rendimiento utilizando la medida $R^2$ (@sec-rendimiento-regresion). </span>
<span id="cb28-336"><a href="#cb28-336" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-339"><a href="#cb28-339" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb28-340"><a href="#cb28-340" aria-hidden="true" tabindex="-1"></a>folds <span class="op">=</span> [x <span class="cf">for</span> x <span class="kw">in</span> KFold(shuffle<span class="op">=</span><span class="va">True</span>).split(D, y)]</span>
<span id="cb28-341"><a href="#cb28-341" aria-hidden="true" tabindex="-1"></a>perf <span class="op">=</span> []</span>
<span id="cb28-342"><a href="#cb28-342" aria-hidden="true" tabindex="-1"></a>dims <span class="op">=</span> <span class="bu">range</span>(<span class="dv">2</span>, D.shape[<span class="dv">1</span>] <span class="op">+</span> <span class="dv">1</span>)</span>
<span id="cb28-343"><a href="#cb28-343" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> dim <span class="kw">in</span> dims:</span>
<span id="cb28-344"><a href="#cb28-344" aria-hidden="true" tabindex="-1"></a>    model <span class="op">=</span> SelectFromModel(reg, max_features<span class="op">=</span>dim,</span>
<span id="cb28-345"><a href="#cb28-345" aria-hidden="true" tabindex="-1"></a>                            threshold<span class="op">=-</span>np.inf, prefit<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb28-346"><a href="#cb28-346" aria-hidden="true" tabindex="-1"></a>    hy <span class="op">=</span> np.empty_like(y)</span>
<span id="cb28-347"><a href="#cb28-347" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> tr, vs <span class="kw">in</span> folds:</span>
<span id="cb28-348"><a href="#cb28-348" aria-hidden="true" tabindex="-1"></a>        Dt <span class="op">=</span> model.transform(D)</span>
<span id="cb28-349"><a href="#cb28-349" aria-hidden="true" tabindex="-1"></a>        m <span class="op">=</span> LinearRegression().fit(Dt[tr], y[tr])</span>
<span id="cb28-350"><a href="#cb28-350" aria-hidden="true" tabindex="-1"></a>        hy[vs] <span class="op">=</span> m.predict(Dt[vs])</span>
<span id="cb28-351"><a href="#cb28-351" aria-hidden="true" tabindex="-1"></a>    perf.append(<span class="bu">dict</span>(r2<span class="op">=</span>r2_score(y, hy), numero<span class="op">=</span>dim))</span>
<span id="cb28-352"><a href="#cb28-352" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb28-353"><a href="#cb28-353" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-354"><a href="#cb28-354" aria-hidden="true" tabindex="-1"></a>La @fig-seleccion-por-modelo muestra el rendimiento ($R^2$) para cada modelo generado, empezando cuando se usan las $2$ características más importantes y terminando con el modelo que tiene todas las características. En la figura se puede observar que el rendimiento mejora considerablemente de $2$ a $3$ características, este sigue mejorando y llega un punto donde incluir más características tiene un impacto negativo en el rendimiento. </span>
<span id="cb28-355"><a href="#cb28-355" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-358"><a href="#cb28-358" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb28-359"><a href="#cb28-359" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: Rendimiento variando el número de características</span></span>
<span id="cb28-360"><a href="#cb28-360" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-seleccion-por-modelo</span></span>
<span id="cb28-361"><a href="#cb28-361" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-fold: true</span></span>
<span id="cb28-362"><a href="#cb28-362" aria-hidden="true" tabindex="-1"></a>df <span class="op">=</span> pd.DataFrame(perf)</span>
<span id="cb28-363"><a href="#cb28-363" aria-hidden="true" tabindex="-1"></a>ax <span class="op">=</span> sns.relplot(df, x<span class="op">=</span><span class="st">'numero'</span>, y<span class="op">=</span><span class="st">'r2'</span>, kind<span class="op">=</span><span class="st">'line'</span>)</span>
<span id="cb28-364"><a href="#cb28-364" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb28-365"><a href="#cb28-365" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-366"><a href="#cb28-366" aria-hidden="true" tabindex="-1"></a><span class="fu">## Análisis de Componentes Principales {#sec-pca}</span></span>
<span id="cb28-367"><a href="#cb28-367" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-368"><a href="#cb28-368" aria-hidden="true" tabindex="-1"></a>Los algoritmos de selección hacia atrás y adelante tiene la característica de requerir un conjunto de entrenamiento de aprendizaje supervisado, por lo que no podrían ser utilizados en problemas de aprendizaje no-supervisado. En esta sección se revisará el uso de Análisis de Componentes Principales (Principal Components Analysis - PCA) para la reducción de dimensión. PCA tiene la firma: $f: \mathbb R^d \rightarrow \mathbb R^m$ donde $m &lt; d$</span>
<span id="cb28-369"><a href="#cb28-369" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-370"><a href="#cb28-370" aria-hidden="true" tabindex="-1"></a>La idea de PCA es buscar una matriz de proyección $W^\intercal \in \mathbb R^{m \times d}$ tal que los elementos de $\mathcal D = <span class="sc">\{</span>x_i<span class="sc">\}</span>$ sea transformados utilizando $z = W^\intercal x$ donde $z \in \mathbb R^m$. El objetivo es que la muestra $z_1$ tenga la mayor variación posible. Es decir, se quiere observar en la primera característica de los elementos transformados la mayor variación; esto se puede lograr de la siguiente manera.</span>
<span id="cb28-371"><a href="#cb28-371" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-372"><a href="#cb28-372" aria-hidden="true" tabindex="-1"></a>Si suponemos que $\mathbf x \sim \mathcal N_d(\mu, \Sigma)$ y $\mathbf w \in \mathbb R^d$ entonces $\mathbf w \cdot \mathbf x \sim \mathcal N(\mathbf w \cdot \mu, \mathbf w^\intercal \Sigma \mathbf w)$ y por lo tanto $\textsf{Var} (\mathbf w \cdot \mathbf x) = \mathbf w^\intercal \Sigma \mathbf w.$</span>
<span id="cb28-373"><a href="#cb28-373" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-374"><a href="#cb28-374" aria-hidden="true" tabindex="-1"></a>Utilizando esta información se puede describir el problema como encontrar $\mathbf w_1$ tal que $\textsf{Var}(\mathbf z_1)$ sea máxima, donde $\textsf{Var}(\mathbf z_1) = \mathbf w_1^\intercal \Sigma \mathbf w_1.$ Dado que en este problema de optimización tiene multiples soluciones, se busca además maximizar bajo la restricción de $\mid\mid \mathbf w_1 \mid\mid = 1.$ Escribiéndolo como un problema de Lagrange quedaría como: $\max_{\mathbf w_1} \mathbf w_1^\intercal \Sigma \mathbf w_1 - \alpha (\mathbf w_1 \cdot \mathbf w_1 - 1).$ Derivando con respecto a $\mathbf w_1$ se tiene que la solución es: $\Sigma \mathbf w_i = \alpha \mathbf w_i$ donde esto se cumple solo si $\mathbf w_1$ es un eigenvector de $\Sigma$ y $\alpha$ es el eigenvalor correspondiente. Para encontrar $\mathbf w_2$ se requiere $\mid\mid \mathbf w_2 \mid\mid = 1$ y que los vectores sean ortogonales, es decir, $\mathbf w_2 \cdot \mathbf w_1 = 0.$ Realizando las operaciones necesarias se encuentra que $\mathbf w_2$ corresponde al segundo eigenvector y así sucesivamente.</span>
<span id="cb28-375"><a href="#cb28-375" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-376"><a href="#cb28-376" aria-hidden="true" tabindex="-1"></a><span class="fu">### Ejemplo - Visualización {#sec-visualizacion-iris}</span></span>
<span id="cb28-377"><a href="#cb28-377" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-378"><a href="#cb28-378" aria-hidden="true" tabindex="-1"></a>Supongamos que deseamos visualizar los ejemplos del problema del iris. Los ejemplos se encuetran en $\mathbb R^4$ entonces para poderlos graficar en $\mathbb R^2$ se requiere realizar una transformación como podría ser Análisis de Componentes Principales.</span>
<span id="cb28-379"><a href="#cb28-379" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-380"><a href="#cb28-380" aria-hidden="true" tabindex="-1"></a>Empezamos por cargar los datos del problema tal y como se muestra en la siguiente instrucción, en la segunda linea se normalizan los datos para tener una media $0$ y desviación estándar $1$, este procedimiento es necesario cuando los valores de las características se encuentran en diferentes escalas. </span>
<span id="cb28-381"><a href="#cb28-381" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-384"><a href="#cb28-384" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb28-385"><a href="#cb28-385" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: true</span></span>
<span id="cb28-386"><a href="#cb28-386" aria-hidden="true" tabindex="-1"></a>D, y <span class="op">=</span> load_iris(return_X_y<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb28-387"><a href="#cb28-387" aria-hidden="true" tabindex="-1"></a>D <span class="op">=</span> StandardScaler().fit_transform(D)</span>
<span id="cb28-388"><a href="#cb28-388" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb28-389"><a href="#cb28-389" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-390"><a href="#cb28-390" aria-hidden="true" tabindex="-1"></a>Habiendo importado los datos el siguiente paso es inicializar la clase de PCA, para esto requerimos especificar el parámetro que indica el número de componentes deseado, dado que el objetivo es representar en $\mathbb R^2$ los datos, entonces el ocupamos dos componentes. La primera línea inicializa la clase de PCA, después, en la segunda línea se hace la proyección. </span>
<span id="cb28-391"><a href="#cb28-391" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-394"><a href="#cb28-394" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb28-395"><a href="#cb28-395" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: true</span></span>
<span id="cb28-396"><a href="#cb28-396" aria-hidden="true" tabindex="-1"></a>pca <span class="op">=</span> decomposition.PCA(n_components<span class="op">=</span><span class="dv">2</span>).fit(D)</span>
<span id="cb28-397"><a href="#cb28-397" aria-hidden="true" tabindex="-1"></a>Xn <span class="op">=</span> pca.transform(D)</span>
<span id="cb28-398"><a href="#cb28-398" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb28-399"><a href="#cb28-399" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-400"><a href="#cb28-400" aria-hidden="true" tabindex="-1"></a>El siguiente código se utiliza para visualizar los datos. El resultado se muestra en la @fig-pca-iris, donde se observa en diferente color cada una de las clases. </span>
<span id="cb28-401"><a href="#cb28-401" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-404"><a href="#cb28-404" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb28-405"><a href="#cb28-405" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: Proyección mediante PCA del problema del Iris</span></span>
<span id="cb28-406"><a href="#cb28-406" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-pca-iris</span></span>
<span id="cb28-407"><a href="#cb28-407" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-fold: true</span></span>
<span id="cb28-408"><a href="#cb28-408" aria-hidden="true" tabindex="-1"></a>data <span class="op">=</span> pd.DataFrame([<span class="bu">dict</span>(x<span class="op">=</span>x, y<span class="op">=</span>y, clase<span class="op">=</span>c)</span>
<span id="cb28-409"><a href="#cb28-409" aria-hidden="true" tabindex="-1"></a>                     <span class="cf">for</span> (x, y), c <span class="kw">in</span> <span class="bu">zip</span>(Xn, y)])</span>
<span id="cb28-410"><a href="#cb28-410" aria-hidden="true" tabindex="-1"></a>fig <span class="op">=</span> sns.relplot(data, kind<span class="op">=</span><span class="st">'scatter'</span>,</span>
<span id="cb28-411"><a href="#cb28-411" aria-hidden="true" tabindex="-1"></a>                  x<span class="op">=</span><span class="st">'x'</span>, y<span class="op">=</span><span class="st">'y'</span>, hue<span class="op">=</span><span class="st">'clase'</span>)</span>
<span id="cb28-412"><a href="#cb28-412" aria-hidden="true" tabindex="-1"></a>fig.tick_params(bottom<span class="op">=</span><span class="va">False</span>, top<span class="op">=</span><span class="va">False</span>, </span>
<span id="cb28-413"><a href="#cb28-413" aria-hidden="true" tabindex="-1"></a>                left<span class="op">=</span><span class="va">False</span>, right<span class="op">=</span><span class="va">False</span>,</span>
<span id="cb28-414"><a href="#cb28-414" aria-hidden="true" tabindex="-1"></a>                labelbottom<span class="op">=</span><span class="va">False</span>, labelleft<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb28-415"><a href="#cb28-415" aria-hidden="true" tabindex="-1"></a>fig.<span class="bu">set</span>(xlabel<span class="op">=</span><span class="va">None</span>, ylabel<span class="op">=</span><span class="va">None</span>)</span>
<span id="cb28-416"><a href="#cb28-416" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb28-417"><a href="#cb28-417" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-418"><a href="#cb28-418" aria-hidden="true" tabindex="-1"></a><span class="fu">## UMAP: Uniform Manifold Approximation and Projection for Dimension Reduction {#sec-umap-visualizacion}</span></span>
<span id="cb28-419"><a href="#cb28-419" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-420"><a href="#cb28-420" aria-hidden="true" tabindex="-1"></a>En esta sección se presenta el uso de UMAP propuesto por @UMAP. UMAP es una técnica no lineal para reducir la dimensión y que con dimensiones bajas ($d=2$) permite tener una visualización de los datos. El ejemplo que se realizará es visualizar mediante UMAP los datos del problema de Dígitos, estos datos se pueden obtener con la siguiente instrucción; en estas instrucciones también se normalizan los datos para tener una media $0$ y desviación estándar $1.$ </span>
<span id="cb28-421"><a href="#cb28-421" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-424"><a href="#cb28-424" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb28-425"><a href="#cb28-425" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: true</span></span>
<span id="cb28-426"><a href="#cb28-426" aria-hidden="true" tabindex="-1"></a>D, y <span class="op">=</span> load_digits(return_X_y<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb28-427"><a href="#cb28-427" aria-hidden="true" tabindex="-1"></a>D <span class="op">=</span> StandardScaler().fit_transform(D)</span>
<span id="cb28-428"><a href="#cb28-428" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb28-429"><a href="#cb28-429" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-430"><a href="#cb28-430" aria-hidden="true" tabindex="-1"></a>Con el objetivo de ilustrar la diferencia entre una técnica de proyección lineal como PCA con respecto a una técnica no lineal, la @fig-pca-digits presenta la proyección del conjunto de Dígitos utilizando PCA. Como se puede observar en la figura, esta proyección no da una separación clara entre grupos, aunque si se puede identificar una estructura, donde algunos números se encuentran en la perifería de la figura. </span>
<span id="cb28-431"><a href="#cb28-431" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-434"><a href="#cb28-434" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb28-435"><a href="#cb28-435" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: Proyección mediante PCA del problema de Dígitos</span></span>
<span id="cb28-436"><a href="#cb28-436" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-pca-digits</span></span>
<span id="cb28-437"><a href="#cb28-437" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-fold: true</span></span>
<span id="cb28-438"><a href="#cb28-438" aria-hidden="true" tabindex="-1"></a>pal <span class="op">=</span> mpl.cm.Paired</span>
<span id="cb28-439"><a href="#cb28-439" aria-hidden="true" tabindex="-1"></a>pca <span class="op">=</span> decomposition.PCA(n_components<span class="op">=</span><span class="dv">2</span>).fit(D)</span>
<span id="cb28-440"><a href="#cb28-440" aria-hidden="true" tabindex="-1"></a>Xn <span class="op">=</span> pca.transform(D)</span>
<span id="cb28-441"><a href="#cb28-441" aria-hidden="true" tabindex="-1"></a>df <span class="op">=</span> pd.DataFrame(Xn, columns<span class="op">=</span>[<span class="st">'x'</span>, <span class="st">'y'</span>])</span>
<span id="cb28-442"><a href="#cb28-442" aria-hidden="true" tabindex="-1"></a>df[<span class="st">'Clase'</span>] <span class="op">=</span> y</span>
<span id="cb28-443"><a href="#cb28-443" aria-hidden="true" tabindex="-1"></a>fig <span class="op">=</span> sns.relplot(df, kind<span class="op">=</span><span class="st">'scatter'</span>,</span>
<span id="cb28-444"><a href="#cb28-444" aria-hidden="true" tabindex="-1"></a>                  legend<span class="op">=</span><span class="st">'full'</span>, palette<span class="op">=</span>pal,</span>
<span id="cb28-445"><a href="#cb28-445" aria-hidden="true" tabindex="-1"></a>                  x<span class="op">=</span><span class="st">'x'</span>, y<span class="op">=</span><span class="st">'y'</span>, hue<span class="op">=</span><span class="st">'Clase'</span>)</span>
<span id="cb28-446"><a href="#cb28-446" aria-hidden="true" tabindex="-1"></a>fig.tick_params(bottom<span class="op">=</span><span class="va">False</span>, top<span class="op">=</span><span class="va">False</span>, </span>
<span id="cb28-447"><a href="#cb28-447" aria-hidden="true" tabindex="-1"></a>                left<span class="op">=</span><span class="va">False</span>, right<span class="op">=</span><span class="va">False</span>,</span>
<span id="cb28-448"><a href="#cb28-448" aria-hidden="true" tabindex="-1"></a>                labelbottom<span class="op">=</span><span class="va">False</span>, labelleft<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb28-449"><a href="#cb28-449" aria-hidden="true" tabindex="-1"></a>fig.<span class="bu">set</span>(xlabel<span class="op">=</span><span class="va">None</span>, ylabel<span class="op">=</span><span class="va">None</span>)</span>
<span id="cb28-450"><a href="#cb28-450" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb28-451"><a href="#cb28-451" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-452"><a href="#cb28-452" aria-hidden="true" tabindex="-1"></a>UMAP genera una proyección que intuitivamente trata de preservar la distancia que existe entre los $K$ vecinos cercanos en la dimensión original como en la dimensión destino. Por este motivo un parámetro importante de UMAP es la cantidad de vecinos, que se tiene el efecto de priorizar la estructura global o local de los datos. En el siguiente código se realiza una exploración de este parámetro generando una visualización para $K=<span class="co">[</span><span class="ot">2, 4, 8, 16</span><span class="co">]</span>.$ El código que realiza está exploración se muestra en las siguientes instrucciones. </span>
<span id="cb28-453"><a href="#cb28-453" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-456"><a href="#cb28-456" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb28-457"><a href="#cb28-457" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: true</span></span>
<span id="cb28-458"><a href="#cb28-458" aria-hidden="true" tabindex="-1"></a><span class="co">#| warning: false</span></span>
<span id="cb28-459"><a href="#cb28-459" aria-hidden="true" tabindex="-1"></a>df <span class="op">=</span> pd.DataFrame()</span>
<span id="cb28-460"><a href="#cb28-460" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> K <span class="kw">in</span> [<span class="dv">2</span>, <span class="dv">4</span>, <span class="dv">8</span>, <span class="dv">16</span>]:</span>
<span id="cb28-461"><a href="#cb28-461" aria-hidden="true" tabindex="-1"></a>    reducer <span class="op">=</span> umap.UMAP(n_neighbors<span class="op">=</span>K)</span>
<span id="cb28-462"><a href="#cb28-462" aria-hidden="true" tabindex="-1"></a>    low_dim <span class="op">=</span> reducer.fit_transform(D)</span>
<span id="cb28-463"><a href="#cb28-463" aria-hidden="true" tabindex="-1"></a>    _ <span class="op">=</span> pd.DataFrame(low_dim, columns<span class="op">=</span>[<span class="st">'x'</span>, <span class="st">'y'</span>])</span>
<span id="cb28-464"><a href="#cb28-464" aria-hidden="true" tabindex="-1"></a>    _[<span class="st">'Clase'</span>] <span class="op">=</span> y</span>
<span id="cb28-465"><a href="#cb28-465" aria-hidden="true" tabindex="-1"></a>    _[<span class="st">'K'</span>] <span class="op">=</span> K</span>
<span id="cb28-466"><a href="#cb28-466" aria-hidden="true" tabindex="-1"></a>    df <span class="op">=</span> pd.concat((df, _))</span>
<span id="cb28-467"><a href="#cb28-467" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb28-468"><a href="#cb28-468" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-469"><a href="#cb28-469" aria-hidden="true" tabindex="-1"></a>La @fig-umap-digits-k muestra las diferentes visualizaciones cuando el número de vecinos de cambia en UMAP, se puede observar como para $K=2$ la visualización no muestra ninguna estructura, pero partiendo de $K=4$ se puede visualizar grupos que corresponden a los diferentes dígitos. </span>
<span id="cb28-470"><a href="#cb28-470" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-473"><a href="#cb28-473" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb28-474"><a href="#cb28-474" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: Proyección mediante UMAP del problema de Dígitos variando el número de vecinos</span></span>
<span id="cb28-475"><a href="#cb28-475" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-umap-digits-k</span></span>
<span id="cb28-476"><a href="#cb28-476" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-fold: true</span></span>
<span id="cb28-477"><a href="#cb28-477" aria-hidden="true" tabindex="-1"></a>fig <span class="op">=</span> sns.relplot(df, kind<span class="op">=</span><span class="st">'scatter'</span>, x<span class="op">=</span><span class="st">'x'</span>, y<span class="op">=</span><span class="st">'y'</span>,</span>
<span id="cb28-478"><a href="#cb28-478" aria-hidden="true" tabindex="-1"></a>                  legend<span class="op">=</span><span class="st">'full'</span>, palette<span class="op">=</span>pal,</span>
<span id="cb28-479"><a href="#cb28-479" aria-hidden="true" tabindex="-1"></a>                  hue<span class="op">=</span><span class="st">'Clase'</span>, col<span class="op">=</span><span class="st">'K'</span>)</span>
<span id="cb28-480"><a href="#cb28-480" aria-hidden="true" tabindex="-1"></a>fig.tick_params(bottom<span class="op">=</span><span class="va">False</span>, top<span class="op">=</span><span class="va">False</span>, </span>
<span id="cb28-481"><a href="#cb28-481" aria-hidden="true" tabindex="-1"></a>                left<span class="op">=</span><span class="va">False</span>, right<span class="op">=</span><span class="va">False</span>,</span>
<span id="cb28-482"><a href="#cb28-482" aria-hidden="true" tabindex="-1"></a>                labelbottom<span class="op">=</span><span class="va">False</span>, labelleft<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb28-483"><a href="#cb28-483" aria-hidden="true" tabindex="-1"></a>fig.<span class="bu">set</span>(xlabel<span class="op">=</span><span class="va">None</span>, ylabel<span class="op">=</span><span class="va">None</span>)            </span>
<span id="cb28-484"><a href="#cb28-484" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb28-485"><a href="#cb28-485" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-486"><a href="#cb28-486" aria-hidden="true" tabindex="-1"></a>Con el objetivo de mostrar con mayor detalle los grupos encontrados, la @fig-umap-digits-k8 muestra la visualización de $K=8$ en ella se puede observar como los ejemplos se agrupan de acuerdo al dígito que representan, también se puede ver la similitud entre los diferentes dígitos y como algunos ejemplos están muy cerca a un grupo al cual no pertenecen. </span>
<span id="cb28-487"><a href="#cb28-487" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-490"><a href="#cb28-490" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb28-491"><a href="#cb28-491" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: Proyección mediante UMAP del problema de dígitos con ocho vecinos</span></span>
<span id="cb28-492"><a href="#cb28-492" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-umap-digits-k8</span></span>
<span id="cb28-493"><a href="#cb28-493" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-fold: true</span></span>
<span id="cb28-494"><a href="#cb28-494" aria-hidden="true" tabindex="-1"></a>fig <span class="op">=</span> sns.relplot(df[df.K<span class="op">==</span><span class="dv">8</span>], kind<span class="op">=</span><span class="st">'scatter'</span>, </span>
<span id="cb28-495"><a href="#cb28-495" aria-hidden="true" tabindex="-1"></a>                  legend<span class="op">=</span><span class="st">'full'</span>, palette<span class="op">=</span>pal,</span>
<span id="cb28-496"><a href="#cb28-496" aria-hidden="true" tabindex="-1"></a>                  x<span class="op">=</span><span class="st">'x'</span>, y<span class="op">=</span><span class="st">'y'</span>, hue<span class="op">=</span><span class="st">'Clase'</span>)</span>
<span id="cb28-497"><a href="#cb28-497" aria-hidden="true" tabindex="-1"></a>fig.tick_params(bottom<span class="op">=</span><span class="va">False</span>, top<span class="op">=</span><span class="va">False</span>, </span>
<span id="cb28-498"><a href="#cb28-498" aria-hidden="true" tabindex="-1"></a>                left<span class="op">=</span><span class="va">False</span>, right<span class="op">=</span><span class="va">False</span>,</span>
<span id="cb28-499"><a href="#cb28-499" aria-hidden="true" tabindex="-1"></a>                labelbottom<span class="op">=</span><span class="va">False</span>, labelleft<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb28-500"><a href="#cb28-500" aria-hidden="true" tabindex="-1"></a>fig.<span class="bu">set</span>(xlabel<span class="op">=</span><span class="va">None</span>, ylabel<span class="op">=</span><span class="va">None</span>)</span>
<span id="cb28-501"><a href="#cb28-501" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
</code><button title="Copiar al portapapeles" class="code-copy-button" data-in-quarto-modal=""><i class="bi"></i></button></pre></div>
</div></div></div></div></div>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
      &nbsp;
    </div>   
    <div class="nav-footer-center">
      &nbsp;
    </div>
    <div class="nav-footer-right">
<p><a href="http://creativecommons.org/licenses/by-sa/4.0/"><img src="https://i.creativecommons.org/l/by-sa/4.0/80x15.png" class="img-fluid"></a> <br> Esta obra está bajo una <a href="http://creativecommons.org/licenses/by-sa/4.0/">Licencia Creative Commons Atribución-CompartirIgual 4.0 Internacional</a></p>
</div>
  </div>
</footer>




</body></html>