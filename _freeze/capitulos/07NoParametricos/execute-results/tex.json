{
  "hash": "b5aa2e8c7ed85a2dff6ecd3b3a34e910",
  "result": {
    "engine": "jupyter",
    "markdown": "# Métodos No Paramétricos {#sec-metodos-no-parametricos}\n\nEl **objetivo** de la unidad es conocer las características de diferentes métodos no paramétricos y aplicarlos para resolver problemas de regresión y clasificación.\n\n\n## Paquetes usados\n\n::: {.cell execution_count=1}\n``` {.python .cell-code}\nfrom sklearn.neighbors import NearestNeighbors,\\\n                              KNeighborsClassifier,\\\n                              KNeighborsRegressor\nfrom sklearn.metrics import pairwise_distances\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.datasets import load_digits, load_diabetes\nfrom scipy.stats import norm\nfrom collections import Counter\nfrom matplotlib import pylab as plt\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\n```\n:::\n\n\n\n\n\n\n::: {.content-visible when-format=\"html\"}\n\n---\n\n\n{{< video https://www.youtube.com/embed/Mn5SweqYZWE width=\"560\" height=\"315\" >}}\n\n\n\n---\n\n:::\n\n## Introducción {#sec-intro-07}\n\nLos métodos paramétricos asumen que los datos provienen de un modelo común, esto da la ventaja de que el problema de estimar el modelo se limita a encontrar los parámetros del mismo, por ejemplo los parámetros de una distribución Gausiana. Por otro lado en los métodos no paramétricos asumen que datos similares se comportan de manera similar, estos algoritmos también se les conoces como algoritmos de memoria o basados en instancias.\n\n## Histogramas {#sec-no-parametricos-histogramas}\n\nEl primer problema que estudiaremos será la estimación no paramétrica de una función de densidad, $f$, recordando que se cuenta con un conjunto $\\mathcal D = \\{x_i\\}$ que es tomado de $f$ y el objetivo es usar $\\mathcal D$ para estimar la función de densidad $\\hat f$. \n\nEl **histograma** es una manera para estimar la función de densidad. Para formar un histograma se divide la línea en $h$ segmentos disjuntos, los cuales se denominan _bins_. El histograma corresponde a una función constante por partes, donde la altura es la proporción de elementos de $\\mathcal D$ que caen en el bin analizado. \n\nSuponiendo que todos los valores en $\\mathcal D$ están en el rango $[0, 1]$, los bins se pueden definir como:\n\n$$\nB_1 = [0, \\frac{1}{m}), B_2=[\\frac{1}{m}, \\frac{2}{m}), \\ldots, B_m=[\\frac{m-1}{m}, 1],\n$$ \n\ndonde $m$ es el número de bins y $h=\\frac{1}{m}$. Se puede definir a $\\hat p_j = \\frac{1}{N} \\sum_{x \\in \\mathcal D} 1( x \\in B_j )$ y $p_j = \\int_{B_j} f(u) du$, donde $p_j$ es la probabilidad del $j$-ésimo bin y $\\hat p_j$ es su estimación. Usando está definición se puede definir la estimación de $f$ como: \n\n$$\n\\hat f(x) = \\sum_{j=1}^N \\frac{\\hat p_j}{h} 1(x \\in B_j).\n$$\n\nCon esta formulación se puede ver la motivación de usar histogramas como estimador de $f$ véase:\n\n$$\n\\mathbb E(\\hat f(x)) = \\frac{\\mathbb E(\\hat p_j)}{h} = \\frac{p_j}{h} = \\frac{\\int_{B_j} f(u) du}{h} \\approx \\frac{hf(x)}{h} = f(x).\n$$\n\n### Selección del tamaño del bin\n\nUna parte crítica para usar un histograma es la selección de $h$ o equivalente el número de bins del estimador. Utilizando el método descrito en @Wasserman2004, el cual se basa en minimizar el riesgo haciendo una validación cruzada, obteniendo la siguiente ecuación:\n\n$$\n\\hat J(h) = \\frac{2}{(N-1) h} - \\frac{N+1}{(N-1) h} \\sum_{j=1}^N {\\hat p}^2_j.\n$$\n\nPara ilustrar el uso de la ecuación de minimización del riesgo se utilizará en el ejemplo utilizado en @Wasserman2004. Los datos se pueden descargar de [http://www.stat.cmu.edu/~larry/all-of-statistics/=Rprograms/a1882_25.dat](http://www.stat.cmu.edu/~larry/all-of-statistics/=Rprograms/a1882_25.dat).\n\nEl primer paso es leer el conjunto de datos, dentro del ejemplo usado en @Wasserman2004 se eliminaron todos los datos menores a $0.2$, esto se refleja en la última línea. \n\n::: {.cell execution_count=4}\n``` {.python .cell-code}\nD = np.r_[[list(map(float, x.strip().split())) \n          for x in open(\"a1882_25.dat\").readlines()]]\nD = D[:, 2]\nD = D[D <= 0.2]\n```\n:::\n\n\nHaciendo un paréntesis en el ejemplo, para poder calcular $\\hat p_j$ es necesario calcular el histograma; dado que los valores están normalizados podemos realizar el histograma utilizando algunas funciones de `numpy` y librerías tradicionales. \n\nPara el ilustrar el método para generar el histograma se genera un histograma con 100 bins (primera línea). El siguiente paso (segunda línea) es encontrar los límites de los bins, para este proceso se usa la función `np.linspace`. En la tercera línea se encuentra el bin de cada elemento, con la característica que `np.searchsorted` regresa $0$ si el valor es menor que el límite inferior y el tamaño del arreglo si es mayor. Entonces las líneas $4$ y $5$ se encargan de arreglar estas dos características. Finalmente se cuenta el número de elementos que pertenecen a cada bin con la ayuda de la clase `Counter`.\n\n::: {.cell execution_count=5}\n``` {.python .cell-code}\nm = 100\nlimits = np.linspace(D.min(), D.max(), m + 1)\n_ = np.searchsorted(limits, D, side='right')\n_[_ == 0] = 1\n_[_ == m + 1] = m\np_j = Counter(_)\n```\n:::\n\n\nRealizando el procedimiento anterior se obtiene el histograma presentado en la @fig-no-param-hist\n\n::: {.cell execution_count=6}\n\n::: {.cell-output .cell-output-display}\n![Histograma](07NoParametricos_files/figure-pdf/fig-no-param-hist-output-1.pdf){#fig-no-param-hist}\n:::\n:::\n\n\nUniendo estos elementos se puede definir una función de riesgo de la siguiente manera\n\n::: {.cell execution_count=7}\n``` {.python .cell-code}\ndef riesgo(D, m=10):\n    \"\"\"Riesgo de validación cruzada de histograma\"\"\"\n    N = D.shape[0]\n    limits = np.linspace(D.min(), D.max(), m + 1)\n    h = limits[1] - limits[0]\n    _ = np.searchsorted(limits, D, side='right')\n    _[_ == 0] = 1\n    _[_ == m + 1] = m\n    p_j = Counter(_)\n    cuadrado = sum([(x / N)**2 for x in p_j.values()])\n    return (2 / ((N - 1) * h)) - ((N + 1) * cuadrado / ((N - 1) * h))\n```\n:::\n\n\ndonde las partes que no han sido descritas solamente implementan la ecuación $\\hat J(h)$.\n\nFinalmente se busca el valor $h$ que minimiza la ecuación, iterando por diferentes valores de $m$ se obtiene la @fig-no-param-riesgo donde se observa la variación del riesgo con diferentes niveles de bin. \n\n::: {.cell execution_count=8}\n\n::: {.cell-output .cell-output-display}\n![Riesgo](07NoParametricos_files/figure-pdf/fig-no-param-riesgo-output-1.pdf){#fig-no-param-riesgo}\n:::\n:::\n\n\n## Estimador de Densidad por Kernel\n\nComo se puede observar el histograma es un estimador discreto, otro estimador muy utilizado que cuenta con la característica de ser suave es el estimador de densidad por kernel, $K$, el cual está definido de la siguiente manera.\n\n$$\n\\hat f(x) = \\frac{1}{hN} \\sum_{w \\in \\mathcal D} K(\\frac{x - w}{h}),\n$$\n\ndonde el kernel $K$ podría ser $K(x) = \\frac{1}{\\sqrt{2\\pi}} \\exp [-\\frac{x^2}{2}],$ con parámetros $\\mu=0$ y $\\sigma=1$. La @fig-no-param-kernel muestra la estimación obtenida, con $h=0.003$, en los datos utilizados en el ejemplo del histograma. \n\n::: {.cell execution_count=9}\n\n::: {.cell-output .cell-output-display}\n![Estimación por Kernel](07NoParametricos_files/figure-pdf/fig-no-param-kernel-output-1.pdf){#fig-no-param-kernel}\n:::\n:::\n\n\n### Caso multidimensional\n\nPara el caso multidimensional el estimador quedaría como \n\n$$\n\\hat f(\\mathbf x) = \\frac{1}{h^dN} \\sum_{\\mathbf w \\in \\mathcal D} K(\\frac{\\mathbf x - \\mathbf w}{h}),\n$$\n\ndonde $d$ corresponde al número de dimensiones. Un kernel utilizado es:\n\n$$\nK(\\mathbf x) = (\\frac{1}{\\sqrt{2\\pi}})^d \\exp [- \\frac{\\mid\\mid \\mathbf x \\mid\\mid ^2}{2}].\n$$\n\n## Estimador de Densidad por Vecinos Cercanos\n\nDado un conjunto $\\mathcal D=(x_1, \\ldots, x_N)$, es decir, donde se conoce la posición de $x$ en $\\mathcal D$ y una medida de distancia $d$, los $k$ vecinos cercanos a $x$, $\\textsf{kNN}(x)$, se puede calcular ordenando $\\mathcal D$ de la siguiente manera. Sea $(\\pi_1, \\pi_2, \\ldots, \\pi_N)$ la permutación tal que $x_{\\pi_1}=\\textsf{arg min}_{w \\in \\mathcal D} d(x, w)$, donde $w_{\\pi_1} \\in \\mathcal D$, $\\pi_2$ corresponde al segundo índice menor, y así sucesivamente. Usando esta notación los $k$ vecinos corresponden a $\\textsf{kNN}(x)=(x_{\\pi_1}, x_{\\pi_2}, \\ldots, x_{\\pi_k}).$ \n\nUna maneara intuitiva de definir $h$ sería en lugar de pensar en un valor constante para toda la función, utilizar la distancia que existe con el $k$ vecino mas cercano, es decir, $h=d(w_{\\pi_k}, x)=d_k(x)$, donde $w_{\\pi_k} \\in \\mathcal D$. Remplazando esto en el estimado de densidad por kernel se obtiene:\n\n$$\n\\hat f(x) = \\frac{1}{d_k(x) N} \\sum_{w \\in \\mathcal D} K(\\frac{x - w}{d_k(x)}).\n$$\n\nUtilizando los datos anteriores el estimador por vecinos cercanos, con $k=50$, quedaría como:\n\n::: {.cell execution_count=10}\n\n::: {.cell-output .cell-output-display}\n![Estimación por Vecinos Cercanos](07NoParametricos_files/figure-pdf/fig-no-param-vecinos-output-1.pdf){#fig-no-param-vecinos}\n:::\n:::\n\n\n## Clasificador de vecinos cercanos\n\nEl clasificador de vecinos cercanos es un clasificador simple de entender, la idea es utilizar el conjunto de entrenamiento y una función de distancia para asignar la clase de acuerdo a los k-vecinos más cercanos al objeto deseado.\n\nUtilizando la notación $kNN(x)$ se define el volumen de $kNN(x)$ como $V(x)$ y $N_c(x)=\\sum_{x_{\\pi} \\in \\textsf{kNN}(x)} 1(y_\\pi=c)$ donde $y_\\pi$ es la salida asociada a $x_\\pi$. $N_c(x)$ corresponde al número de vecinos de $x$ que pertenecen a la clase $c$. Con esta notación se define la verosimilitud como:\n\n$$\n\\mathbb P(\\mathcal X=x \\mid \\mathcal Y=c) = \\frac{N_c(x)}{N_c V(x)},\n$$\n\ndonde $N_c$ es el número de elementos en $\\mathcal D$ de la clase $c.$\n\nUtilizando el Teorema de Bayes y sabiendo que $\\mathcal P(Y=c)=\\frac{N_c}{N}$ la probabilidad a posteriori queda como:\n\n$$\n\\begin{split}\n\\mathcal P(\\mathcal Y=c \\mid \\mathcal X=x) &= \\frac{\\frac{N_c(x)}{N_c V(x)} \\frac{N_c}{N}}{\\sum_u \\frac{N_u(x)}{N_u V(x)} \\frac{N_u}{N}} \\\\\n&= \\frac{N_c(x)}{\\sum_u N_u(x)} \\\\\n&= \\frac{N_c(x)}{k},\n\\end{split}\n$$\n\ndonde $\\sum_u N_u(x)=k$ porque $N_u(x)$ corresponde al número de elementos de $\\textsf{kNN}(x)$ que pertenecen a la clase $u$ y en total se seleccionan $k$ elementos. \n\n### Implementación\n\nEl clasificador de vecinos cercanos tiene una implementación directa, aunque ineficiente, cuando el número de ejemplos en el conjunto de entrenamiento es grande. Esta implementación se ejemplifica con los datos de dígitos que se cargan y se dividen en el conjunto de entrenamiento y prueba de la siguiente manera. \n\n::: {.cell execution_count=11}\n``` {.python .cell-code}\nX, y = load_digits(return_X_y=True)\nT, G, y_t, y_g = train_test_split(X, y, test_size=0.2)\n```\n:::\n\n\n#### $\\textsf{kNN}$\n\nLo primero que se realiza es la función para calcular los $\\textsf{kNN}(x)$ esto se puede generando una función `kNN` que recibe de parámetros $x$, el conjunto $\\mathcal D$, la cantidad de vecinos ($k$) y la distancia. \n\nEl código de la función `kNN` se muestra a continuación, donde en la primera línea se convierte a $x$ en un arreglo de dos dimensiones. Esto tiene el objetivo de generar un código que pueda buscar los $k$ vecinos cercanos de un conjunto de puntos. Por ejemplo, se podría calcular los vecinos cercanos de todo el conjunto $\\mathcal G.$\n\nLa segunda línea calcula los vecinos cercanos usando la función `argsort` lo único que se tiene que conocer es el eje donde se va a realizar la operación que en este caso el $0.$ La transpuesta es para regresar el índice de los vecinos en cada renglón. \n\n::: {.cell execution_count=12}\n``` {.python .cell-code}\ndef kNN(x, D, k=1, d=lambda x, y: pairwise_distances(x, y)):\n    x = np.atleast_2d(x)\n    return (d(D, x).argsort(axis=0))[:k].T\n```\n:::\n\n\nEn este momento es importante mencionar que el problema de los $k$ vecinos cercanos tiene muchas aplicaciones además del los algoritmos de aprendizaje supervisado que se verán en esta unidad. Por ejemplo, cuando uno tiene una colección de objetos que podrían ser documentos, videos, fotografías o cualquier objeto, este problema permite encontrar los objetos más cercanos a un objeto dado. Lo que se desea es que el algoritmo regrese el resultado lo antes posible y por ese motivo no se puede utilizar el algoritmo que se acaba de mencionar dado que compara $x$ contra todos los elementos de $\\mathcal D.$ El área que estudia este tipo de problemas es el área de Recuperación de Información.  \n\nPor ejemplo, el siguiente código calcula los cinco vecinos más cercanos de los tres primeros elementos de $\\mathcal G.$\n\n::: {.cell execution_count=13}\n``` {.python .cell-code}\nkNN(G[:3], T, k=5)\n```\n\n::: {.cell-output .cell-output-display execution_count=13}\n```\narray([[ 748,  885,  855,  486,  844],\n       [ 129, 1263,    0,    8,  801],\n       [1320, 1261,   99,  309,   53]])\n```\n:::\n:::\n\n\nLa manera más sencilla de crear el clasificador de vecinos cercanos es utilizando un método exhaustivo en el cálculo de distancia. Como se comentó, existen métodos más eficientes y la clase `NearestNeighbors` implementa dos de ellos adicionales al método exhaustivo. Por ejemplo, el siguiente código realiza el procedimiento equivalente al ejemplo visto previamente. \n\n::: {.cell execution_count=14}\n``` {.python .cell-code}\nknn = NearestNeighbors(n_neighbors=5).fit(T)\nknn.kneighbors(G[:3], return_distance=False)\n```\n\n::: {.cell-output .cell-output-display execution_count=14}\n```\narray([[ 748,  885,  855,  486,  844],\n       [ 129, 1263,    0,    8,  801],\n       [1320, 1261,   99,  309,   53]])\n```\n:::\n:::\n\n\n#### $N_c(x)$\n\nEl clasificador se basa en la función $N_c(x)$, esta función se implementa conociendo las etiquetas y $\\textsf{kNN}(x)$. Aunque $N_c(x)$ requiere el parámetro de la clase, la función calculará $N_c(x)$ para todas las clases. La función `N_c` recibe de parámetros todos los parámetros de `kNN` y además requiere la clases de cada elemento de $\\mathcal D$ estas clases se dan como un arreglo adicional. El siguiente código muestra la función, donde en la primera línea se calcula los $k$ vecinos y después se transforman los índices a las clases correspondientes, el resultado es guardado en la variable `knn`. La segunda línea usa la clase `Counter` para contar la frecuencia de cada clase en cada ejemplo dado en `x`. \n\n::: {.cell execution_count=15}\n``` {.python .cell-code}\ndef N_c(x, D, clases, k=1,\n        d=lambda x, y: pairwise_distances(x, y)):\n    knn = clases[kNN(x, D, k=k, d=d)]\n    return [Counter(x) for x in knn]\n```\n:::\n\n\nPor ejemplo, la siguiente instrucción calcula $N_c(x)$ para todos los datos en $\\mathcal G$ usando $k=5.$\n\n::: {.cell execution_count=16}\n``` {.python .cell-code}\nnc = N_c(G, T, y_t, k=5)\n```\n:::\n\n\n\n\nEl elemento en el índice 100,  tiene el siguiente resultado Counter({7: 5}), que indica que la clase $7$, fue vista $5$. El error de este algoritmo en el conjunto de prueba es $0.0083$, calculado con las siguientes instrucciones. Se observa que la primera línea genera las predicciones usando la función `most_common` y a continuación se calcula el error.  \n\n::: {.cell execution_count=18}\n``` {.python .cell-code}\nhy = np.array([x.most_common(n=1)[0][0] for x in nc])\nerror = (y_g != hy).mean()\n```\n:::\n\n\nUna implementación del clasificador de vecinos cercanos usando métodos eficientes para calcular $\\textsf{kNN}$ se encuentra en la clase `KNeighborsClassifier` la cual se puede utilizar de la siguiente manera. \n\n::: {.cell execution_count=19}\n``` {.python .cell-code}\nkcl = KNeighborsClassifier().fit(T, y_t)\nhy = kcl.predict(G)\n```\n:::\n\n\n## Regresión {#sec-regresion}\n\nLa idea de utilizar vecinos cercanos no es solamente para problemas de clasificación, en problemas de regresión se puede seguir un razonamiento equivalente, el único cambio es en la función $N_c(x)$ donde en lugar de calcular la frecuencia de las clases de los vecinos cercanos a $x$ se hace un promedio (pesado) de la respuesta de cada uno de los vecinos cercanos. \n\nPara ilustrar esta adecuación en problemas de regresión se utiliza el conjunto de datos de diabetes, estos datos y los conjuntos se obtienen con las siguientes instrucciones. \n\n::: {.cell execution_count=20}\n``` {.python .cell-code}\nX, y = load_diabetes(return_X_y=True)\nT, G, y_t, y_g = train_test_split(X, y, test_size=0.2)\n```\n:::\n\n\nSe puede observar en la función `regresion` que la diferencia con clasificación es que se calcula el promedio, en lugar de contar la frecuencia de las clases. \n\n::: {.cell execution_count=21}\n``` {.python .cell-code}\ndef regresion(x, D, respuesta, k=1,\n              d=lambda x, y: pairwise_distances(x, y)):\n    knn = respuesta[kNN(x, D, k=k, d=d)]\n    return knn.mean(axis=1)\n```\n:::\n\n\n\n\nLa media del error absoluto en el conjunto $\\mathcal G$ es $41.5663$ calculado con las siguientes instrucciones.\n\n```python\nhy = regresion(G, T, y_t, k=5)\nerror = np.fabs(y_g - hy).mean()\n```\n\nLa clase equivalente a `KNeighborsClassifier` para regresión es `KNeighborsRegressor` la cual se puede utilizar asi. \n\n::: {.cell execution_count=23}\n``` {.python .cell-code}\nkrg = KNeighborsRegressor().fit(T, y_t)\nhy = krg.predict(G)\n```\n:::\n\n\n",
    "supporting": [
      "07NoParametricos_files/figure-pdf"
    ],
    "filters": []
  }
}