{
  "hash": "41d6198396bd3e602e607604cd45a623",
  "result": {
    "engine": "jupyter",
    "markdown": "# Rendimiento\n\nEl **objetivo** es contrastar las características de diferentes medidas de rendimiento en aprendizaje supervisado así como simular un procedimiento de aprendizaje supervisado. \n\n## Paquetes usados {.unnumbered}\n\n::: {#22959227 .cell execution_count=1}\n``` {.python .cell-code}\nfrom EvoMSA.model import GaussianBayes\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.datasets import load_breast_cancer,\\\n                             load_diabetes\nfrom sklearn.metrics import recall_score\nfrom sklearn.model_selection import train_test_split, KFold\nfrom sklearn import metrics\nimport numpy as np\nimport pandas as pd\nfrom matplotlib import pylab as plt\nimport seaborn as sns\n```\n:::\n\n\n\n\n::: {.content-visible when-format=\"html\"}\n\n---\n\n\n\n\n\n\n\n{{< video https://www.youtube.com/embed/NUbGplcMRmw width=\"560\" height=\"315\" >}}\n\n\n\n\n\n\n\n\n\n\n\n\n\n---\n\n:::\n\n## Introducción {#sec-intro-04}\n\nEs importante conocer el rendimiento del algoritmo de aprendizaje computacional desarrollado. En aprendizaje supervisado la medición se hace mediante el conjunto de prueba, $\\mathcal G$, mientras que en aprendizaje no supervisado es posible utilizar el conjunto de entrenamiento $\\mathcal T$ o utilizar un conjunto de prueba. Es importante notar que aunque en el proceso de entrenamiento puede usar una función de rendimiento para estimar o encontrar el algoritmo que modela los datos, es importante complementar esta medición con otras funciones de rendimiento. Esta unidad describe algunas de las medidas más utilizadas para medir el rendimiento de algoritmos de clasificación y regresión. \n\n## Clasificación {#sec-clasificacion}\n\nEn clasificación existen diferentes medidas de rendimiento, algunas de ellas son exactitud (*accuracy*), precisión (*precision*), recall, y $F_1$, entre otras. @10.1145/2808194.2809449 describe de manera axiomática algunas de estas medidas y se dan recomendaciones en general sobre medidas de rendimiento para clasificadores. \n\nVarias de las medidas de rendimiento toman como insume la **Tabla de Confusión** (@tbl-confusion), la cual contiene la información del proceso de clasificación. La siguiente tabla muestra la estructura de esta tabla para un problema binario, donde se tiene una clase positiva identificada con $p$ y una clase negativa ($n$). La variable $\\mathcal Y$ indica las clases reales y la variable $\\mathcal{\\hat Y}$ representa la estimación (predicción) hecha por el clasificador. Adicionalmente, la tabla se puede extender a $K$ clases siguiendo la misma estructura; la diagonal contienen los elementos correctamente identificados y los elementos fuera de la diagonal muestra los errores. \n\n|                |$\\mathcal{\\hat Y}=p$|$\\mathcal{\\hat Y}=n$|\n|----------------|----------------------|----------------------|\n|$\\mathcal Y=p$|Verdaderos Pos.       |Falsos Neg.           |\n|$\\mathcal Y=n$|Falsos Pos.           |Verdaderos Neg.       |\n\n: Tabla de Confusión {#tbl-confusion}\n\nLa tabla se puede ver como valores nominales, es decir contar el número de ejemplos clasificados como verdaderos positivos o como proporción de tal manera que las cuatro celdas sumen $1$. En esta descripción se asume que son proporcionen, esto porque se seguirá una interpretación probabilística descrita en [este artículo](https://link.springer.com/chapter/10.1007/978-3-540-31865-1_25) para presentar las diferentes medidas de rendimiento.\n\nViendo la @tbl-confusion como una proporción y combinando con la interpretación probabilística la tabla quedaría de la siguiente manera.\n\n|                |$\\mathcal{\\hat Y}=p$|$\\mathcal{\\hat Y}=n$|\n|----------------|----------------------|----------------------|\n|$\\mathcal Y=p$|$\\mathbb P(\\mathcal Y=p, \\mathcal{\\hat Y=p})$|$\\mathbb P(\\mathcal Y=p, \\mathcal{\\hat Y=n})$|\n|$\\mathcal Y=n$|$\\mathbb P(\\mathcal Y=n, \\mathcal{\\hat Y=p})$|$\\mathbb P(\\mathcal Y=n, \\mathcal{\\hat Y=n})$|\n\n: Tabla de Confusión como Proporción {#tbl-confusion-prob}\n\nPartiendo de la @tbl-confusion-prob se puede calcular la probabilidad marginal de cualquier variable y también las probabilidades condicionales, por ejemplo $\\mathbb P(\\mathcal Y=p) = \\sum_k \\mathbb P(\\mathcal Y=p, \\mathcal{\\hat Y=k})$ que es la suma de los elementos del primer renglón de la tabla anterior.\n\n### Error {#sec-error }\n\nSe empieza la descripción con el error de clasificación (@sec-error-clasificacion) el cual es la proporción de errores y se puede definir como\n\n$$\n\\textsf{error}(\\mathcal Y, \\mathcal{\\hat Y}) = 1 -  \\textsf{accuracy}(\\mathcal Y, \\mathcal{\\hat Y}).\n$$\n\n### Exactitud (*Accuracy*) {#sec-accuracy}\n\nEl error se define mediante la exactitud. La exactitud es la proporción de ejemplos correctamente clasificados, utilizando la notación de la tabla de confusión quedaría como:\n\n$$\n\\textsf{accuracy}(\\mathcal Y, \\mathcal{\\hat Y}) = \\mathbb P(\\mathcal Y=p, \\mathcal{\\hat Y}=p) + \\mathbb P(\\mathcal Y=n, \\mathcal{\\hat Y}=n).\n$$\n\nUna manera equivalente de ver la exactitud es utilizando la probabilidad condicional, es decir, \n\n$$\n\\begin{split}\n\\textsf{accuracy}(\\mathcal Y, \\mathcal{\\hat Y}) &= \\mathbb P( \\mathcal{\\hat Y}=p \\mid \\mathcal Y=p)\\mathbb P(\\mathcal Y=p)\\\\ \n&+ \\mathbb P(\\mathcal{\\hat Y}=n \\mid \\mathcal Y=n)\\mathbb P(\\mathcal Y=n).\n\\end{split}\n$$\n\nEsta manera ayuda a entender el caso cuando se tiene una clase con muchos ejemplos, e.g., $\\mathbb P(\\mathcal Y=p) \\gg \\mathbb P(\\mathcal Y=n),$ en ese caso se ve que la exactitud está dominado por el primer término, i.e., $\\mathbb P( \\mathcal{\\hat Y}=p \\mid \\mathcal Y=p)\\mathbb P(\\mathcal Y=p).$ En este caso, la manera trivial de optimizar la exactitud es crear un clasificador que siempre regrese la clase $p.$ Por esta razón la exactitud no es una medida adecuada cuando las clases son desbalanciadas, es buena medida cuando $\\mathbb P(\\mathcal Y=p) \\approx \\mathbb P(\\mathcal Y=n).$\n\n### Coberturba (*Recall*) {#sec-recall}\n\nLa siguiente medida de rendimiento es el recall, este calcula la probabilidad de ejemplos correctamente clasificados como $p$ dados todos los ejemplos que se tienen de la clase $p$. En base a esta ecuación se puede observar que un algoritmo trivial con el máximo valor de recall solamente tiene que predecir como clase $p$ todos los elementos. \n\nLa segunda ecuación ayuda a medir en base de la tabla de confusión.\n\n$$\n\\begin{split}\n\\textsf{recall}_p(\\mathcal Y, \\mathcal{\\hat Y}) &= \\mathbb P(\\mathcal{\\hat Y}=p \\mid \\mathcal{Y}=p) \\\\\n&= \\frac{\\mathbb P(\\mathcal{\\hat Y}=p, \\mathcal{Y}=p)}{\\mathbb P(\\mathcal Y=p)}\n\\end{split}\n$$ {#eq-recall}\n\n### Precisión (*Precision*) {#sec-precision}\n\nLa precisión complementa el recall, al calcular la probabilidad de los ejemplos correctamente clasificados como $p$ dadas las predicciones de los ejemplos. Es decir, en la probabilidad condicional se observa que se conocen las predicciones positivas y de esas predicciones se mide si estas son correctamente clasificadas. Basándose en esto, se puede ver que una manera de generar un algoritmo competitivo en esta media corresponde a predecir la clase solo cuando exista una gran seguridad de la clase. \n\n$$\n\\begin{split}\n\\textsf{precision}_p(\\mathcal Y, \\mathcal{\\hat Y}) &= \\mathbb P(\\mathcal Y=p \\mid \\mathcal{\\hat Y}=p)\\\\\n&= \\frac{\\mathbb P(\\mathcal Y=p, \\mathcal{\\hat Y}=p)}{\\mathbb P(\\mathcal{\\hat Y}=p)}\n\\end{split}\n$$ {#eq-precision}\n\n### $F_\\beta$ {#sec-f1}\n\nFinalmente, una manera de combinar el recall (@eq-recall) con la precisión (@eq-precision) es la medida $F_\\beta$, es probable que esta medida se reconozca más cuando $\\beta=1$. La idea de $\\beta$ es ponderar el peso que se le quiere dar a la precisión con respecto al recall.\n\n$$\nF^p_\\beta(\\mathcal Y, \\mathcal{\\hat Y}) = (1 + \\beta^2) \\frac{\\textsf{precision}_p(\\mathcal Y, \\mathcal{\\hat Y}) \\cdot \\textsf{recall}_p(\\mathcal Y, \\mathcal{\\hat Y})}{\\beta^2 \\cdot \\textsf{precision}_p(\\mathcal Y, \\mathcal{\\hat Y}) + \\textsf{recall}_p(\\mathcal Y, \\mathcal{\\hat Y})}\n$$ {#eq-f1}\n\n### Medidas Macro {#sec-macro}\n\nEn las definiciones de precisión (@eq-precision), recall (@eq-recall) y $F_\\beta$ (@eq-f1) se ha usado un subíndice y superíndice con la letra $p$ esto es para indicar que la medida se está realizando con respecto a la clase $p$. Esto ayuda también a ilustrar que en un problema de $K$ clases se tendrán $K$ diferentes medidas de precisión, recall y $F_\\beta;$ cada una de esas medidas corresponde a cada clase. \n\nEn ocasiones es importante tener solamente una medida que englobe el rendimiento en el caso de los tres rendimientos que se han mencionado, se puede calcular su versión macro que es la media de la medida. Esto es para un problema de $K$ clases la precisión, recall y $F_\\beta$ se definen de la siguiente manera.\n\n$$\n\\textsf{macro-precision}(\\mathcal Y, \\mathcal{\\hat Y}) =  \\frac{1}{K}\\sum_{k} \\textsf{precision}_k(\\mathcal Y, \\mathcal{\\hat Y}),\n$$ \n\n$$\n\\textsf{macro-recall}(\\mathcal Y, \\mathcal{\\hat Y}) =  \\frac{1}{K}\\sum_{k} \\textsf{recall}_k(\\mathcal Y, \\mathcal{\\hat Y}),\n$$ \n\n$$\n\\textsf{macro-}F_\\beta(\\mathcal Y, \\mathcal{\\hat Y}) =  \\frac{1}{K}\\sum_{k} F^k_\\beta(\\mathcal Y, \\mathcal{\\hat Y}).\n$$ \n\n### Entropía Cruzada {#sec-entropia-cruzada}\n\n\nUna función de costo que ha sido muy utilizada en redes neuronales y en particular en aprendizaje profundo es la **Entropía Cruzada** (Cross Entropy) que para una distribución discreta se define como: $H(P, Q) = - \\sum_x P(x) \\log Q(x)$. \n\nPara cada ejemplo $x$ se tiene $\\mathbb P(\\mathcal Y=k \\mid \\mathcal X=x)$ y el clasificador predice $\\mathbb{\\hat P}(\\mathcal Y=k \\mid \\mathcal X=x).$ Utilizando estas definiciones se puede decir que $P=\\mathbb P$ y $Q=\\mathbb{\\hat P}$ en la definición de entropía cruzada; entonces\n\n$$\n\\begin{split}\nH(\\mathbb P(\\mathcal Y \\mid \\mathcal X=x) &, \\mathbb{\\hat P}(\\mathcal Y \\mid \\mathcal X=x)) =\\\\ \n&-\\sum_k^K \\mathbb P(\\mathcal Y=k \\mid \\mathcal X=x) \\log \\mathbb{\\hat P}(\\mathcal Y=k \\mid \\mathcal X=x).\n\\end{split}\n$$\n\nFinalmente la medida de rendimiento quedaría como $\\sum_x H(\\mathbb P(\\mathcal Y \\mid \\mathcal X=x), \\mathbb{\\hat P}(\\mathcal Y \\mid \\mathcal X=x)).$ \n\n### Área Bajo la Curva *ROC* {#sec-roc-curve}\n\nEl área bajo la curva *ROC* (*Relative Operating Characteristic*) es una medida de rendimiento que también está pasada en la probabilidad a posteriori $\\mathbb P(\\mathcal Y \\mid \\mathcal X)$ con la característica de que la clase se selecciona en base a un umbral $\\rho$. Es decir, dado un ejemplo $x$, este ejemplo pertenece a la clase $p$ si $\\mathbb P(\\mathcal Y=p \\mid \\mathcal X=x) \\geq \\rho.$\n\nSe observa que modificando el umbral $\\rho$ se tienen diferentes tablas de confusión, para cada tabla de confusión posible se calcula la tasa de verdaderos positivos (TPR) que corresponde al recall (@sec-recall), i.e., $\\mathbb P(\\mathcal{\\hat Y}=p \\mid \\mathcal Y=p),$ y la tasa de falsos positivos (FPR) que es $\\mathbb P(\\mathcal{\\hat Y}=p \\mid \\mathcal Y=n).$ Cada par de TPR y FPR representan un punto de la curva *ROC*. El rendimiento corresponde al área debajo de la curva delimitada por los pares TPR y FPR.\n\n### Ejemplo\n\nEl ejemplo de Breast Cancer Wisconsin (@sec-ejemplo-breast-cancer-wisconsin) se utiliza para ilustrar el uso de la medidas de rendimiento presentadas hasta el momento. \n\n::: {#2b49a18f .cell execution_count=3}\n``` {.python .cell-code}\nD, y = load_breast_cancer(return_X_y=True)\nT, G, y_t, y_g = train_test_split(D, y, \n                                  random_state=0,  \n                                  test_size=0.2)\ngaussian = GaussianBayes().fit(T, y_t)\nhy_gaussian = gaussian.predict(G)\n```\n:::\n\n\n\n\nEl clasificador Gausiano tiene un `accuracy` de $0.8947$, el cual se puede calcular con el siguiente código.\n\n::: {#2f14d590 .cell execution_count=5}\n``` {.python .cell-code}\naccuracy = metrics.accuracy_score(y_g, hy_gaussian)\n```\n:::\n\n\nLas medidas de `recall`, `precision` y `f1` se presentan en la @tbl-performance, en la última columna se presenta el macro de cada una de las medidas. \n\n::: {#e5c96e7a .cell execution_count=6}\n``` {.python .cell-code}\nrecall = metrics.recall_score(y_g, hy_gaussian,\n                              average=None)\nprecision = metrics.precision_score(y_g, hy_gaussian,\n                                    average=None)\nf1 = metrics.f1_score(y_g, hy_gaussian,\n                      average=None)\n```\n:::\n\n\n::: {#tbl-performance .cell tbl-cap='Rendimiento' execution_count=7}\n\n::: {.cell-output .cell-output-display .cell-output-markdown execution_count=7}\n|        |$\\mathcal Y=0$ | $\\mathcal Y=1$|Macro|\n|----------------------|----------------------|----------------------|----------------------|\n|`recall`|$0.8298$ | $0.9403$ | $0.8850$|\n|`precision`|$0.9070$ | $0.8873$ | $0.8972$|\n|`f1`|$0.8667$ | $0.9130$ | $0.8899$|\n:::\n:::\n\n\n\n\nPor otro lado la `entropia` cruzada es $2.1071$ que se puede calcular con el siguiente código.\n\n::: {#94dd9c97 .cell execution_count=9}\n``` {.python .cell-code}\nprob = gaussian.predict_proba(G)\nentropia = metrics.log_loss(y_g, prob)\n```\n:::\n\n\nComplementando la información de las medidas que se calculan mediante la posteriori se encuentra la curva ROC, la cual se puede calcular con el siguiente código y se muestra en la @fig-roc-curve\n\n::: {#cell-fig-roc-curve .cell execution_count=10}\n``` {.python .cell-code}\nfpr, tpr, thresholds = metrics.roc_curve(y_g, prob[:, 1])\ndf = pd.DataFrame(dict(FPR=fpr, TPR=tpr))\nsns.set_style('whitegrid')\nfig = sns.lineplot(df, x='FPR', y='TPR')\n```\n\n::: {.cell-output .cell-output-display}\n![ROC](04Rendimiento_files/figure-html/fig-roc-curve-output-1.png){#fig-roc-curve width=585 height=427}\n:::\n:::\n\n\n\n\nTeniendo un valor de área bajo la curva (`auc_score`) de $0.9540$ que se obtuvo de la siguiente manera.\n\n```python\nauc_score = metrics.roc_auc_score(y_g, prob[:, 1])\n```\n\n::: {.callout-tip collapse=\"true\"}\n### Actividad\n\nMedir el rendimiento del Clasificador Gausiano Ingenuo (@sec-cl-bayesiano-ingenuo) en el problema del del Iris (ver @sec-iris) utilizando la función $F_\\beta$ (@sec-f1) variando $\\beta$ entre $[0, 1],$ tal y como se muestra en la @fig-actividad-fbeta\n\n::: {#cell-fig-actividad-fbeta .cell execution_count=12}\n\n::: {.cell-output .cell-output-display}\n![Rendimiento de un Clasificador Bayesiano Ingenuo en el problema del Iris estimado mediante una validación cruzada estratificada.](04Rendimiento_files/figure-html/fig-actividad-fbeta-output-1.png){#fig-actividad-fbeta width=567 height=470}\n:::\n:::\n\n\n:::\n\n## Regresión {#sec-rendimiento-regresion}\n\nCon respecto a regresión las siguientes funciones son utilizadas como medidas de rendimiento.\n\nError cuadrático medio (Mean Square Error): \n\n$$\n\\textsf{mse}(\\mathcal Y, \\mathcal{\\hat Y}) = \\frac{1}{N} \\sum_{i=1}^N (\\mathcal Y_i - \\mathcal{\\hat Y}_i)^2.\n$$ {#eq-mse}\n\nError absoluto medio (Mean Absolute Error):\n\n$$\n\\textsf{mae}(\\mathcal Y, \\mathcal{\\hat Y}) = \\frac{1}{N} \\sum_{i=1}^N \\mid \\mathcal Y_i - \\mathcal{\\hat Y}_i \\mid.\n$$ {#eq-mae}\n\nMedia del porcentaje de error absoluto: \n$$\n\\textsf{mape}(\\mathcal Y, \\mathcal{\\hat Y}) = \\frac{1}{N} \\sum_{i=1}^N \\mid \\frac{\\mathcal Y_i - \\mathcal{\\hat Y}_i}{\\mathcal Y_i}\\mid.\n$$ {#eq-mape}\n\nLa proporción de la varianza explicada por el modelo: \n\n$$\nR^2(\\mathcal Y, \\mathcal{\\hat Y}) = 1 - \\frac{\\sum_{i=1}^N (\\mathcal Y_i - \\mathcal{\\hat Y}_i)^2)}{\\sum_{i=1}^N (\\mathcal Y_i - \\mathcal{\\bar Y}_i)^2)}.\n$$ {#eq-r2}\n\n### Ejemplo\n\nLas medidas anteriores se ejemplifican utilizando el ejemplo de diabetes(#sec-diabetes) que se puede descargar y modelar mediante OLS (@sec-regresion-ols) de la siguiente manera.\n\n::: {#e645dddc .cell execution_count=13}\n``` {.python .cell-code}\nX, y = load_diabetes(return_X_y=True)\nT, G, y_t, y_g = train_test_split(X, y,\n                                  random_state=0,  \n                                  test_size=0.2)\nm = LinearRegression().fit(T, y_t) \n```\n:::\n\n\nLa predicción en el conjunto de prueba sería:\n\n::: {#870d05e9 .cell execution_count=14}\n``` {.python .cell-code}\nhy = m.predict(G)\n```\n:::\n\n\nLas diferentes medidas de rendimiento para problemas de regresión se puede calcular de la siguiente manera. \n\nEl error cuadrático medio (@eq-mse), `mse` corresponde a\n\n::: {#ee0c7b21 .cell execution_count=15}\n``` {.python .cell-code}\nmse = metrics.mean_squared_error(y_g, hy)\n```\n:::\n\n\n\n\ny tienen un valor de $3424.2593$.\n\nEl error absoluto medio (@eq-mae), `mae`, tiene un valor de $46.1736$ calculado de la siguiente manera\n\n```python\nmae = metrics.mean_absolute_error(y_g, hy)\n```\n\nLa media del porcentaje de error absoluto (@eq-mape), `mape`, es $0.3805$ obtenido con el siguiente código \n\n```python\nmape = metrics.mean_absolute_percentage_error(y_g, hy)\n```\n\nFinalmente, la varianza explicada por el modelo $R^2$ (@eq-r2), `r2`, es $0.3322$\n\n```python\nr2 = metrics.r2_score(y_g, hy)\n```\n\n## Conjunto de Validación y Validación Cruzada {#sec-validacion-cruzada}\n\nAntes de inicia la descripción de otro algoritmo para la selección de características es necesario describir otro conjunto que se utiliza para optimizar los hiperparámetros del algoritmo de aprendizaje. Previamente se describieron los conjuntos de Entrenamiento y Prueba (@sec-conjunto-entre-prueba), i.e., $\\mathcal T$ y $\\mathcal G$. En particular estos conjuntos se definieron utilizando todos los datos $\\mathcal D$ con lo que se especifica el problema.\n\nLa mayoría de algoritmos de aprendizaje tiene hiperparámetros que pueden ser ajustados para optimizar su comportamiento al conjunto de datos que se está analizando. Estos hiperparámetros pueden estar dentro del algoritmo o pueden ser modificaciones al conjunto de datos para adecuarlos al algoritmo. El segundo caso es el que se analizará en esta unidad. Es decir, se seleccionarán las variables que facilitan el aprendizaje. \n\nPara optimizar los parámetros es necesario medir el rendimiento del algoritmo, es decir, observar como se comporta el algoritmo en el proceso de predicción. La manera trivial sería utilizar el conjunto de prueba $\\mathcal G$ para medir el rendimiento. Pero es necesario recordar que este conjunto no debe ser visto durante el aprendizaje y la optimización de los parámetros es parte de ese proceso. Si se usara $\\mathcal G$ entonces dejaría de ser el conjunto de prueba y se tendría que seleccionar otro conjunto de prueba. \n\nEntonces para optimizar los parámetros del algoritmo se selecciona del conjunto de entrenamiento, i.e., $\\mathcal T$, el **conjunto de validación**, $\\mathcal V$. Este conjunto tiene la característica que $\\mathcal T \\cap \\mathcal V \\cap \\mathcal G = \\emptyset$ y $\\mathcal T \\cup \\mathcal V \\cup \\mathcal G = \\mathcal D.$ Una manera de realizar estos es seleccionar primeramente el conjunto de prueba $\\mathcal G$ y de los datos restantes generar los conjuntos de entrenamiento $\\mathcal T$ y validación $\\mathcal V.$\n\nPara ejemplificar esta idea se utiliza el ejemplo de Breast Cancer Wisconsin (@sec-ejemplo-breast-cancer-wisconsin) utilizando un Clasificador Bayesiano donde el hiperparámetro es si se utilizar un Bayesiano Ingenuo o se estima la matriz de covarianza. \n\nEl primer paso es obtener los datos del problema, lo cual se muestra en la siguiente instrucción.\n\n::: {#c21ced42 .cell execution_count=17}\n``` {.python .cell-code}\nD, y = load_breast_cancer(return_X_y=True)\n```\n:::\n\n\nCon los datos $\\mathcal D$ se genera el conjunto de prueba $\\mathcal G$ y los datos para estimar los parámetros y optimizar los hiperparámetros del algoritmo. En la variable `T` se tiene los datos para encontrar el algoritmo y en `G` se tiene el conjunto de prueba.\n\n::: {#5d02cdec .cell execution_count=18}\n``` {.python .cell-code}\nT, G, y_t, y_g = train_test_split(D, y,\n                                  random_state=0,\n                                  test_size=0.2)\n```\n:::\n\n\nLos datos de entrenamiento y validación se generan de manera equivalente tal como se muestra en la siguiente instrucción. El conjunto de validación ($\\mathcal V$) se encuentra en la variable `V` y la variable dependiente en `y_v.`\n\n::: {#72db16b9 .cell execution_count=19}\n``` {.python .cell-code}\nT, V, y_t, y_v = train_test_split(T, y_t,\n                                  random_state=0,\n                                  test_size=0.3)\n```\n:::\n\n\n\n\nEn este momento ya se tienen todos los elementos para medir el rendimiento de cada hiperparámetro. Empezando por el clasificador con la matriz de covarianza completa. El recall en ambas clases es [0.9615, 0.8824]. \n\n::: {#34ffdea7 .cell execution_count=21}\n``` {.python .cell-code}\ngaussian = GaussianBayes().fit(T, y_t)\nhy_gaussian = gaussian.predict(V)\nrecall = recall_score(y_v, hy_gaussian, average=None)\n```\n:::\n\n\n\n\nLa segunda opción es utilizar un clasificador Bayesiano Ingenuo, el cual se especifica con el parámetro `naive` tal y como se muestra en las siguientes instrucciones. El recall en las dos clases es [0.8654, 0.9765].  \n\n::: {#2759a9c7 .cell execution_count=23}\n``` {.python .cell-code}\ningenuo = GaussianBayes(naive=True).fit(T, y_t)\nhy_ingenuo = ingenuo.predict(V)\nscore = recall_score(y_v, hy_ingenuo, average=None)\n```\n:::\n\n\n\n\nComparando el rendimiento de los dos hiperparámetros se observa cual de los dos modelos obtiene el mejor rendimiento. Con el fin de completar el ejemplo se describe calcular el rendimiento en  $\\mathcal G$ del algoritmo con la matriz de covarianza completa. Este algoritmo tiene un rendimiento de [0.8298, 0.9403] que se puede calcular con el siguiente código. \n\n::: {#ec86b6d5 .cell execution_count=25}\n``` {.python .cell-code}\ngaussian = GaussianBayes().fit(np.concatenate((T, V)),\n                               np.concatenate((y_t, y_v)))\nhy_gaussian = gaussian.predict(G)\nscore = recall_score(y_g, hy_gaussian, average=None)\n```\n:::\n\n\n### k-Iteraciones de Validación Cruzada {#sec-kfold-cross-validation}\n\nCuando se cuenta con pocos datos para medir el rendimiento del algoritmo es común utilizar la técnica de _k-fold cross-validation_ la cual consiste en partir $k$ veces el conjunto de entrenamiento para generar $k$ conjuntos de entrenamiento y validación. \n\nLa idea se ilustra con la siguiente tabla, donde se asume que los datos son divididos en 5 bloques ($k=5$), cada columna de la tabla ilustra los datos de ese bloque. Si los datos se dividen en $k=5$ bloques, entonces existen $k$ iteraciones que son representadas por cada renglón de la siguiente tabla, quitando el encabezado de la misma. La letra en cada celda identifica el uso que se le dará a esos datos en la respectiva iteración, es decir, $\\mathcal T$ representa que se usará como conjunto de entrenamiento y $\\mathcal V$ se usa para identificar aquellos datos que se usarán como conjunto de validación.\n\nLa idea es entrenar y probar el rendimiento del algoritmo $k$ veces usando las particiones en cada renglón. Es decir, la primera vez se usan los datos de la primera columna como el conjunto de validación, y el resto de columnas, $[2, 3, 4, 5]$, como conjunto de entrenamiento para estimar los parámetros del algoritmo. En la segunda iteración se usan los datos del segundo renglón donde se observa que los datos en la cuarta columna corresponden al conjunto de validación y los datos en las columnas $[1, 2, 3, 5]$ son usados como conjunto de prueba. Las iteraciones siguen hasta que todos los datos fueron utilizados en una ocasión como conjunto de validación. \n\n|  1 |  2 |  3 |  4 |  5 |\n|----|----|----|----|----|\n|$\\mathcal V$   |$\\mathcal T$   |$\\mathcal T$   |$\\mathcal T$   |$\\mathcal T$   |\n|$\\mathcal T$   |$\\mathcal T$   |$\\mathcal T$   |$\\mathcal V$   |$\\mathcal T$   |\n|$\\mathcal T$   |$\\mathcal T$   |$\\mathcal V$   |$\\mathcal T$   |$\\mathcal T$   |\n|$\\mathcal T$   |$\\mathcal T$   |$\\mathcal T$   |$\\mathcal T$   |$\\mathcal V$   |\n|$\\mathcal T$   |$\\mathcal V$   |$\\mathcal T$   |$\\mathcal T$   |$\\mathcal T$   |\n\nSe utiliza el mismo problema para medir el rendimiento del hiperparámetro del clasificador Gausiano. Lo primero es seleccionar el conjunto de prueba ($\\mathcal G$) que se realiza con el siguiente código. \n\n::: {#f7c9d540 .cell execution_count=26}\n``` {.python .cell-code}\nT, G, y_t, y_g = train_test_split(D, y,\n                                  random_state=0,  \n                                  test_size=0.2)\n```\n:::\n\n\n\n\nLa validación cruzada con k-iteraciones se puede realizar con la clase `KFold` de la siguiente manera. La primera línea crear una variable para guardar el rendimiento. En la segunda línea se inicializa el procedimiento indicando que los datos sean tomados al azar. Después se realiza el ciclo con las $k$ iteraciones, para cada iteración se genera un índice `ts` que indica cuales son los datos del conjunto de entrenamiento y `vs` que corresponde a los datos de validación. Se estiman los parámetros usando `ts` tal y como se observa en la cuarta línea. Habiendo estimado los parámetros se predicen los datos del conjunto de validación (5 línea), se mide el recall en todas las clases y se guarda en la lista `perf.` Al final se calcula la media de los $k$ rendimientos medidos, teniendo un valor de [0.8903, 0.9174].\n\n::: {#71f4d562 .cell execution_count=28}\n``` {.python .cell-code}\nperf = []\nkfold = KFold(shuffle=True, random_state=0)\nfor ts, vs in kfold.split(T):\n    gaussian = GaussianBayes().fit(T[ts], y_t[ts])\n    hy_gaussian = gaussian.predict(T[vs])\n    _ = recall_score(y_t[vs], hy_gaussian, average=None)    \n    perf.append(_)\nperf = np.mean(perf, axis=0)    \n```\n:::\n\n\nUn procedimiento equivalente se realiza para el caso del clasificador Bayesiano Ingenuo tal y como se muestra a continuación. La media del recall en las clases es [0.8260, 0.9785] Se observa que el clasificador Bayesiano con la matriz de covarianza tiene un mejor rendimiento en validación que el clasificador Bayesiano Ingenuo. El último paso sería calcular el rendimiento en el conjunto $\\mathcal G$ lo cual fue presentado anteriormente.  \n\n::: {#f78eaea8 .cell execution_count=29}\n``` {.python .cell-code}\nperf = []\nkfold = KFold(shuffle=True)\nfor ts, vs in kfold.split(T):\n    gaussian = GaussianBayes(naive=True).fit(T[ts], y_t[ts])\n    hy_gaussian = gaussian.predict(T[vs])\n    _ = recall_score(y_t[vs], hy_gaussian, average=None)    \n    perf.append(_)\nperf = np.mean(perf, axis=0)  \n```\n:::\n\n\n",
    "supporting": [
      "04Rendimiento_files/figure-html"
    ],
    "filters": [],
    "includes": {}
  }
}