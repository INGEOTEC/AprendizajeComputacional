{
  "hash": "e01b1805453f4f75aaa34224e0974100",
  "result": {
    "engine": "jupyter",
    "markdown": "# Ensambles\n\nEl **objetivo** de la unidad es conocer y aplicar diferentes técnicas para realizar un ensamble de clasificadores o regresores.\n\n## Paquetes usados\n\n::: {#ede921ab .cell execution_count=1}\n``` {.python .cell-code}\nfrom scipy.stats import binom\nfrom sklearn.datasets import load_diabetes, load_digits\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.svm import LinearSVC, LinearSVR, SVR\nfrom sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\nfrom sklearn.ensemble import BaggingRegressor\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.metrics import recall_score, mean_absolute_percentage_error\nfrom collections import Counter\nfrom matplotlib import pylab as plt\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\n```\n:::\n\n\n\n\n::: {.content-visible when-format=\"html\"}\n\n---\n\n\n{{< video https://www.youtube.com/embed/eqpMGmlWIP8 width=\"560\" height=\"315\" >}}\n\n\n\n---\n\n:::\n\n## Introducción {#sec-intro-12}\n\nComo se ha visto hasta el momento, cada algoritmo de clasificación y regresión tiene un sesgo, este puede provenir de los supuestos que se asumieron cuando se entrenó o diseño; por ejemplo, asumir que los datos provienen de una distribución gausiana multivariada o que se pueden separar los ejemplos mediante un hiperplano, entre otros. Dado un problema se desea seleccionar aquel algoritmo que tiene el mejor rendimiento, visto de otra manera, se selecciona el algoritmo cuyo sesgo este mejor alineado al problema. Una manera complementaria sería utilizar varios algoritmos y tratar de predecir basados en las predicciones individuales de cada algoritmo. En esta unidad se explicarán diferentes metodologías que permiten combinar predicciones de algoritmos de clasificación y regresión. \n\n## Fundamentos {#sec-fundamentos}\n\nLa descripción de ensambles se empieza observando el siguiente comportamiento. Suponiendo que se cuenta con $M$ algoritmos de clasificación binaria cada uno tiene una exactitud de $p=0.51$ y estos son completamente independientes. El proceso de clasificar un elemento corresponde a preguntar la clase a los $M$ clasificadores y la clase que se recibe mayor votos es la clase seleccionada, esta votación se comporta como una variable aleatoria que tiene una distribución Binomial. Suponiendo con la clase del elemento es $1$, en esta condición la función cumulativa de distribución ($\\textsf{cdf}$) con parámetros $k=\\lfloor \\frac{M}{2}\\rfloor,$ $n=M$ y $p=0.51$ indica seleccionar la clase $0$ y $1 - \\textsf{cdf}$ corresponde a la probabilidad de seleccionar la clase $1$. \n\nLa @fig-ensambles-acc-bin muestra como cambia la exactitud, cuando el número de clasificadores se incrementa, cada uno de esos clasificadores son independientes y tiene una exactitud de $p=0.51,$ se puede observar que cuando $M=501$ el rendimiento es $0.673$ y con $9,999$ clasificadores se tiene un valor de $0.977.$\n\n::: {#cell-fig-ensambles-acc-bin .cell execution_count=3}\n``` {.python .cell-code code-fold=\"true\"}\nN = range(3, 10002, 2)\ncdf_c = [1 - binom.cdf(np.floor(n / 2), n, 0.51) for n in N]\ndf = pd.DataFrame(dict(accuracy=cdf_c, ensamble=N))\n_ = sns.relplot(data=df, x='ensamble', y='accuracy', kind='line')\n```\n\n::: {.cell-output .cell-output-display}\n![Rendimiento cuando el número de clasificadores se incrementa](12Ensambles_files/figure-html/fig-ensambles-acc-bin-output-1.png){#fig-ensambles-acc-bin width=471 height=471}\n:::\n:::\n\n\n</details>\n\n\nEn el caso de regresión, en particular cuando se usa como función de error el cuadrado del error, i.e., $(\\hat y - y)^2$ se tiene el intercambio entre varianza y sesgo, el cual se deriva de la siguiente manera. \n\n$$\n\\begin{split}\n\\mathbb E[(\\hat y - y)^2] &=\\\\\n&=\\mathbb E[(\\hat y - \\mathbb E[\\hat y] + \\mathbb E[\\hat y] - y)^2]\\\\\n&=\\underbrace{\\mathbb E[(\\hat y - \\mathbb E[\\hat y])^2]}_{\\mathbb V(\\hat y)} + \\mathbb E[(\\mathbb E[\\hat y] - y)^2] \\\\\n&+ 2 \\mathbb E[(\\hat y - \\mathbb E[\\hat y])(\\mathbb E[\\hat y] - y)]\\\\\n&=\\mathbb V(\\hat y) + (\\underbrace{\\mathbb E[\\hat y] - y}_{\\text{sesgo}})^2 + 2 \\underbrace{\\mathbb E[(\\hat y - \\mathbb E[\\hat y])]}_{\\mathbb E[\\hat y] - \\mathbb E[\\hat y] = 0}(\\mathbb E[\\hat y] - y)\\\\\n&=\\mathbb V(\\hat y) + (\\mathbb E[\\hat y] - y)^2\n\\end{split}\n$$\n\n\nSe observa que el cuadrado del error está definido por la varianza de $\\hat y$ (i.e., $\\mathbb V(\\hat y)$), la cual es independiente de la salida $y$ y el sesgo al cuadrado del algoritmo (i.e., $(\\mathbb E[\\hat y] - y)^2$).\n\nEn el contexto de ensamble, asumiendo que se tienen $M$ regresores independientes donde la predicción está dada por $\\bar y = \\frac{1}{M}\\sum_{i=1}^M \\hat y^i$, se tiene que el sesgo de cada predictor individual es igual al sesgo de su promedio (i.e., $(\\mathbb E[\\bar y] - y) = (\\mathbb E[\\hat y^i] - y)$) como se puede observar a continuación. \n\n$$\n\\begin{split}\n\\mathbb E[\\bar y] &= \\mathbb E[\\frac{1}{M} \\sum_{i=1}^M \\hat y^i]\\\\\n&=\\frac{1}{M} \\sum_{i=1}^M \\underbrace{\\mathbb E[\\hat y^i]}_{\\mathbb E[\\hat y]} =\\frac{1}{M} M \\mathbb E[\\hat y] =\\mathbb E[\\hat y]\n\\end{split}\n$$\n\nPor otro lado la varianza del promedio (i.e., $\\mathbb V(\\bar y)$) está dada por $\\mathbb V(\\bar y)=\\frac{1}{M} \\mathbb V(\\hat y)$, que se deriva siguiendo los pasos del error estándar de la media (@sec-error-estandar-media).\n\nEsto quiere decir que si se tienen $M$ regresores independientes, entonces el error cuadrado de su promedio es menor que el error de cada regresor individual, esto es porque su la varianza se reduce tal y como se mostró. \n\nTanto en el caso de clasificación como en el caso del error cuadrado, es poco probable contar con clasificadores y regresores que sean completamente independientes, entonces sus predicciones van a estar relacionadas en algún grado y no se podrá llegar a las reducciones obtenidas en el procedimiento presentado. \n\n## Bagging\n\nSiguiendo con la idea de combinar $M$ instancias independientes de un tipo de algoritmo, en esta sección se presenta el algoritmo Bagging (Bootstrap Aggregation) el cual como su nombre lo indica se basa la técnica de Bootstrap (@sec-bootstrap) para generar $M$ instancias del algoritmo y la combinación es mediante votación o el promedio en caso de regresión o que se cuente con la probabilidad de cada clase.\n\n### Ejemplo: Dígitos\n\nPara ejemplificar el uso del algoritmo de Bagging se utilizará el conjunto de datos de Dígitos. Estos datos se pueden obtener y generar el conjunto de entrenamiento ($\\mathcal T$) y prueba ($\\mathcal G$) con las siguientes instrucciones.\n\n::: {#aed304eb .cell execution_count=4}\n``` {.python .cell-code}\nX, y = load_digits(return_X_y=True)\nT, G, y_t, y_g = train_test_split(X, y,\n                                  test_size=0.2,\n                                  random_state=0)\n```\n:::\n\n\nLos algoritmos que se utilizarán de base son Máquinas de Soporte Vectorial Lineal (@sec-svm) y Árboles de Decisión (@sec-arboles-decision). Lo primero que se realizará es entrenar una instancia de estos algoritmos para poder comparar su rendimiento en el conjunto de prueba contra Bagging. \n\nLa siguientes instrucciones entrenan una máquina de soporte vectorial, calculando en la segunda línea el macro-recall (@sec-macro). El rendimiento se presenta en una tabla para facilitar la comparación. \n\n::: {#0c7917f8 .cell execution_count=5}\n``` {.python .cell-code}\nsvc = LinearSVC(dual=False).fit(T, y_t)\nsvc_recall = recall_score(y_g, svc.predict(G),\n                          average=\"macro\")\n```\n:::\n\n\nComplementando las instrucciones anteriores, en el siguiente código se entrena un Árbol de Decisión. \n\n::: {#4f33b90e .cell execution_count=6}\n``` {.python .cell-code}\ntree = DecisionTreeClassifier(criterion='entropy',\n                              min_samples_split=9).fit(T, y_t)\ntree_recall = recall_score(y_g, tree.predict(G),\n                           average=\"macro\")\n```\n:::\n\n\nEl algoritmo de Bootstrap inicia generando las muestras tal y como se realizó en el ejemplo del error estándar de la media (@sec-bootstrap-ejemplo); el siguiente código genera las muestras, en particular el ensamble sería de $M=11$ elementos. \n\n::: {#64331f44 .cell execution_count=7}\n``` {.python .cell-code}\nB = np.random.randint(T.shape[0],\n                      size=(11, T.shape[0]))\n```\n:::\n\n\nEmpezando con Bagging usando como clasificador base la Máquina de Soporte Vectorial Lineal. La primera línea de las siguientes instrucciones, entra los máquinas de soporte, después se realizan las predicciones. En la tercera línea se calcula la clase que tuvo la mayor cantidad de votos y finalmente se calcula el error en términos de macro-recall.\n\n::: {#311e4dfe .cell execution_count=8}\n``` {.python .cell-code}\nsvc_ins = [LinearSVC(dual=False).fit(T[b], y_t[b])\n           for b in B]\nhys = np.array([m.predict(G) for m in svc_ins])\nhy = np.array([Counter(x).most_common(n=1)[0][0]\n               for x in hys.T])\nbsvc_recall = recall_score(y_g, hy, average=\"macro\")\n```\n:::\n\n\nEl siguiente algoritmo son los Árboles de Decisión; la única diferencia con respecto a las instrucciones anteriores es la primera línea donde se entrenan los árboles. \n\n::: {#2e8a4d19 .cell execution_count=9}\n``` {.python .cell-code}\ntree_ins = [DecisionTreeClassifier(criterion='entropy',\n                                   min_samples_split=9).fit(T[b], y_t[b])\n            for b in B]\nhys = np.array([m.predict(G) for m in tree_ins])\nhy = np.array([Counter(x).most_common(n=1)[0][0]\n               for x in hys.T])\nbtree_recall = recall_score(y_g, hy,\n                            average=\"macro\")\n```\n:::\n\n\nComo se mencionó, la predicción final se puede realizar de dos manera en clasificación una es usando votación, como se vió en los códigos anteriores y la segunda es utilizando el promedio de las probabilidades. En el caso de las Máquinas de Soporte Vectorial, estas no calculas las probabilidad de cada clase, pero se cuenta con el valor de la función de decisión, en el siguiente código se usa está información, la segunda y tercera línea normaliza los valores para que ningún valor sea mayor que $1$ y menor que $-1$ y finalmente se calcula la suma para después seleccionar la clase que corresponde al argumento máximo. \n\n::: {#9c3593de .cell execution_count=10}\n``` {.python .cell-code}\nhys = np.array([m.decision_function(G) for m in svc_ins])\nhys = np.where(hys > 1, 1, hys)\nhys = np.where(hys < -1, -1, hys)\nhys = hys.sum(axis=0)\ncsvc_recall = recall_score(y_g, hys.argmax(axis=1),\n                           average=\"macro\")\n```\n:::\n\n\nEl procedimiento anterior se puede adaptar a los Árboles de Decisión utilizando el siguiente código. \n\n::: {#875ce085 .cell execution_count=11}\n``` {.python .cell-code}\nhys = np.array([m.predict_proba(G)\n                for m in tree_ins])\nctree_recall = recall_score(y_g,\n                            hys.sum(axis=0).argmax(axis=1),\n                            average=\"macro\")\n```\n:::\n\n\nFinalmente, la @tbl-ensambles-perf-cl muestra el rendimiento de las diferentes combinaciones, se puede observar el valor tanto de las Máquinas de Soporte Vectorial (M.S.V) Lineal y de los Árboles de decisión cuando se utilizaron fuera del ensamble; en el segundo renglón se muestra el rendimiento cuando la predicción del ensamble se hizo mediante votación y el último renglón presenta el rendimiento cuando se hace la suma. \n\nComparando los diferentes rendimientos, se puede observar que no existe mucha diferencia en rendimiento en las M.S.V Lineal y que la mayor mejora se presentó en los Árboles de Decisión. Este comportamiento es esperado dado que para que Bagging funciones adecuadamente requiere algoritmos inestables, es decir, algoritmos cuyo comportamiento cambia considerablemente con un cambio pequeño en el conjunto de entrenamiento, este es el caso de los Árboles. Por otro lado las M.S.V son algoritmos estables y un cambio pequeño en su conjunto de entrenamiento no tendrá una repercusión considerable en el comportamiento del algoritmo. \n\n::: {#tbl-ensambles-perf-cl .cell tbl-cap='Rendimiento (macro-recall) de bagging y estimadores base.' execution_count=12}\n\n::: {.cell-output .cell-output-display .cell-output-markdown execution_count=12}\n|                   |M.S.V. Lineal|Árboles de Decisión|\n|-------------------|-------------|-------------------|\n|Único|$0.9425$ | $0.8296$|\n|Votación ($M=11$)|$0.9446$ | $0.9321$|\n|Suma ($M=11$)|$0.9505$ | $0.9313$|\n\n:::\n:::\n\n\n### Ejemplo: Diabetes\n\nAhora toca el turno de atacar un problema de regresión mediante Bagging, el problema que se utilizará es el de Diabetes. Las instrucciones para obtener el problema y generar los conjuntos de entrenamiento ($\\mathcal T$) y prueba ($\\mathcal G$) se muestra a continuación. \n\n::: {#c8b97fd0 .cell execution_count=13}\n``` {.python .cell-code}\nX, y = load_diabetes(return_X_y=True)\nT, G, y_t, y_g = train_test_split(X, y,\n                                  random_state=0,\n                                  test_size=0.2)\n```\n:::\n\n\nTomando el caso de Dígitos como base, el primer algoritmo a entrenar es la M.S.V. Lineal y se usa como mediada de rendimiento el porcentaje del error absoluto (@eq-mape); tal y como se muestran en las siguientes instrucciones. \n\n::: {#7df3a90c .cell execution_count=14}\n``` {.python .cell-code}\nsvr = LinearSVR(dual='auto').fit(T, y_t)\nsvr_mape = mean_absolute_percentage_error(y_g,\n                                          svr.predict(G))\n```\n:::\n\n\nEl árbol de decisión y su rendimiento se implementa con el siguiente código. \n\n::: {#398e0e8e .cell execution_count=15}\n``` {.python .cell-code}\ntree = DecisionTreeRegressor(min_samples_split=9).fit(T,\n                                                      y_t)\ntree_mape = mean_absolute_percentage_error(y_g,\n                                           tree.predict(G))\n```\n:::\n\n\nAl igual que en el caso de clasificación, la siguiente instrucción genera los índices para generar las muestras. Se hace un ensamble de $M=11$ elementos. \n\n::: {#842aabd3 .cell execution_count=16}\n``` {.python .cell-code}\nB = np.random.randint(T.shape[0], size=(11, T.shape[0]))\n```\n:::\n\n\nEn el caso de regresión, la predicción final corresponde al promedio de las predicciones individuales, la primera línea de las siguientes instrucciones se entrena las M.S.V Lineal, en la segunda instrucción se hacen las predicciones y se en la tercera se realiza el promedio y se mide el rendimiento. \n\n::: {#fddc99d8 .cell execution_count=17}\n``` {.python .cell-code}\nsvr_ins = [LinearSVR(dual='auto').fit(T[b], y_t[b])\n           for b in B]\nhys = np.array([m.predict(G) for m in svr_ins])\nbsvr_mape = mean_absolute_percentage_error(y_g,\n                                           hys.mean(axis=0))\n```\n:::\n\n\nDe manera equivalente se entrenan los Árboles de Decisión, como se muestra a continuación. \n\n::: {#bdd742fd .cell execution_count=18}\n``` {.python .cell-code}\ntree_ins = [DecisionTreeRegressor(min_samples_split=9).fit(T[b], y_t[b])\n            for b in B]\nhys = np.array([m.predict(G) for m in tree_ins])\nbtree_mape = mean_absolute_percentage_error(y_g,\n                                            hys.mean(axis=0))\n```\n:::\n\n\nLa @tbl-ensambles-perf-reg muestra el rendimiento de los algoritmos de regresión utilizados, al igual que en el caso de clasificación, la M.S.V. no se ve beneficiada con el uso de Bagging. Por otro lado los Árboles de Decisión tienen un incremento en rendimiento considerable al usar Bagging. \n\n::: {#tbl-ensambles-perf-reg .cell tbl-cap='Rendimiento (MAPE) de bagging y estimadores base.' execution_count=19}\n\n::: {.cell-output .cell-output-display .cell-output-markdown execution_count=19}\n|                   |M.S.V. Lineal|Árboles de Decisión|\n|-------------------|-------------|-------------------|\n|Único|$0.4131$ | $0.5661$|\n|Promedio ($M=11$)|$0.4136$ | $0.3845$|\n\n:::\n:::\n\n\nHasta este momento los ensambles han sido de $M=11$ elementos, queda la duda como varía el rendimiento con respecto al tamaño del ensamble. La @fig-ensamble-variacion muestra el rendimiento de Bagging utilizando Árboles de Decisión, cuando el ensamble cambia $M=2,\\ldots,500.$ Se observa que alrededor que hay un decremento importante cuando el ensamble es pequeño, después el error se incrementa y vuelve a bajar alrededor de $M=100.$ Finalmente se ve que el rendimiento es estable cuando $M>200.$\n\n::: {#cell-fig-ensamble-variacion .cell execution_count=20}\n``` {.python .cell-code code-fold=\"true\"}\nB = np.random.randint(T.shape[0], size=(500, T.shape[0]))\ntree_ins = [DecisionTreeRegressor(min_samples_split=9).fit(T[b], y_t[b]) for b in B]\nhys = np.array([m.predict(G) for m in tree_ins])\n\nM = range(2, len(tree_ins) + 1)\np = [mean_absolute_percentage_error(y_g, \n                                    hys[:i].mean(axis=0))\n     for i in M]\ndf = pd.DataFrame(dict(error=p, ensamble=M))\nsns.relplot(data=df, x='ensamble', y='error', kind='line')\n```\n\n::: {.cell-output .cell-output-display}\n![Variación del rendimiento con respecto al tamaño del ensamble ($M$).](12Ensambles_files/figure-html/fig-ensamble-variacion-output-1.png){#fig-ensamble-variacion width=471 height=470}\n:::\n:::\n\n\n## Stack Generalization\n\nContinuando con la descripción de ensambles, se puede observar que Bagging en el caso de la media se puede representar como $\\sum_i^M \\frac{1}{M} \\hat y^i,$ donde el factor $\\frac{1}{M}$ se puede observar como un parámetro a identificar. Entonces la idea siguiente sería como se podría estimar parámetros para cada uno de los estimadores bases, e.g., $\\sum_i^M w_i \\hat y^i$ donde $w_i$ para bagging corresponde a $\\frac{1}{M}.$ Se podría ir más allá y pensar que las predicciones $\\mathbf y=(\\hat y^1, \\ldots, \\hat y^M)$ podrían ser la entrada a otro estimador. \n\nEsa es la idea detrás de Stack Generalization, la idea es utilizar las predicciones de los estimadores bases como las entradas de otro estimador. Para poder llevar este proceso es necesario contar con un conjunto independiente del conjunto de entrenamiento para encontrar los parámetros del estimador que combina las predicciones. \n\n### Ejemplo: Diabetes\n\nPara ejemplificar el uso de Stack Generalization, se usa el conjunto de datos de Diabetes. Como se acaba de describir es necesario contar con un conjunto independiente para estimar los parámetros del estimador del stack. Entonces el primer paso es dividir el conjunto de entrenamiento en un conjunto de entrenamiento y validación ($\\mathcal V,$) tal y como se muestra en la siguiente instrucción. \n\n::: {#67a9c160 .cell execution_count=21}\n``` {.python .cell-code}\nT1, V, y_t1, y_v = train_test_split(T, y_t,\n                                    test_size=0.3)\n```\n:::\n\n\nPara este ejemplo se usará como regresores bases el algoritmo de Vecinos Cercanos (@sec-regresion) con diferentes parámetros (primera línea), después se usan los modelos para predecir el conjunto de validación ($\\mathcal V$) y prueba ($\\mathcal G$), esto se observa en la segunda y tercera línea del siguiente código. \n\n::: {#906fe783 .cell execution_count=22}\n``` {.python .cell-code}\nmodels = [KNeighborsRegressor(n_neighbors=n).fit(T1, y_t1)\n          for n in [7, 9]]\nV_stack = np.array([m.predict(V) for m in models]).T\nG_stack = np.array([m.predict(G) for m in models]).T\n```\n:::\n\n\nEl porcentaje del error absoluto (@eq-mape) de los estimadores bases en el conjunto de prueba se puede calcular con el siguiente código\n\n::: {#0d22d767 .cell execution_count=23}\n``` {.python .cell-code}\nmape_test = []\nfor hy in G_stack.T:\n  mape = mean_absolute_percentage_error(y_g, hy)\n  mape_test.append(mape)\n```\n:::\n\n\n\n\nteniendo los siguientes valores $0.3762$ y $0.3587$, respectivamente.\n\nFinalmente, es momento de entrenar el regresor que combinará las salidas de los estimadores bases, i.e., Vecinos Cercanos. Se decidió utilizar una Máquina de Soporte Vectorial (@sec-svm) con kernel polinomial de grado $2$. Los parámetros de la máquina se estiman en la primera línea, y después se predicen los datos del conjunto de prueba. \n\n::: {#923cbf60 .cell execution_count=25}\n``` {.python .cell-code}\nstacking = SVR(kernel='poly', degree=2).fit(V_stack, y_v)\nhy = stacking.predict(np.vstack(G_stack))\n```\n:::\n\n\n\n\nEl error (@eq-mape) obtenido por este procedimiento es de $0.3579$; cabe mencionar que no en todos los casos el procedimiento de stacking consigue un mejor rendimiento que los estimadores bases, por ejemplo, en las siguientes instrucciones se entrena un Bagging con Árboles de Decisión para ser utilizado en lugar de la Máquina de Soporte Vectorial. \n\n::: {#4a7c2a45 .cell execution_count=27}\n``` {.python .cell-code}\nst_trees = BaggingRegressor(estimator=DecisionTreeRegressor(min_samples_split=9),\n                            n_estimators=200).fit(V_stack, y_v)\nhy = st_trees.predict(np.vstack(G_stack))\nmape = mean_absolute_percentage_error(y_g, hy)\n```\n:::\n\n\nEl rendimiento de este cambio es $0.4118$ lo cual es mayor que el error obtenido por los estimadores base. \n\n",
    "supporting": [
      "12Ensambles_files/figure-html"
    ],
    "filters": [],
    "includes": {}
  }
}