{
  "hash": "0f5a18148860da4e5deec99ea3850e27",
  "result": {
    "engine": "jupyter",
    "markdown": "# Árboles de Decisión {#sec-arboles-decision}\n\nEl **objetivo** de la unidad es conocer y aplicar árboles de decisión a problemas de clasificación y \nregresión.\n\n## Paquetes usados\n\n::: {.cell execution_count=1}\n``` {.python .cell-code}\nfrom sklearn import tree\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.datasets import load_breast_cancer, load_diabetes\nfrom sklearn.inspection import DecisionBoundaryDisplay\nfrom scipy.stats import multivariate_normal\nfrom matplotlib import pylab as plt\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\n```\n:::\n\n\n\n\n::: {.content-visible when-format=\"html\"}\n\n---\n\n\n\n\n\n{{< video https://www.youtube.com/embed/HhLRmQWV9h0 width=\"560\" height=\"315\" >}}\n\n\n\n\n\n\n\n\n\n---\n\n:::\n\n## Introducción {#sec-intro-08}\n\nLos árboles de decisión son una estructura de datos jerárquica, la cual se construye utilizando una estrategia de divide y vencerás. Los árboles son un método no paramétrico diseñado para problemas de regresión y clasificación. \n\nEl árbol se camina desde la raíz hacia las hojas; en cada nodo se tiene una regla que muestra el camino de acuerdo a la entrada y la hoja indica la clase o respuesta que corresponde a la entrada.\n\n## Clasificación { #sec-arboles-clasificacion }\n\nUtilizando el procedimiento para generar tres Distribuciones Gausianas (@sec-tres-normales) se generan las siguientes poblaciones (@fig-arboles-tres-distribuciones) con medias $\\mu_1=[5, 5]^\\intercal$, $\\mu_2=[-5, -10]^\\intercal$ y $\\mu_3=[15, -6]^\\intercal$; utilizando las matrices de covarianza originales.\n\n::: {.cell execution_count=3}\n\n::: {.cell-output .cell-output-display}\n![Tres distribuciones Gausianas](08Arboles_files/figure-pdf/fig-arboles-tres-distribuciones-output-1.pdf){#fig-arboles-tres-distribuciones}\n:::\n:::\n\n\nCon estas tres poblaciones, donde cada distribución genera una clase se crea un árbol de decisión. El árbol se muestra en la @fig-arboles-arbol-decision, donde se observa, en cada nodo interno, la siguiente información. La primera línea muestra el identificador del nodo, la segunda corresponde a la función de corte, la tercera línea es la entropía ($H(\\mathcal Y) = -\\sum_{y \\in \\mathcal Y} \\mathbb P(\\mathcal Y=y) \\log_2 \\mathbb P(\\mathcal Y=y)$), la cuarta es el número de elementos que llegaron al nodo y la última la frecuencia de cada clase en ese nodo. \n\n::: {.cell execution_count=4}\n\n::: {.cell-output .cell-output-display}\n![Árbol de decisión](08Arboles_files/figure-pdf/fig-arboles-arbol-decision-output-1.pdf){#fig-arboles-arbol-decision}\n:::\n:::\n\n\n\n\nPor ejemplo el nodo raíz del árbol tiene una entropía de $1.5850$, la función de decisión es $x \\leq 10.5605$ que indica que todos los elementos con un valor en $x$ menor o igual del valor calculado están del lado izquierdo. Los hojas (nodos \\#2\\, \\#3\\, \\#5\\, y \\#6) no cuentan con una función de corte, dado que son la parte final del árbol. En el árbol mostrado se observa que la entropía en todos los casos es $0$, lo cual indica que todos los elementos que llegaron a ese nodo son de la misma clase. No en todos los casos las hojas tienen entropía cero y existen parámetros en la creación del árbol que permiten crear árboles más simples. Por ejemplo, hay hojas que tienen muy pocos ejemplos, uno se podría preguntar ¿qué pasaría si esas hojas se eliminan? para tener un árbol más simple. \n\n\n\nLa siguiente @fig-arboles-arbol-decision-funcion muestra el árbol generado cuando se remueven el nodo #6. Se observa un árbol con menos nodos, aunque la entropía en es diferente de cero en algunas hojas. La segunda parte de la figura muestra la función de decisión que genera el árbol de decisión. Se observa que cada regla divide el espacio en dos usando la información que se muestra en cada nodo. \n\n::: {.cell execution_count=7}\n\n::: {.cell-output .cell-output-display}\n![Árbol de decisión y función](08Arboles_files/figure-pdf/fig-arboles-arbol-decision-funcion-output-1.pdf){#fig-arboles-arbol-decision-funcion}\n:::\n:::\n\n\n### Predicción { #sec-arboles-prediccion }\n\n\n\nUtilizando el árbol y la función de decisión mostrada en @fig-arboles-arbol-decision-funcion, se puede explicar el proceso de clasificar un nuevo elemento. Por ejemplo, el elemento $\\mathbf u=(x=-3, y=0.5)$ pasaría por los nodos \\#0\\, \\#1 y \\#3 para llegar a la clase correspondiente. \n\n### Entrenamiento { #sec-clasificacion-entrenamiento }\n\nExisten diferentes sistemas para la generación de un árbol de decisión (e.g., @Quinlan1986) la mayoría de ellos comparten las siguiente estructura general. La construcción un árbol se realiza mediante un procedimiento recursivo en donde se aplica la función de corte $f_m(\\mathbf x) = x_i \\leq a$ en el nodo $m$, donde el parámetro $a$ y la componente $x_i$ se identifican utilizando los datos que llegan al nodo $m$ de tal manera que se maximice una función de costo.\n\nUna función de costo podría estar basada en la entropía, es decir, para cada posible corte se mide la entropía en los nodos generados y se calcula la esperanza de la entropía de la siguiente manera. \n\n$$\nL(x_i, a) = \\sum_h \\frac{\\mid \\mathcal D_h \\mid}{\\mid \\mathcal D_m \\mid}  H(\\mathcal D_h),\n$$\n\ndonde $H(\\mathcal D_h)$ es la entropía de las etiquetas del conjunto $\\mathcal D_h$, la entropía se puede calcular con la siguiente función. La función recibe un arreglo con las clases, está protegida para calcular $0 \\log 0 = 0$ y finalmente regresa la entropía de `arr`.\n\n::: {.cell execution_count=9}\n``` {.python .cell-code}\ndef H(arr):\n    a, b = np.unique(arr, return_counts=True)\n    b = b / b.sum()\n    return - (b * np.log2(b, where=b != 0)).sum()\n```\n:::\n\n\nLa función que optimiza $L(x_i, a),$ para encontrar $a$ se implementa en el procedimiento `corte_var`. Este procedimiento asume que las etiquetas (`labels`) están ordenadas por la variable $x_i$, es decir la primera etiqueta corresponde al valor mínimo de $x_i$ y la última al valor máximo. Considerando esto, el valor de $a$ es el índice con el menor costo. En la primera línea se inicializa la variable `mejor` para guardar el valor de $a$ con mejor costo. La segunda línea corresponde a $\\mid \\mathcal D_m \\mid$, en la tercera línea se identifican los diferentes valores de $a$ que se tiene que probar, solo se tienen que probar aquellos puntos donde cuando la clase cambia con respecto al elemento adyacente, esto se calcula con la función `np.diff`; dado que está quita el primer elemento entonces es necesario incrementar $1.$ El ciclo es por todos los puntos de corte, se calculan el costo para los elementos que están a la izquierda y derecha del corte y se compara el resultado con el costo con menor valor encontrado hasta el momento. La última línea regresa el costo mejor así como el índice donde se encontró. \n\n::: {.cell execution_count=10}\n``` {.python .cell-code}\ndef corte_var(labels):\n    mejor = (np.inf, None)\n    D_m = labels.shape[0]\n    corte = np.where(np.diff(labels))[0] + 1\n    for j in corte:\n        izq = labels[:j]\n        der = labels[j:]\n        a = (izq.shape[0] / D_m) * H(izq)\n        b = (der.shape[0] / D_m) * H(der)\n        perf = a + b\n        if perf < mejor[0]:\n          mejor = (perf, j)\n    return mejor\n```\n:::\n\n\n\n\nEn el siguiente ejemplo se usa la función `corte_var`; la función regresa un costo de $0.4591$ y el punto de corte es el elemento $3$, se puede observar que es el mejor punto de corte en el arreglo dado. \n\n::: {.cell execution_count=12}\n``` {.python .cell-code}\ncosto, indice = corte_var(np.array([0, 0, 1, 0, 0, 0]))\n```\n:::\n\n\nCon la función `corte_var` se optimiza el valor $a$ de $L(x_i, a)$, ahora es el turno de optimizar $x_i$ con respecto a la función de costo. El procedimiento `corte` encuentra el mínimo con respecto de $x_i$, está función recibe los índices (`idx`) donde se buscará estos valores, en un inicio `idx` es un arreglo de $0$ al número de elemento del conjunto $\\mathcal D$ menos uno. La primera línea define la variable donde se guarda el menor costo, en la segunda línea se ordenan las variables, la tercera línea se obtienen las etiquetas involucradas. El ciclo va por todas las variables $x_i$. Dentro del ciclo se llama a la función `corte_var` donde se observa como las etiquetas van ordenadas de acuerdo a la variable que se está analizando; la función regresa el corte con menor costo y se compara con el menor costo obtenido hasta el momento, si es menor se guarda en `mejor`. Finalmente, se regresa `mejor` y los índices ordenados para poder identificar los elementos del hijo izquierdo y derecho.  \n\n::: {.cell execution_count=13}\n``` {.python .cell-code}\ndef corte(idx):\n    mejor = (np.inf, None, None)\n    orden = np.argsort(X[idx], axis=0)\n    labels = y[idx]\n    for i, x in enumerate(orden.T):\n        comp = corte_var(labels[x])\n        if comp[0] < mejor[0]:\n            mejor = (comp[0], i, comp[1])\n    return mejor, idx[orden[:, mejor[1]]]\n```\n:::\n\n\nCon la función `corte` se puede encontrar los parámetros de la función de corte $f_m(\\mathbf x) = x_i \\leq a$ para cada nodo del árbol completo del ejemplo anterior. Por ejemplo, los parámetros de la función de decisión para la raíz (#0) que se observa en la @fig-arboles-arbol-decision-funcion se puede obtener con el siguiente código. \n\n::: {.cell execution_count=14}\n``` {.python .cell-code}\nbest, orden = corte(np.arange(X.shape[0]))\nperf, i, j = best\n(X[orden[j], i] + X[orden[j-1], i]) / 2\n```\n\n::: {.cell-output .cell-output-display execution_count=14}\n```\n10.560464864725406\n```\n:::\n:::\n\n\nLa variable `orden` tiene la información para dividir el conjunto dado, lo cual se realiza en las siguientes instrucciones, donde `idx_i` corresponde a los elementos a la izquierda y `idx_d` son los de la derecha. \n\n::: {.cell execution_count=15}\n``` {.python .cell-code}\nidx_i = orden[:j]\nidx_d = orden[j:]\n```\n:::\n\n\nTeniendo los elementos a la izquierda y derecha, se puede calcular los parámetros de la función de corte del nodo #1 los cuales se pueden calcular con las siguientes instrucciones. \n\n::: {.cell execution_count=16}\n``` {.python .cell-code}\nbest, orden = corte(idx_i)\nperf, i, j = best\n(X[orden[j], i] + X[orden[j-1], i]) / 2\n```\n\n::: {.cell-output .cell-output-display execution_count=16}\n```\n-1.8516821607950367\n```\n:::\n:::\n\n\n::: {.callout-note}\nLa función `corte` no verifica que se esté en una hoja, entonces si se hace el corte en una hora regresará `(np.inf, none, None)`\n:::\n\n### Ejemplo: Breast Cancer Wisconsin\n\nSe utiliza el conjunto de datos de Breast Cancer Wisconsin para ejemplificar el algoritmo de Árboles de Decisión. Las siguientes instrucciones se descargan los datos y se dividen en los conjuntos de entrenamiento y prueba.\n\n::: {.cell execution_count=17}\n``` {.python .cell-code}\nX, y = load_breast_cancer(return_X_y=True)\nT, G, y_t, y_g = train_test_split(X, y, test_size=0.2)\n```\n:::\n\n\nLa siguiente instrucción entrena un árbol de decisión utilizando como función de costo la entropía. En la librería se encuentran implementadas otras funciones como el coeficiente Gini y Entropía Cruzada @sec-entropia-cruzada (_Log-loss_). \n\n::: {.cell execution_count=18}\n``` {.python .cell-code}\narbol = tree.DecisionTreeClassifier(criterion='entropy').fit(T, y_t)\n```\n:::\n\n\nComo es de esperarse la predicción se realiza con el método `predict` como se ve a continuación. \n\n::: {.cell execution_count=19}\n``` {.python .cell-code}\nhy = arbol.predict(G)\n```\n:::\n\n\n\n\nEl error en el conjunto de prueba $\\mathcal G$ es $0.0439$, se puede comparar este error con otros algoritmos utilizados en este conjunto como clasificadores paramétricos basados en distribuciones Gausianas (@sec-gaussina-perf-breast_cancer). La siguiente instrucción muestra el cálculo del error. \n\n::: {.cell execution_count=21}\n``` {.python .cell-code}\nerror = (y_g != hy).mean()\n```\n:::\n\n\nUn dato interesante, considerando los parámetros con los que se inicializó el árbol, entonces este hizo que todas las hojas fueran puras, es decir, con entropía cero. Por lo tanto el error de clasificación en el conjunto de entrenamiento $\\mathcal T$ es cero, como se puede verificar con el siguiente código. \n\n::: {.cell execution_count=22}\n``` {.python .cell-code}\n(y_t != arbol.predict(T)).mean()\n```\n\n::: {.cell-output .cell-output-display execution_count=22}\n```\n0.0\n```\n:::\n:::\n\n\n## Regresión\n\nLos árboles de decisión aplicados a problemas de regresión siguen una idea equivalente a los desarrollados en problemas de clasificación. Para ejemplificar las diferencias se utiliza el siguiente problema sintético; el cual corresponde a la suma de un seno y un coseno como se muestra a continuación. \n\n::: {.cell execution_count=23}\n``` {.python .cell-code}\nX = np.linspace(-5, 5, 100)\ny = np.sin(X) + 0.3 * np.cos(X * 3.)\n```\n:::\n\n\nCon este problema se genera un árbol de decisión utilizando la siguiente instrucción. El método `fit` espera recibir un arreglo en dos dimensiones por eso se usa la función `np.atleast_2d` y se calcula la transpuesta siguiendo el formato esperado. Se observa el uso del parámetro `max_depth` para limitar la profundidad del árbol de decisión. \n\n::: {.cell execution_count=24}\n``` {.python .cell-code}\narbol = tree.DecisionTreeRegressor(max_depth=3).fit(np.atleast_2d(X).T, y)\n```\n:::\n\n\nEl árbol de decisión obtenido se muestra en la @fig-arboles-regresion. La información que se muestra en cada nodo interno es equivalente a la mostrada en los árboles de clasificación. La diferencia es que en los árboles de regresión se muestra el promedio (`value`) de las salidas que llegan a ese nodo y en regresión es la frecuencia de clases. Se observa que si la entrada es $x=-4.5$ entonces la respuesta la da el nodo #4 con un valor de $1.088.$\n\n::: {.cell execution_count=25}\n\n::: {.cell-output .cell-output-display}\n![Árbol de Regresión](08Arboles_files/figure-pdf/fig-arboles-regresion-output-1.pdf){#fig-arboles-regresion}\n:::\n:::\n\n\n### Predicción\n\nEl árbol anterior se usa para predecir todos los puntos del conjunto de entrenamiento, el resultado se muestra en la @fig-arboles-regresion-func. Se observa que la predicción es discreta, son escalones y esto es porque las hojas predicen el promedio de los valores que llegaron hasta ahí, en este caso el árbol tiene 8 hojas entonces a lo más ese árbol puede predecir 8 valores distintos. \n\n::: {.cell execution_count=26}\n\n::: {.cell-output .cell-output-display}\n![Problema de regresión](08Arboles_files/figure-pdf/fig-arboles-regresion-func-output-1.pdf){#fig-arboles-regresion-func}\n:::\n:::\n\n\n### Entrenamiento\n\nCon respecto al proceso de entrenamiento la diferencia entre clasificación y regresión se encuentra en la función de costo que guía el proceso de optimización. En el caso de clasificación la función de costo era la esperanza de la entropía. Por otro lado, en regresión una función de costo utilizada es la varianza que es el error cuadrático que se muestra en los nodos. Para ejemplificar el uso de esta función de costo se utilizan los datos de Diabetes tal y como se muestran en las siguientes instrucciones. \n\n::: {.cell execution_count=27}\n``` {.python .cell-code}\nX, y = load_diabetes(return_X_y=True)\nT, G, y_t, y_g = train_test_split(X, y, test_size=0.2)\n```\n:::\n\n\nCon los datos de entrenamiento se genera el siguiente árbol de decisión para regresión. Solamente se muestran la información de la raíz y sus dos hijos. En la raíz se observa los parámetros de la función de corte, se selecciona la variable con índice 8 y se envían 235 elementos al hijo izquierdo y el resto al hijo derecho. \n\n::: {.cell execution_count=28}\n``` {.python .cell-code}\narbol = tree.DecisionTreeRegressor().fit(T, y_t)\n_ = tree.plot_tree(arbol, max_depth=1)\n```\n\n::: {.cell-output .cell-output-display}\n![](08Arboles_files/figure-pdf/cell-29-output-1.pdf){fig-pos='H'}\n:::\n:::\n\n\nEl siguiente método implementa la función de corte para regresión se puede observar que la única diferente con la función `corte_var` definida en clasificación (@sec-arboles-clasificacion) es que la entropía `H` se cambia por la varianza `np.var`. \n\n::: {.cell execution_count=29}\n``` {.python .cell-code}\ndef corte_var(response):\n    mejor = (np.inf, None)\n    D_m = response.shape[0]\n    corte = np.where(np.diff(response))[0] + 1\n    for j in corte:\n        izq = response[:j]\n        der = response[j:]\n        a = (izq.shape[0] / D_m) * np.var(izq)\n        b = (der.shape[0] / D_m) * np.var(der)\n        perf = a + b\n        if perf < mejor[0]:\n          mejor = (perf, j)\n    return mejor    \n```\n:::\n\n\nLa función `corte_var` de regresión se utiliza para encontrar el punto de corte en los datos del conjunto de entrenamiento de la siguiente manera. En la primera línea se ordenan las variables independientes y en la segunda línea se itera por todas las variables independientes para calcular el corte con costo mínimo. \n\n::: {.cell execution_count=30}\n``` {.python .cell-code}\norden = T.argsort(axis=0)\nres = [corte_var(y_t[orden[:, x]]) for x in range(10)]\nres\n```\n\n::: {.cell-output .cell-output-display execution_count=30}\n```\n[(5821.870111316635, 192),\n (6051.777858634163, 327),\n (4439.784729909201, 213),\n (5024.621906234658, 249),\n (5783.246762498026, 208),\n (5907.906566527737, 239),\n (5242.6465777420835, 146),\n (5097.395681935321, 137),\n (4170.906105558959, 174),\n (5341.358768379872, 273)]\n```\n:::\n:::\n\n\nEl resultado de ejecutar el código anterior se muestra a continuación; donde se observa que el costo mínimo corresponde a la variable con índice 8 tal y como se muestra en la figura anterior nodo derecho de la raíz. \n\n",
    "supporting": [
      "08Arboles_files/figure-pdf"
    ],
    "filters": []
  }
}