{
  "hash": "808df589663e0b1314dea4a6736dcd1b",
  "result": {
    "markdown": "# Discriminantes Lineales {#sec-discriminantes-lineales}\n\nEl **objetivo** de la unidad es conocer y aplicar diferentes métodos lineales de discriminación para atacar problemas de clasificación.\n\n## Paquetes usados\n\n::: {#bf08e957 .cell execution_count=1}\n``` {.python .cell-code}\nfrom sklearn.svm import LinearSVC, SVC\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.decomposition import PCA\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import f1_score, recall_score, precision_score\nfrom scipy.stats import multivariate_normal\nfrom sklearn.datasets import load_iris\nfrom matplotlib import pylab as plt\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\n```\n:::\n\n\n\n\n::: {.content-visible when-format=\"html\"}\n\n---\n\n\n{{< video https://www.youtube.com/embed/BgLm0tVxW8A width=\"560\" height=\"315\" >}}\n\n\n\n---\n\n:::\n\n## Introducción\n\nEn unidades anteriores se han visto diferentes técnicas para discriminar entre clases; en particular se ha descrito el uso de la probabilidad $\\mathbb P(\\mathcal Y \\mid \\mathcal X)$ para encontrar la clase más probable. Los parámetros de $\\mathbb P(\\mathcal Y \\mid \\mathcal X)$ se han estimado utilizando métodos [paramétricos](@sec-metodos-parametricos) y [no paramétricos](@sec-metodos-no-parametricos). En está unidad se describe el uso de funciones discriminantes para la clasificación y su similitud con el uso de $\\mathbb P(\\mathcal Y \\mid \\mathcal X).$\n\n## Función Discriminante {#sec-discriminante}\n\nEn la unidad de Teoría de Decisión Bayesiana (@sec-teoria-decision-bayesianas) se describió el uso de $\\mathbb P(\\mathcal Y \\mid \\mathcal X)$ para clasificar, se mencionó que la clase a la que pertenece $\\mathcal X=x$ es la de mayor probabilidad, es decir,  \n\n$$\nC(x) = \\textsf{argmax}_{k=1}^K \\mathbb P(\\mathcal Y=k \\mid \\mathcal X=x),\n$$\n\ndonde $K$ es el número de clases y $\\mathcal Y=k$ representa la $k$-ésima clase. Considerando que la [evidencia](@eq-evidencia) es un factor que normaliza, entonces, $C(x)$ se puede definir de la siguiente manera. \n\n$$\nC(x) = \\textsf{argmax}_{k=1}^K \\mathbb P(\\mathcal X=x \\mid \\mathcal Y=k)\\mathbb P(\\mathcal Y=k).\n$$\n\nAgrupando la probabilidad a priori y verosimilitud en una función $g_k,$ es decir, $g_k(x) = P(\\mathcal X=x \\mid \\mathcal Y=k)\\mathbb P(\\mathcal Y=k),$  hace que $C(x)$ se sea:\n\n$$\nC(x) = \\textsf{argmax}_{k=1}^K g_k(x).\n$$\n\nObservando $C(x)$ y olvidando los pasos utilizados para derivarla, uno se puede imaginar que lo único necesario para generar un clasificador de $K$ clases es definir un conjunto de functions $g_k$ que separen las clases correctamente. En esta unidad se presentan diferentes maneras para definir $g_k$ con la característica de que todas ellas son lineales, e.g., $g_k(\\mathbf x) = \\mathbf w_k \\cdot \\mathbf x + w_{k_0}.$\n\n### Clasificación Binaria {#sec-binaria}\n\nLa descripción de discriminantes lineales empieza con el caso particular de dos clases, i.e., $K=2$. En este caso $C(\\mathbf x)$ es encontrar el máximo de las dos funciones $g_1$ y $g_2$. Una manear equivalente sería definir a $C(\\mathbf x)$ como \n\n$$\nC(\\mathbf x) = \\textsf{sign}(g_1(\\mathbf x) - g_2(\\mathbf x)),\n$$\n\ndonde $\\textsf{sign}$ es la función que regresa el signo, entonces solo queda asociar el signo positivo a la clase 1 y el negativo a la clase 2. Utilizando esta definición se observa lo siguiente\n\n$$\n\\begin{split}\n    g_1(\\mathbf x) - g_2(\\mathbf x) &= (\\mathbf w_1 \\cdot \\mathbf x + w_{1_0}) - (\\mathbf w_2 \\cdot \\mathbf x + w_{2_0}) \\\\\n         &= (\\mathbf w_1 + \\mathbf w_2) \\cdot \\mathbf x + (w_{1_0} - w_{2_0}) \\\\\n         &= \\mathbf w \\cdot \\mathbf x + w_0\n\\end{split},\n$$\n\ndonde se concluye que para el caso binario es necesario definir solamente una función discriminante y que los parámetros de esta función son $\\mathbf w$ y $\\mathbf w_0.$ Otra característica que se ilustra es que el parámetro $\\mathbf w_0$ está actuando como un umbral, es decir, $\\mathbf x$ corresponde a la clase positiva si $\\mathbf w \\cdot \\mathbf x > -w_0.$\n\nEn la @fig-lineal-discriminante se observa el plano (linea) que divide las dos clases, este plano representa los puntos que satisfacen $g(\\mathbf x)=0$. \n\n::: {#cell-fig-lineal-discriminante .cell execution_count=3}\n``` {.python .cell-code code-fold=\"true\"}\nX_1 = multivariate_normal(mean=[15, 20],\n                          seed=0,\n                          cov=[[3, -3], [-3, 8]]).rvs(1000)\nX_2 = multivariate_normal(mean=[5, 5],\n                          seed=0,\n                          cov=[[4, 0], [0, 2]]).rvs(1000)\nT = np.concatenate((X_1, X_2))\ny_t = np.array(['P'] * X_1.shape[0] + ['N'] * X_2.shape[0])\nlinear = LinearSVC(dual=False).fit(T, y_t)\nw_1, w_2 = linear.coef_[0]\nw_0 = linear.intercept_[0]\ng_0 = [dict(x1=x, x2=y, tipo='g(x)=0')\n       for x, y in zip(T[:, 0], (-w_0 - w_1 * T[:, 0]) / w_2)]\ndf = pd.DataFrame(g_0 + \\\n                  [dict(x1=x, x2=y, clase='P') for x, y in X_1] + \\\n                  [dict(x1=x, x2=y, clase='N') for x, y in X_2]\n                 )\nax = sns.scatterplot(data=df, x='x1', y='x2', hue='clase', legend=True)\nsns.lineplot(data=df, x='x1', y='x2', ax=ax, hue='tipo', palette=['k'], legend=True)\n_ = ax.axis('equal')\n```\n\n::: {.cell-output .cell-output-display}\n![Función Discriminante](09Lineal_files/figure-html/fig-lineal-discriminante-output-1.png){#fig-lineal-discriminante width=582 height=428}\n:::\n:::\n\n\n### Geometría de la Función de Decisión {#sec-geometria-funcion-decision}\n\nLa función discriminante $g(\\mathbf x) = \\mathbf w \\cdot \\mathbf x + w_0$ tiene una representación gráfica. Lo primero que se observa es que los parámetros $\\mathbf w$ viven en al mismo espacio que los datos, tal y como se puede observar en la @fig-lineal-repr-df. \n\n::: {#cell-fig-lineal-repr-df .cell execution_count=4}\n``` {.python .cell-code code-fold=\"true\"}\n_ = pd.DataFrame([dict(x1=w_1, x2=w_2, clase='w')])\ndf = pd.concat((df, _), axis=0)\nax = sns.scatterplot(data=df, x='x1', y='x2', hue='clase', legend=True)\nsns.lineplot(data=df, x='x1', y='x2', ax=ax, hue='tipo', palette=['k'], legend=True)\n_ = ax.axis('equal')\n```\n\n::: {.cell-output .cell-output-display}\n![Función discriminante](09Lineal_files/figure-html/fig-lineal-repr-df-output-1.png){#fig-lineal-repr-df width=582 height=428}\n:::\n:::\n\n\nSiguiendo con la descripción, los parámetros $\\mathbf w$ y la función $g(\\mathbf x)$ son ortogonales, tal y como se muestra en la @fig-lineal-ort. Analiticamente la ortogonalidad se define de la siguiente manera. Sea $\\mathbf x_a$ y $\\mathbf x_b$ dos puntos en $g(\\mathbf x)=0$, es decir, \n\n$$\n\\begin{split}\ng(\\mathbf x_a) &= g(\\mathbf x_b) \\\\\n\\mathbf w \\cdot \\mathbf x_a + w_0 &= \\mathbf w \\cdot \\mathbf x_b + w_0\\\\\n\\mathbf w \\cdot (\\mathbf x_a -  \\mathbf x_b) &= 0,\n\\end{split}\n$$\n\ndonde el vector $\\mathbf x_a -  \\mathbf x_b$ es paralelo a $g(\\mathbf x)=0$, ortogonal a $\\mathbf w$ y el sub-espacio generado por $\\mathbf w \\cdot (\\mathbf x_a -  \\mathbf x_b) = 0$ pasa por el origen. \n\n::: {#cell-fig-lineal-ort .cell execution_count=5}\n``` {.python .cell-code code-fold=\"true\"}\nw = np.array([w_1, w_2]) / np.linalg.norm([w_1, w_2])\nlen_0 = w_0 / np.linalg.norm([w_1, w_2])\ndf = pd.DataFrame(g_0 + \\\n                  [dict(x1=x, x2=y, clase='P') for x, y in X_1] + \\\n                  [dict(x1=x, x2=y, clase='N') for x, y in X_2] + \\\n                  [dict(x1=0, x2=0, tipo='lw'),\n                   dict(x1=-w[0]*len_0, x2=-w[1]*len_0, tipo='lw')]\n                 )\nax = sns.scatterplot(data=df, x='x1', y='x2', hue='clase', legend=True)\nsns.lineplot(data=df, x='x1', y='x2', ax=ax, hue='tipo',\n             palette=['k'] + sns.color_palette()[2:3],\n             legend=True)\n_ = ax.axis('equal')\n```\n\n::: {.cell-output .cell-output-display}\n![Visualizando que $\\mathbf w$ y la función discriminante son ortogonales.](09Lineal_files/figure-html/fig-lineal-ort-output-1.png){#fig-lineal-ort width=582 height=428}\n:::\n:::\n\n\nEn la figura anterior, $\\ell \\mathbf w$ corresponde al vector $\\mathbf w$ multiplicado por un factor $\\ell$ de tal manera que intersecte con $g(\\mathbf x)=0.$ El factor $\\ell$ corresponde a la distancia que hay del origen a $g(\\mathbf x)=0$ la cual es $\\ell = \\frac{w_0}{\\mid\\mid \\mathbf w \\mid\\mid}.$ El signo de $\\ell$ indica el lado donde se encuentra el origen con respecto a $g(\\mathbf x)=0.$\n\nLa @fig-lineal-dist-hyp muestra en rojo la línea generada por $\\mathbf w \\cdot \\mathbf x=0$, la función discriminante $g(\\mathbf x)=0$ (negro), la línea puntuada muestra la distancia entre ellas, que corresponde a $\\ell$ y el vector $\\mathbf w$. Visualmente, se observa que $\\mathbf w$ está pegado a la línea roja, pero esto solo es un efecto de la resolución y estos elementos no se tocan. \n\n::: {#cell-fig-lineal-dist-hyp .cell execution_count=6}\n``` {.python .cell-code code-fold=\"true\"}\nvec = np.array([1, (- w_1 * 1) / w_2])\nx_max = T[:, 0].max()\nlength = np.linalg.norm(np.array([x_max, (-w_0 - w_1 * x_max) / w_2]) -\n                        np.array([-w[0]*len_0, -w[1]*len_0]))\nvec_der = length * vec / np.linalg.norm(vec)\nx_min = T[:, 0].min()\nlength = np.linalg.norm(np.array([x_min, (-w_0 - w_1 * x_min) / w_2]) -\n                        np.array([-w[0]*len_0, -w[1]*len_0]))\nvec_izq = -length * vec / np.linalg.norm(vec)\n\ng = [dict(x1=x, x2=(- w_1 * x) / w_2, tipo='wx=0')\n     for x in np.linspace(vec_izq[0], vec_der[0])]\ndf = pd.DataFrame([dict(x1=x, x2=y, clase='P') for x, y in X_1] + \\\n                  [dict(x1=x, x2=y, clase='N') for x, y in X_2] +\\\n                  [dict(x1=w_1, x2=w_2, clase='w')] +\\\n                  g_0 + g)\nax = sns.scatterplot(data=df, x='x1', y='x2', hue='clase', legend=True)\nsns.lineplot(data=df, x='x1', y='x2', ax=ax, hue='tipo',\n             palette=['k'] + sns.color_palette()[3:4],\n             legend=True)\nax.plot([vec_der[0], x_max], [vec_der[1], (-w_0 - w_1 * x_max) / w_2], '--',\n        color=sns.color_palette()[4])\n_ = ax.axis('equal')\n```\n\n::: {.cell-output .cell-output-display}\n![Geometría de la función discriminante.](09Lineal_files/figure-html/fig-lineal-dist-hyp-output-1.png){#fig-lineal-dist-hyp width=582 height=427}\n:::\n:::\n\n\nFinalmente, será de utilidad representar a cada punto en $\\mathcal D$ de la siguiente manera \n\n$$\n\\mathbf x = \\mathbf x_g + \\ell \\frac{\\mathbf w}{\\mid\\mid \\mathbf w \\mid\\mid},\n$$\n\ndonde $\\mathbf x_g$ corresponde a la proyección en el hiperplano ($g(\\mathbf x) = 0$) de $\\mathbf x$ y $\\ell$ es la distancia que hay del hiperplano a $\\mathbf x$. Utilizando esta representación se puede derivar la distancia $\\ell$ de $\\mathbf x$ con el siguiente procedimiento. \n\n$$\n\\begin{split}\ng(\\mathbf x) &= g(\\mathbf x_g + \\ell \\frac{\\mathbf w}{\\mid\\mid \\mathbf w \\mid\\mid})\\\\\n&= \\mathbf w \\cdot (\\mathbf x_g + \\ell \\frac{\\mathbf w}{\\mid\\mid \\mathbf w \\mid\\mid}) + w_0\\\\\n&= \\mathbf w \\cdot (\\mathbf x_g + \\ell \\frac{\\mathbf w}{\\mid\\mid \\mathbf w \\mid\\mid})\\\\\n&= \\mathbf w \\cdot \\mathbf x_g + \\ell \\mathbf w \\cdot \\frac{\\mathbf w}{\\mid\\mid \\mathbf w \\mid\\mid}\\\\\n&= \\ell \\mathbf w \\cdot \\frac{\\mathbf w}{\\mid\\mid \\mathbf w \\mid\\mid}\\\\\n&= \\ell \\mid\\mid\\mathbf w\\mid\\mid\\\\\n\\ell &= \\frac{g(\\mathbf x)}{\\mid\\mid\\mathbf w \\mid\\mid}\n\\end{split}\n$$ {#eq-distancia-hiperplano}\n\nComo ya se había visto la distancia del origen al hiperplano está dada por $\\ell_0 = \\frac{w_0}{\\mid\\mid\\mathbf w \\mid\\mid}$ y de cualquier elemento por $\\ell_{\\mathbf x} = \\frac{g(\\mathbf x)}{\\mid\\mid\\mathbf w \\mid\\mid}.$ La @fig-lineal-dis-ele muestra la $\\ell_{\\mathbf x}$ en un elemento de la clase negativa. Se puede observar el punto $\\mathbf x_g$ que es donde intersecta la línea con el hiperplano.\n\n::: {#cell-fig-lineal-dis-ele .cell execution_count=7}\n``` {.python .cell-code code-fold=\"true\"}\npoint = X_2[X_2.argmax(axis=0)[1]]\npoint_g = vec *  np.dot(point, vec) / np.dot(vec, vec) - len_0 * w\ndf = pd.DataFrame(g_0 + \\\n                  [dict(x1=x, x2=y, clase='P') for x, y in X_1] +\\\n                  [dict(x1=x, x2=y, clase='N') for x, y in X_2] +\\\n                  [dict(x1=point_g[0], x2=point_g[1], tipo='lx')] +\\\n                  [dict(x1=point[0], x2=point[1], tipo='lx')]                  \n                 )\nax = sns.scatterplot(data=df, x='x1', y='x2', hue='clase', legend=True)\nsns.lineplot(data=df, x='x1', y='x2', ax=ax,\n             hue='tipo',palette=['k'] + sns.color_palette()[4:5], legend=True)\n_ = ax.axis('equal')\n```\n\n::: {.cell-output .cell-output-display}\n![Distancia ($\\ell_{\\mathbf x} = x$) de un elemento al hiperplano](09Lineal_files/figure-html/fig-lineal-dis-ele-output-1.png){#fig-lineal-dis-ele width=582 height=428}\n:::\n:::\n\n\nConsiderando que el problema mostrado en la figura anterior está en $\\mathbb R^2$, entonces $\\mathbf x_g$ está dado por \n\n$$\n\\mathbf x_g = \\frac{\\mathbf x \\cdot \\mathbf x_0}{\\mathbf x_0 \\cdot \\mathbf x_0} \\mathbf x_0 - \\ell_0 \\frac{\\mathbf w}{\\mid\\mid\\mathbf w \\mid\\mid},\n$$\n\ndonde $\\ell_0$ es la distancia del origen al hiperplano y $\\mathbf x_0$ es cualquier vector que está en $\\mathbf x_0 \\cdot \\mathbf w=0.$ Para dimensiones mayores el término $\\frac{\\mathbf x \\cdot \\mathbf x_0}{\\mathbf x_0 \\cdot \\mathbf x_0}$ es la proyección al hiperplano $A$ tal que $A \\mathbf w = 0.$\n\n### Múltiples Clases {#sec-multiples-clases}\n\nUna manera de tratar un problema de $K$ clases, es convertirlo en $K$ problemas de clasificación binarios, a este procedimiento se le conoce como _Uno vs Resto_. La idea es entrenar $K$ clasificadores donde la clase positiva corresponde a cada una de las clases y la clase de negativa se construye con todas las clases que no son la clase positiva en esa iteración. Finalmente, la clase predicha corresponde al clasificador que tiene el valor máximo en la función discriminante. \n\nLa @fig-lineal-multiclase ejemplifica el comportamiento de esta técnica en un problema de tres clases y utilizando un clasificador con discrimitante lineal. En la figura se muestra las tres funciones discriminantes $g_k(\\mathbf x)=0$, los parámetros escalados de esas funciones, i.e., $\\ell_k \\mathbf w_k$ y los datos. Por ejemplo se observa como la clase $1$ mostrada en azul, se separa de las otras dos clases con la función $g_1(\\mathbf x)=0$, es decir, para $g_1(\\mathbf x)=0$ la clase positiva es $1$ y la clase negativa corresponde a los elementos que corresponde a las clases $2$ y $3.$\n\n::: {#cell-fig-lineal-multiclase .cell execution_count=8}\n``` {.python .cell-code code-fold=\"true\"}\nseed = 3\nX_1 = multivariate_normal(mean=[15, 20],\n                          seed=seed,\n                          cov=[[3, -3], [-3, 8]]).rvs(1000)\nX_2 = multivariate_normal(mean=[5, 5],\n                          seed=seed,\n                          cov=[[4, 0], [0, 2]]).rvs(1000)\nX_3 = multivariate_normal(mean=[-5, 20],\n                          seed=seed,\n                          cov=[[2, 1], [1, 2]]).rvs(1000)\n\nT = np.concatenate((X_1, X_2, X_3))\ny_t = np.array(['1'] * X_1.shape[0] + ['2'] * X_2.shape[0] + ['3'] * X_3.shape[0])\nlinear = LinearSVC(dual=False).fit(T, y_t)\ng_0 = []\nfor i, (w, w_0) in enumerate(zip(linear.coef_, linear.intercept_)):\n    w_1, w_2 = w\n    g_0 += [dict(x1=x, x2=y, tipo=f'g{i+1}(x)=0')\n            for x, y in zip(T[:, 0], (-w_0 - w[0] * T[:, 0]) / w[1])]\nW = [-w0 * w / np.linalg.norm(w)**2 \n     for w, w0 in zip(linear.coef_, linear.intercept_)]    \ndf = pd.DataFrame(g_0 + \\\n                  [dict(x1=x, x2=y, clase='1') for x, y in X_1] +\\\n                  [dict(x1=x, x2=y, clase='2') for x, y in X_2] +\\\n                  [dict(x1=x, x2=y, clase='3') for x, y in X_3] +\\\n                  [dict(x1=w_1, x2=w_2, clase=f'lw{i+1}')\n                   for i, (w_1, w_2) in enumerate(W)]                  \n                 )\nax = sns.scatterplot(data=df, x='x1', y='x2', hue='clase', legend=True)\nsns.lineplot(data=df, x='x1', y='x2', ax=ax, hue='tipo',\n             palette=sns.color_palette()[6:9], legend=True)\n_ = ax.axis('equal')\n```\n\n::: {.cell-output .cell-output-display}\n![Problema multiclase](09Lineal_files/figure-html/fig-lineal-multiclase-output-1.png){#fig-lineal-multiclase width=582 height=427}\n:::\n:::\n\n\n## Máquinas de Soporte Vectorial {#sec-svm}\n\nEs momento de describir algunos algoritmos para estimar los parámetros $\\mathbf w$, y $w_0$ empezando por las máquinas de soporte vectorial. En este clasificador se asume un problema binario y las clases están representadas por $-1$ y $1$, es decir, $y \\in \\{-1, 1\\}$. Entonces, las máquinas de soporte vectorial tratan de encontrar una función con las siguientes características. \n\nSea $\\mathbf x_i$ un ejemplo que corresponde a la clase $1$ entonces se busca $\\mathbf w$ tal que\n\n$$\n\\mathbf w \\cdot \\mathbf x_i + w_0 \\geq +1.\n$$\n\nEn el caso contrario, es decir, $\\mathbf x_i$ un ejemplo de la clase $-1$, entonces \n\n$$\n\\mathbf w \\cdot \\mathbf x_i + w_0 \\leq -1.\n$$\n\nEstas ecuaciones se pueden escribir como \n\n$$\n(\\mathbf w \\cdot \\mathbf x_i + w_0) y_i \\geq +1,\n$$\n\ndonde $(\\mathbf x_i, y_i) \\in \\mathcal D.$ \n\nLa función discriminante es $g(\\mathbf x) = \\mathbf w \\cdot \\mathbf x + w_0$ y la distancia (@eq-distancia-hiperplano) que existe entre cualquier punto $\\mathbf x_i$ al discriminante está dada por \n\n$$\n\\frac{g(\\mathbf x_i)}{\\mid\\mid \\mathbf w \\mid\\mid}y_i.\n$$\n\nEntonces, se puede ver que lo que se busca es encontrar $\\mathbf w$ de tal manera que cualquier punto $\\mathbf x_i$ esté lo mas alejada posible del discriminante, esto se logra minimizando $\\mathbf w$, es decir, resolviendo el siguiente problema de optimización:\n\n$$\n\\min \\frac{1}{2} \\mid\\mid\\mathbf w \\mid\\mid\n$$\n\nsujeto a $(\\mathbf w \\cdot \\mathbf x_i + w_0) y_i \\geq +1, \\forall (\\mathbf x_i, y_i) \\in \\mathcal D.$\n\n### Optimización\n\nEste es un problema de optimización que se puede resolver utilizando multiplicadores de Lagrange lo cual quedaría como \n\n$$\nf_p = \\frac{1}{2}\\mid\\mid\\mathbf w \\mid\\mid - \\sum_i^N \\alpha_i ((\\mathbf w \\cdot \\mathbf x_i + w_0) y_i - 1),\n$$\n\ndonde el mínimo corresponde a maximizar con respecto a $\\alpha_i \\geq 0$ y minimizar con respecto a $\\mathbf w$ y $w_0.$ En esta formulación existe el problema para aquellos problemas donde no es posible encontrar un hiperplano que separa las dos clases. Para estos casos donde no es posible encontrar una separación perfecta se propone utilizar \n\n$$\n(\\mathbf w \\cdot \\mathbf x_i + w_0) y_i \\geq 1 - \\xi_i,\n$$ \n\ndonde $\\xi$ captura los errores empezando por aquellos elementos que están del lado correcto del hiperplano, pero que no son mayores a $1$. La @fig-lineal-xi muestra un ejemplo donde existe un elemento negativo que se encuentra entre la función de decisión y el hiperplano de margen, i.e., el que corresponde a la restricción $\\mathbf w \\cdot \\mathbf x_i + w_0 \\geq 1$, es decir ese punto tiene un $0 < \\xi < 1.$ También se observa un elemento positivo que está muy cerca a $g(\\mathbf x) = 1.$\n\n::: {#cell-fig-lineal-xi .cell execution_count=9}\n``` {.python .cell-code code-fold=\"true\"}\nseed = 2\nX_1 = multivariate_normal(mean=[15, 20], cov=[[3, -3], [-3, 8]], seed=seed).rvs(1000)\nX_2 = multivariate_normal(mean=[8, 8], cov=[[4, 0], [0, 2]], seed=seed).rvs(1000)\nT = np.concatenate((X_1, X_2))\ny_t = np.array(['P'] * X_1.shape[0] + ['N'] * X_2.shape[0])\n\nlinear = LinearSVC(dual=False).fit(T, y_t)\nw_1, w_2 = linear.coef_[0]\nw_0 = linear.intercept_[0]\nw = np.array([w_1, w_2]) / np.linalg.norm([w_1, w_2])\ng_0 = [dict(x1=x, x2=y, tipo='g(x)=0')\n       for x, y in zip(T[:, 0], (-w_0 - w_1 * T[:, 0]) / w_2)]\ng_p = [dict(x1=p['x1'] + w[0], x2=p['x2'] + w[1], tipo='g(x)=1')\n       for p in g_0]\ng_n = [dict(x1=p['x1'] - w[0], x2=p['x2'] - w[1], tipo='g(x)=-1')\n       for p in g_0]\ndf = pd.DataFrame(g_0 + g_p + g_n +\\\n                  [dict(x1=x, x2=y, clase='P') for x, y in X_1] + \\\n                  [dict(x1=x, x2=y, clase='N') for x, y in X_2]\n                 )\nax = sns.scatterplot(data=df, x='x1', y='x2', hue='clase', legend=True)\nsns.lineplot(data=df, x='x1', y='x2', ax=ax,\n             hue='tipo', palette=['k'] + sns.color_palette()[2:4], legend=True)\n_ = ax.axis('equal')\n```\n\n::: {.cell-output .cell-output-display}\n![Hiperplanos](09Lineal_files/figure-html/fig-lineal-xi-output-1.png){#fig-lineal-xi width=582 height=427}\n:::\n:::\n\n\nContinuando con el problema de optimización, en las condiciones anteriores la función a optimizar es $\\min \\frac{1}{2} \\mid\\mid\\mathbf w \\mid\\mid + C \\sum_i^N \\xi_i,$ utilizando multiplicadores de Lagrange queda como\n\n$$\nf_p = \\frac{1}{2}\\mid\\mid\\mathbf w \\mid\\mid - \\sum_i^N \\alpha_i ((\\mathbf w \\cdot \\mathbf x_i + w_0) y_i - 1 + \\xi_i) - \\sum_i^N \\beta_i \\xi_i.\n$$\n\nSe observa que el parámetro $C$ controla la penalización que se hace a los elementos que se encuentran en el lado incorrecto del hiperplano o dentro del margen. La @fig-lineal-hip-c muestra el hiperplano generado utilizando $C=1$ y $C=0.01.$ Se observa como el elemento que está correctamente clasificado en $C=1$ pasa al lado incorrecto del hiperplano, ademas se ve como la función de decisión rota cuando el valor cambia. \n\n::: {#cell-fig-lineal-hip-c .cell execution_count=10}\n``` {.python .cell-code code-fold=\"true\"}\nfor k, (C, legend) in enumerate(zip([1, 0.01], [False, True])):\n     linear = LinearSVC(dual=False, C=C).fit(T, y_t)\n     w_1, w_2 = linear.coef_[0]\n     w_0 = linear.intercept_[0]\n     w = np.array([w_1, w_2]) / np.linalg.norm([w_1, w_2])\n     g_0 = [dict(x1=x, x2=y, tipo='g(x)=0')\n          for x, y in zip(T[:, 0], (-w_0 - w_1 * T[:, 0]) / w_2)]\n     g_p = [dict(x1=p['x1'] + w[0], x2=p['x2'] + w[1], tipo='g(x)=1')\n          for p in g_0]\n     g_n = [dict(x1=p['x1'] - w[0], x2=p['x2'] - w[1], tipo='g(x)=-1')\n          for p in g_0]\n     df = pd.DataFrame(g_0 + g_p + g_n +\\\n                    [dict(x1=x, x2=y, clase='P') for x, y in X_1] + \\\n                    [dict(x1=x, x2=y, clase='N') for x, y in X_2]\n                    )\n     ax = plt.subplot(1, 2, k + 1)\n     sns.scatterplot(data=df, x='x1', y='x2', hue='clase', legend=legend, ax=ax)\n     sns.lineplot(data=df, x='x1', y='x2', ax=ax,\n               hue='tipo', palette=['k'] + sns.color_palette()[2:4], legend=legend)\n     ax.axis('equal')\n     ax.set_title(f'C={C}')\n     if legend:\n          sns.move_legend(ax, \"upper left\", bbox_to_anchor=(1, 0.65))\n_ = plt.tight_layout()\n```\n\n::: {.cell-output .cell-output-display}\n![Hiperplanos para diferentes valores de $C$. Se observa que en $C=0.01$ se clasifica incorrectamente un elemento positivo.](09Lineal_files/figure-html/fig-lineal-hip-c-output-1.png){#fig-lineal-hip-c width=663 height=470}\n:::\n:::\n\n\n</details>\n\n\nEste problema de optimización cumple con todas las características para poder encontrar su solución optimizando el problema dual. El problema dual corresponde a maximizar $f_p$ con respecto a $\\alpha_i,$ sujeto a que las restricciones de que el gradiente de $f_p$ con respecto a $\\mid\\mid\\mathbf w \\mid\\mid$, $w_0$ y $\\xi_i$ sean cero. Utilizando estas características el problema dual corresponde a \n\n\n$$\nf_d = \\sum_i^N \\alpha_i - \\frac{1}{2} \\sum_i^N \\sum_j^N \\alpha_i \\alpha_j y_i y_j \\mathbf x_i \\cdot \\mathbf x_j,\n$$\n\nsujeto a las restricciones $\\sum_i^N \\alpha_i y_i = 0$ y $0 \\leq \\alpha_i \\leq C.$\n\nEl problema de optimización dual tiene unas características que lo hacen deseable en ciertos casos, por ejemplo, el problema depende del número de ejemplos ($N$) en lugar de la dimensión. Entonces en problemas donde $d > N$ es más conveniente utilizar el dual.\n\n### Kernel\n\nLa otra característica del problema dual es que permite visualizar lo siguiente. Suponiendo que se usa una función $\\phi: \\mathbb R^d \\leftarrow \\mathbf R^{\\hat d},$ de tal manera, que en el espacio $\\phi$ se puede encontrar un hiperplano que separa las clases. Incorporando la función $\\phi$ produce la siguiente función a optimizar \n\n$$\nf_d = \\sum_i^N \\alpha_i - \\frac{1}{2} \\sum_i^N \\sum_j^N \\alpha_i \\alpha_j y_i y_j \\phi(\\mathbf x_i) \\cdot \\phi(\\mathbf x_j),\n$$\n\ndonde primero se transforman todos los datos al espacio generado por $\\phi$ y después se calcula el producto punto. El producto punto se puede cambiar por una función **Kernel**, i.e., $K(\\mathbf x_i, \\mathbf x_j) = \\phi(\\mathbf x_i) \\cdot \\phi(\\mathbf x_j)$ lo cual hace que innecesaria la transformación al espacio $\\phi.$ Utilizando la función de kernel, el problema de optimización dual queda como:\n\n$$\nf_d = \\sum_i^N \\alpha_i - \\frac{1}{2} \\sum_i^N \\sum_j^N \\alpha_i \\alpha_j y_i y_j K(\\mathbf x_i, \\mathbf x_j).\n$$\n\nLa función discriminante está dada por $g(\\mathbf x) = \\sum_i^N \\alpha_i y_i K(\\mathbf x_i, \\mathbf x),$ donde aquellos elementos donde $\\alpha \\neq 0$ se les conoce como los vectores de soporte. Estos elementos son los que se encuentran en el margen, dentro del margen y en el lado incorrecto de la función discriminante.\n\nLa @fig-lineal-kernel muestra los datos del iris (proyectados con Análisis de Componentes Principales @sec-pca), las clases se encuentran en color azul, naranja y verde; en color rojo se muestran los vectores de soporte. La figura derecha muestra en color negro aquellos vectores de soporte que se encuentran en el lado incorrecto del hiperplano. Por otro lado se puede observar como los vectores de soporte separan las clases, del lado izquierdo se encuentran todos los elementos de la clase $0$, después se observan las clases $1$ y del lado derecho las clases $2$. Los vectores de soporte están en la frontera de las clases y los errores se encuentran entre las clases $1$ y $2$ que corresponden a las que no son linealmente separables. \n\n::: {#cell-fig-lineal-kernel .cell execution_count=11}\n``` {.python .cell-code code-fold=\"true\"}\nX, y = load_iris(return_X_y=True)\nlinear = SVC(kernel='poly', degree=2, C=10).fit(X, y)\nhy = linear.predict(X)\nD = PCA(n_components=2).fit_transform(X)\nmask = np.zeros(D.shape[0])\nmask[linear.support_] = True\ns = 'S'\ndf = pd.DataFrame([dict(x1=x1, x2=x2, tipo=f'{l if not c else s}', error=err)\n                   for (x1, x2), c, l, err in zip(D, mask, y, y != hy)])\nfor k, legend in enumerate([False, True]):     \n     if legend:\n          df.loc[df.error, 'tipo'] = 'X'\n     df.sort_values(by='tipo', inplace=True)\n     ax = plt.subplot(1, 2, k + 1)          \n     sns.scatterplot(data=df, x='x1', y='x2',\n                     palette=sns.color_palette()[:4] + ['k'],\n                     hue='tipo', legend=legend, ax=ax)\n     if legend:\n          sns.move_legend(ax, \"upper left\", bbox_to_anchor=(1, 0.65))\n_ = plt.tight_layout()\n```\n\n::: {.cell-output .cell-output-display}\n![Visualización de los vectores de soporte usando PCA.](09Lineal_files/figure-html/fig-lineal-kernel-output-1.png){#fig-lineal-kernel width=662 height=471}\n:::\n:::\n\n\n## Regresión Logística {#sec-regresion-logistica}\n\nEn clasificación binaria (@sec-binaria) se describió que la función discriminante se puede definir como la resta, i.e., $g_1(\\mathbf x) - g_2(\\mathbf x);$ equivalentemente se pudo haber seleccionado la división ($\\frac{g_1(\\mathbf x)}{g_2(\\mathbf x)}$) para generar la función discriminante o el logaritmo de la división, i.e., $\\log \\frac{g_1(\\mathbf x)}{g_2(\\mathbf x)}.$ Esta última ecuación en el caso de $g_i(\\mathbf x)=\\mathbb P(\\mathcal Y=i \\mid \\mathcal X=\\mathbf x)$ corresponde a la función $\\textsf{logit}$, tal y como se muestra a continuación.\n\n$$\n\\begin{split}\n\\log \\frac{\\mathbb P(\\mathcal Y=1 \\mid \\mathcal X=\\mathbf x)}{\\mathbb P(\\mathcal Y=2 \\mid \\mathcal X=\\mathbf x)} &= \\frac{\\mathbb P(\\mathcal Y=1 \\mid \\mathcal X=\\mathbf x)}{1 - \\mathbb P(\\mathcal Y=1 \\mid \\mathcal X=\\mathbf x)}\\\\\n&= \\textsf{logit}(\\mathbb P(\\mathcal Y=1 \\mid \\mathcal X=\\mathbf x)),\n\\end{split}\n$$\n\ndonde la inversa del $\\textsf{logit}$ es la función sigmoide, $\\textsf{sigmoid}(x) = \\frac{1}{1 + \\exp(-x)}$, es decir $\\textsf{sigmoid}(\\textsf{logit}(y)) = y$. \n\nTrabajando un poco con el $\\textsf{logit}$ se puede observar que para el caso de dos clases está función queda como \n\n$$\n\\begin{split}\n\\textsf{logit}(\\mathbb P(\\mathcal Y=1 \\mid \\mathcal X=\\mathbf x)) &= \\log\\frac{\\mathbb P(\\mathcal Y=1 \\mid \\mathcal X=\\mathbf x)}{\\mathbb P(\\mathcal Y=2 \\mid \\mathcal X=\\mathbf x)}\\\\\n&= \\log\\frac{\\mathbb P(\\mathcal X=\\mathbf x \\mid \\mathcal Y=1)\\mathbb P(\\mathcal Y=1)}{\\mathbb P(\\mathcal X=\\mathbf x \\mid \\mathcal Y=2)\\mathbb P(\\mathcal Y=2)}\\\\\n&= \\log\\frac{\\mathbb P(\\mathcal X=\\mathbf x \\mid \\mathcal Y=1)}{\\mathbb P(\\mathcal X=\\mathbf x \\mid \\mathcal Y=2)} + \\log\\frac{\\mathbb P(\\mathcal Y=1)}{\\mathbb P(\\mathcal Y=2)}\n\\end{split},\n$$\n\nasumiendo que la matriz de covarianza ($\\Sigma$) es compartida entre las dos clases la ecuación anterior quedaría como:\n\n$$\n\\begin{split}\n\\textsf{logit}(\\mathbb P(\\mathcal Y=1 \\mid \\mathcal X=\\mathbf x)) &= \\log \\frac{(2\\pi)^{-\\frac{d}{2}} \\mid\\Sigma \\mid^{-\\frac{1}{2}}\\exp{(-\\frac{1}{2}(\\mathbf x - \\mathbf \\mu_1)^\\intercal \\Sigma^{-1}(\\mathbf x - \\mathbf \\mu_1))}}{(2\\pi)^{-\\frac{d}{2}} \\mid\\Sigma \\mid^{-\\frac{1}{2}}\\exp{(-\\frac{1}{2}(\\mathbf x - \\mathbf \\mu_2)^\\intercal \\Sigma^{-1}(\\mathbf x - \\mathbf \\mu_2))}}\\\\ \n&+ \\log\\frac{\\mathbb P(\\mathcal Y=1)}{\\mathbb P(\\mathcal Y=2)}\\\\\n&= \\mathbf w \\cdot \\mathbf x + w_0\n\\end{split}\n$$\n\ndonde $\\mathbf w=\\Sigma^{-1}(\\mathbf \\mu_1 - \\mathbf \\mu_2)$ y $w_0=-\\frac{1}{2}(\\mathbf \\mu_1 + \\mathbf \\mu_2)^\\intercal \\Sigma^{-1}(\\mathbf \\mu_1 + \\mathbf \\mu_2)+ \\log\\frac{\\mathbb P(\\mathcal Y=1)}{\\mathbb P(\\mathcal Y=2)}.$\n\nEn el caso de regresión logística, se asume que $\\textsf{logit}(\\mathbb P(\\mathcal Y=1 \\mid \\mathcal X=\\mathbf x)) = \\mathbf w \\cdot \\mathbf x + w_0$ y se realiza ninguna asunción sobre la distribución que tienen los datos. Equivalentemente, se puede asumir que $\\log\\frac{\\mathbb P(\\mathcal Y=1 \\mid \\mathcal X=\\mathbf x)}{\\mathbb P(\\mathcal Y=2 \\mid \\mathcal X=\\mathbf x)} = \\mathbf w \\cdot \\mathbf x + w_0^0,$ realizando algunas substituciones se puede ver que $w_0 = w_0^0 + \\log\\frac{\\mathbb P(\\mathcal Y=1)}{\\mathbb P(\\mathcal Y=2)}.$\n\n### Optimización\n\nSe puede asumir que $\\mathcal Y \\mid \\mathcal X$ sigue una distribución Bernoulli en el caso de dos clases, entonces el logaritmo de la verosimilitud (@sec-verosimilitud) quedaría como:\n\n$$\n\\ell(\\mathbf w, w_0 \\mid \\mathcal D) = \\prod_{(\\mathbf x, y) \\in \\mathcal D} (C(\\mathbf x))^{y} (1 -  C(\\mathbf x)))^{1-y},\n$$\n\ndonde $C(\\mathbf x)$ es la clase estimada por el clasificador. \n\nSiempre que se tiene que obtener el máximo de una función esta se puede transformar a un problema de minimización, por ejemplo, para el caso anterior definiendo como $E = -\\log \\ell$, utilizando esta transformación el problema sería minimizar la siguiente función:\n\n$$\nE(\\mathbf w, w_0 \\mid \\mathcal D) = - \\sum_{(\\mathbf x, y) \\in \\mathcal D} y \\log C(x) + (1-y) \\log (1 -  C(x)).\n$${#eq-lineal-entropia-cruzada}\n\nEs importante notar que la ecuación anterior corresponde a Entropía cruzada (@sec-entropia-cruzada), donde $y=\\mathbb P(\\mathcal Y=y \\mid \\mathcal X=\\mathbf x))$ y $C(\\mathbf x)=\\mathbb{\\hat P}(\\mathcal Y=y \\mid \\mathcal X=\\mathbf x)$ y los términos $1-y$ y $1-C(\\mathbf x)$ corresponde a la otra clase. \n\nOtra característica de $E(\\mathbf w, w_0 \\mid \\mathcal D)$ es que no tiene una solución cerrada y por lo tanto es necesario utilizar un método de optimización (@sec-optimizacion) para encontrar los parámetros $\\mathbf w$ y $w_0$. \n\n\n## Comparación\n\nEs momento de comparar el comportamiento de los dos métodos de discriminantes lineales visto en la unidad, estos son, Máquinas de Soporte Vectorial (MSV) y Regresión Logística (RL). La @fig-lineal-comparacion muestra el hiperplano generado por MSV y RL, además se puede observar los valores de los pesos $\\mathbf w$ para cada uno de los algoritmos. \n\n::: {#cell-fig-lineal-comparacion .cell execution_count=12}\n``` {.python .cell-code code-fold=\"true\"}\nsvm = LinearSVC(dual=False).fit(T, y_t)\nlr = LogisticRegression().fit(T, y_t)\n\ng = []\nfor model, tipo in zip([svm, lr], ['MSV', 'RL']):\n     w_1, w_2 = model.coef_[0]\n     w_0 = model.intercept_[0]\n     g += [dict(x1=x, x2=y, tipo=tipo)\n           for x, y in zip(T[:, 0], (-w_0 - w_1 * T[:, 0]) / w_2)]\n     g.append(dict(x1=w_1, x2=w_2, clase=tipo))\ndf = pd.DataFrame(g + \\\n                  [dict(x1=x, x2=y, clase='P') for x, y in X_1] + \\\n                  [dict(x1=x, x2=y, clase='N') for x, y in X_2]\n                 )\nax = sns.scatterplot(data=df, x='x1', y='x2', hue='clase', legend=True)\nsns.lineplot(data=df, x='x1', y='x2', \n             ax=ax, hue='tipo', \n             palette=sns.color_palette()[:2], \n             legend=True)\n_ = ax.axis('equal')\n```\n\n::: {.cell-output .cell-output-display}\n![Comparación de dos métodos lineales](09Lineal_files/figure-html/fig-lineal-comparacion-output-1.png){#fig-lineal-comparacion width=582 height=427}\n:::\n:::\n\n\nComplementando la comparación anterior con los datos del iris que se pueden obtener con las siguientes dos instrucciones. \n\n::: {#192110b6 .cell execution_count=13}\n``` {.python .cell-code}\nD, y = load_iris(return_X_y=True)\nT, G, y_t, y_g = train_test_split(D, y,\n                                  test_size=0.4,\n                                  random_state=3)\n```\n:::\n\n\nLos clasificadores a comparar son una máquina de soporte vectorial lineal, una máquina de soporte vectorial usando un kernel polinomial de grado $1$ y una regresión logística, tal y como se muestra en el siguiente código. \n\n::: {#0d8ceb51 .cell execution_count=14}\n``` {.python .cell-code}\nsvm = LinearSVC(dual=False).fit(T, y_t)\nsvm_k = SVC(kernel='poly', degree=1).fit(T, y_t)\nlr = LogisticRegression().fit(T, y_t)\n```\n:::\n\n\nLa @tbl-lineal-rendimiento muestra el rendimiento (en medidas macro) de estos algoritmos en el conjunto de prueba, se puede observar que estos algoritmos tienen rendimientos diferentes para esta selección del conjunto de entrenamiento y prueba. También en esta ocasión la regresión lineal es la que presenta el mejor rendimiento. Aunque es importante aclarar que este rendimiento es resultado del proceso aleatorio de selección del conjunto de entrenamiento y prueba.\n\n::: {#tbl-lineal-rendimiento .cell tbl-cap='Rendimiento de clasificadores lineales' execution_count=15}\n\n::: {.cell-output .cell-output-display execution_count=15}\nClasificador | Precisión | Recall | $F_1$   |\n-------------|-----------|--------|---------|\nMSV - Lineal|$0.9524$|$0.9500$|$0.9473$|\nMSV - Kernel|$0.9474$|$0.9481$|$0.9473$|\nRL|$0.9667$|$0.9667$|$0.9649$|\n\n:::\n:::\n\n\n",
    "supporting": [
      "09Lineal_files/figure-html"
    ],
    "filters": [],
    "includes": {}
  }
}