{
  "hash": "d9daea07e57e5051bd4a2f892e677b6f",
  "result": {
    "markdown": "# Optimización {#sec-optimizacion}\n\nEl **objetivo** de la unidad es conocer y aplicar el método de **Descenso de Gradiente** y **Propagación hacia Atrás** par estimar los parámetros de modelos de clasificación y regresión.\n\n## Paquetes usados\n\n::: {#af606004 .cell execution_count=1}\n``` {.python .cell-code}\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.preprocessing import StandardScaler, minmax_scale\nfrom scipy.stats import multivariate_normal\nfrom jax import grad, jit, value_and_grad, random\nfrom jax.scipy.optimize import minimize\nimport jax.lax as lax\nimport jax.numpy as jnp\nfrom matplotlib import pylab as plt\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\n```\n:::\n\n\n\n\n::: {.content-visible when-format=\"html\"}\n\n---\n\n\n{{< video https://www.youtube.com/embed/oxUDZFgdo-U width=\"560\" height=\"315\" >}}\n\n\n\n---\n\n:::\n\n## Introducción\n\nExisten diferentes modelos de clasificación y regresión donde no es posible encontrar una solución analítica para estimar los parámetros, por ejemplo en Regresión Logística (@sec-regresion-logistica). Es en este escenario donde se voltea a métodos de optimización iterativos para calcular los parámetros. \n\nEn esta unidad se describe posiblemente el método de optimización más conocido que es **Descenso de Gradiente**. Este método como su nombre lo indica utiliza el gradiente como su ingrediente principal; se describirá como se puede calcular el gradiente utilizando un método gráfico y como este método naturalmente realiza **Propagación hacia Atrás**.\n\n## Descenso por Gradiente {#sec-descenso-gradiente}\n\nEn un modelo de clasificación y regresión interesa encontrar un vector de parámetros, $\\mathbf w^{*},$ que minimicen una función de error, $E,$ de la siguiente manera:\n\n$$\n\\mathbf w^{*} = \\textsf{argmin}_{\\mathbf w} E(\\mathbf w \\mid \\mathcal D).\n$$\n\nEn el caso de que $E(\\mathbf w \\mid \\mathcal D)$ sea una función diferenciable, el gradiente está dado por:\n\n$$\n\\nabla_{\\mathbf w} E(\\mathbf w \\mid \\mathcal D) = [\\frac{\\partial E}{\\partial \\mathbf w_1}, \\frac{\\partial E}{\\partial \\mathbf w_2}, \\ldots]^\\intercal.\n$$\n\nLa idea general es tomar la dirección opuesta al gradiente para encontrar el mínimo de la función. Entonces el cambio de parámetro está dado por \n\n$$\n\\begin{split}\n\\Delta \\mathbf w &= - \\eta \\nabla_{\\mathbf w} E\\\\\n       \\mathbf w^{t+1} &= \\mathbf w^{t-1} + \\Delta \\mathbf w \\\\\n       &= \\mathbf w^{t-1} - \\eta \\nabla_{\\mathbf w} E\n\\end{split}\n$$\n\n### Ejemplo - Regresión Lineal {#sec-regresion-lineal-manual}\n\nSuponiendo que se quieren estimar los parámetros de la siguiente ecuación lineal: $f(x) = a x + b,$ para lo cual se tiene un conjunto de entrenamiento en el intervalo $x=[-10, 10],$ generado con los parámetros $a=2.3$ y $b=-3.$ Importante no olvidar que los parámetros $a=2.3$ y $b=-3$ son desconocidos y se quieren estimar usando **Descenso por Gradiente** y también es importante mencionar que para este problema en particular es posible tener una solución analítica para estimar los parámetros. \n\nEl primer paso es definir la función de error $E(a, b \\mid \\mathcal D),$ en problemas de regresión una función de error viable es: $E(a, b \\mid \\mathcal D) = \\sum_{(x, y) \\in \\mathcal D} (y - f(x))^2.$\n\nLa regla para actualizar los valores iniciales es: $w = w - \\eta \\nabla_w E$; por lo que se procede a calcular $\\nabla_w E$ donde $w$ corresponde a los parámetros $a$ y $b$. \n\n$$\n\\begin{split}\n    \\frac{\\partial E}{\\partial w} &= \\frac{\\partial}{\\partial w} \\sum (y - f(x))^2 \\\\\n    &= 2 \\sum (y - f(x)) \\frac{\\partial}{\\partial w} (y - f(x)) \\\\\n    &= - 2 \\sum (y - f(x)) \\frac{\\partial}{\\partial w} f(x)\n\\end{split}\n$$\n\ndonde $\\frac{\\partial}{\\partial a} f(x) = x$ y $\\frac{\\partial}{\\partial b} f(x) = 1.0$ Las ecuaciones para actualizar $a$ y $b$ serían:\n\n$$\n\\begin{split}\n    e(y, x) &= y - f(x) \\\\\n    a &= a + 2 \\eta \\sum_{(x, y) \\in \\mathcal D} e(y, x) x \\\\  \n    b &= b + 2 \\eta \\sum_{(x, y) \\in \\mathcal D} e(y, x)\n\\end{split}\n$$\n\nCon el objetivo de visualizar descenso por gradiente y completar el ejemplo anterior, el siguiente código implementa el proceso de optimización mencionado. Lo primero es generar el conjunto de entrenamiento.\n\n::: {#cd3e10d5 .cell execution_count=3}\n``` {.python .cell-code}\nx = np.linspace(-10, 10, 50)\ny = 2.3 * x - 3\n```\n:::\n\n\nEl proceso inicia con valores aleatorios de $a$ y $b$, estos valores podrían ser $5.3$ y $-5.1,$ además se utilizará una $\\eta=0.0001$, se guardarán todos los puntos visitados en la lista `D`. Las variables y valores iniciales quedarían como:\n\n::: {#fddb10f5 .cell execution_count=4}\n``` {.python .cell-code}\na = 5.3\nb = -5.1\ndelta = np.inf\neta = 0.0001\nD = [(a, b)]\n```\n:::\n\n\nEl siguiente ciclo realiza la iteración del proceso de optimización y se detienen cuando los valores estimados varían poco entre dos iteraciones consecutivas, en particular cuando en promedio el cambio en las constantes sea menor a $0.0001$. \n\n::: {#22468d4b .cell execution_count=5}\n``` {.python .cell-code}\nwhile delta > 0.0001:\n    hy = a * x + b\n    e = (y - hy)\n    a = a + 2 * eta * (e * x).sum()\n    b = b + 2 * eta * e.sum()\n    D.append((a, b))\n    delta = np.fabs(np.array(D[-1]) - np.array(D[-2])).mean()\n```\n:::\n\n\nEn la @fig-optimizacion-evolucion-desc se muestra el camino que siguieron los parámetros hasta llegar a los parámetros que generaron el problema. Los parámetros que generaron el problema se encuentran marcados en negro. \n\n::: {#cell-fig-optimizacion-evolucion-desc .cell execution_count=6}\n``` {.python .cell-code code-fold=\"true\"}\ndf = pd.DataFrame(np.array(D), columns=['a', 'b'])\n_ = np.atleast_2d(np.log(np.arange(1, df.shape[0] + 1))).T\ndf['iteración %'] = minmax_scale(_).flatten()\nsns.relplot(data=df, kind='scatter', x='a', y='b', hue='iteración %')\n_ = plt.tight_layout()\n```\n\n::: {.cell-output .cell-output-display}\n![Camino de los parámetros en el proceso de optimización.](10Optimizacion_files/figure-html/fig-optimizacion-evolucion-desc-output-1.png){#fig-optimizacion-evolucion-desc width=555 height=470}\n:::\n:::\n\n\n## Diferenciación Automática {#sec-diferenciacion-automatica}\n\nNo en todos los casos es posible encontrar una ecuación cerrada para actualizar los parámetros; también el trabajo manual que se requiere para encontrar la solución analítica es considerable y depende de la complejidad de la función objetivo que se quiere derivar. Observando el procedimiento para actualizar los parámetros, i.e., $\\mathbf w^{t+1} = w^{t-1} - \\eta \\nabla_{\\mathbf w} E$, se concluye que se requiere evaluar $\\nabla_{\\mathbf w} E$ en un punto en particular, entonces, para solucionar el problema, es suficiente contar con un procedimiento que permita conocer el valor de $\\nabla_{\\mathbf w} E$ en cualquier punto deseado, y en particular no es necesario conocer la solución analítica de $\\nabla_{\\mathbf w} E.$ Al procedimiento que encuentra el valor de la derivada de cualquier función en un punto de manera automática se le conoce como diferenciación automática.\n\n### Una Variable\n\nSe puede entender el proceso de diferenciación automática siguiendo un par de ejemplos. Sea $f(x) = x^2,$ en este caso $f'(x) = 2x.$ Ahora considerando que se quiere evaluar $f'(2.5)$, se sabe que $f'(2.5)=5,$  pero también se puede generar un programa donde en la primera fase se calcule $f(2.5)=(2.5)^2$ y al momento de calcular $f$ se guarde en un espacio asociado a $f$ el valor de $f'(2.5).$ es decir $5$. Se puede observar que este procedimiento soluciona este problema simple para cualquier función de una variable. \n\nLa librería [JAX](https://jax.readthedocs.io) permite hacer diferenciación automática en Python. Siguiendo el ejemplo anterior, se genera la función $f(x) = x^2$ con el siguiente código\n\n::: {#04aef37a .cell execution_count=7}\n``` {.python .cell-code}\ndef sq(x):\n    return x**2\n```\n:::\n\n\nEsta función se puede evaluar como `sq(2.5)`; lo interesante es que `sq` se puede componer con la función `grad` para calcular la derivada de la siguiente manera. \n\n::: {#04c1900d .cell execution_count=8}\n``` {.python .cell-code}\nres = grad(sq)(2.5)\n```\n:::\n\n\n\n\nEjecutando el código anterior se obtiene $5$, `grad` internamente guarda $5$, `grad` asociado a `sq`. \n\nAhora en el caso de una composición, por ejemplo, sea $g(x)=\\sin(x)$ y se desea conocer $\\frac{d}{dx} g(f(x)),$ siguiendo el mismo principio primero se evalúa $f(2.5)$ y se guarda asociado a $f$ el valor de $5$ que corresponde a $f',$ después se evalúa $g(5)$ y se guarda asociado a $g$ el valor $\\cos(2.5^2).$ Ahora para conocer el valor de $\\frac{d}{dx} g(f(2.5))$ se camina en el sentido inverso multiplicando todos los valores guardados, es decir, se multiplica $5 \\times \\cos(2.5^2).$ Implementando este ejemplo en Python quedaría como:\n\n::: {#348fa5e3 .cell execution_count=10}\n``` {.python .cell-code}\ndef sin_sq(x):\n    return jnp.sin(sq(x))\n```\n:::\n\n\n\n\ndonde la función `sin_sq` tiene la composición de $g \\circ f.$ Ahora la derivada de esta composición en 2.5 se obtiene como `grad(sin_sq)(2.5)` obteniendo un valor de $4.9972$.\n\n### Dos Variables\n\nEl ejemplo anterior se puede extender para cualquier composición de funciones, pero ese ejemplo no deja claro lo que pasaría con una función de dos o más variables, como por ejemplo, $h(x, y) = x \\times y.$ En este caso, el objetivo es calcular tanto $\\frac{d}{dx} h$ como $\\frac{d}{dy} h.$\n\nEl procedimiento es similar, con el ingrediente extra de que se tiene que recordar la función y la posición del argumento, es decir, para $h(2, 5)$ se asocia el valor $\\frac{d}{dx} h$ con el primer argumento de la función $h$ y $\\frac{d}{dx} h$ con el segundo argumento de la función $h$. Para $h'(2, 5)$ se asocia el valor de $5$ \ncon el primer argumento de $h$ y $2$ con el segundo argumento de $h.$\nSe puede observar que los valores guardados corresponden a $\\frac{d}{dx}h(2, 5)=y=5$ y $\\frac{d}{dy}h(2, 5)=x=2.$ Escribiendo este ejemplo en Python quedaría como \n\n::: {#6ec42aeb .cell execution_count=12}\n``` {.python .cell-code}\ndef prod(x):\n    return x[0] * x[1]\n```\n:::\n\n\ncalculando la derivada como `grad(prod)(jnp.array([2., 5.]))` se obtiene  `[5., 2.]` que corresponde a la derivada parcial en cada argumento. \n\n### Visualización\n\nUna manera de visualizar este proceso de diferenciación automática es poniéndolo en un árbol de expresión, en la @fig-optimizacion-arbol-expresion se muestra la ecuación $ax^2+bx+c$. Dentro de cada nodo se observa el valor que se tiene que guardar para cualquier valor de $a,b,c$ y $x.$ \n\n![Árbol de expresión](/docs/assets/images/eval.png){#fig-optimizacion-arbol-expresion}\n\n### Regresión Lineal\n\nEn esta sección se realiza utilizando diferenciación automática el ejemplo de regresión lineal (@sec-regresion-lineal-manual). Lo primero que se tiene que hacer es definir la función para la cual se quieren optimizar los parámetros. Esta función es $ax + b$ la cual se define con el siguiente código. El primer argumento de la función son los parámetros y después viene los argumentos de la función que en este caso son los valores de $x.$\n\n::: {#2e2bc16d .cell execution_count=13}\n``` {.python .cell-code}\ndef func(params, x):\n    a, b = params\n    return a * x + b\n```\n:::\n\n\nEl siguiente paso es definir la función de error, en este caso la función de error definida previamente es $\\sum_{i=1}^{N} (y_i - \\hat y_i)^2$ tal y como se muestra en la siguiente función. \n\n::: {#0ce3a63e .cell execution_count=14}\n``` {.python .cell-code}\ndef error(params, x, y):\n    hy = func(params, x)\n    return lax.fori_loop(0, x.shape[0],\n                         lambda i, acc: acc + (y[i] - hy[i])**2,\n                         0)\n```\n:::\n\n\nFinalmente, se actualizan los parámetros siguiendo un procedimiento equivalente al mostrado anteriormente. La primera linea define el valor de $\\eta,$ el valor de la variable `delta` indica el término del ciclo, la tercera línea define los parámetros iniciales (`params`), la siguiente guarda los todos los puntos visitados (cuarta línea) y la quinta línea deriva la función de `error` para calcular el gradiente. La primera instrucción dentro del ciclo actualiza los parámetros.  \n\n::: {#5235233a .cell execution_count=15}\n``` {.python .cell-code}\neta = 0.0001\ndelta = np.inf\nparams = jnp.array([5.3, -5.1])\nx = jnp.array(x)\ny = jnp.array(y)\nD = [params]\nerror_grad = grad(error)\nwhile delta > 0.0001:\n    params -= eta * error_grad(params, x, y)\n    D.append(params)\n    delta = jnp.fabs(D[-1] - D[-2]).mean()\n```\n:::\n\n\n## Regresión Logística {#sec-regresion-logistica-optimizacion}\n\nEn esta sección, se presenta la implementación de Regresión Logística (@sec-regresion-logistica) utilizando diferenciación automática. \n\n\nEl método se probará en los datos generados por dos normales en $\\mathbb R^2$ que se generan con las siguientes instrucciones. \n\n::: {#28505ddb .cell execution_count=16}\n``` {.python .cell-code}\nX_1 = multivariate_normal(mean=[15, 20], cov=[[3, -3], [-3, 8]]).rvs(1000)\nX_2 = multivariate_normal(mean=[8, 8], cov=[[4, 0], [0, 2]]).rvs(1000)\nT = np.concatenate((X_1, X_2))\nnormalize = StandardScaler().fit(T)\nT = jnp.array(normalize.transform(T))\ny_t = jnp.array([1] * X_1.shape[0] + [0] * X_2.shape[0])\n```\n:::\n\n\nSiguiendo los pasos utilizados en el ejemplo anterior, la primera función a implementar es el model, es decir la función sigmoide, i.e., $\\textsf{sigmoid}(\\mathbf x) = \\frac{1}{1 + \\exp(- (\\mathbf w \\cdot \\mathbf x + w_0))}.$\n\n::: {#18dcd9bc .cell execution_count=17}\n``` {.python .cell-code}\ndef modelo(params, x):\n    _ = jnp.exp(-(jnp.dot(x, params[:2]) + params[-1]))\n    return 1 / (1 + _)\n```\n:::\n\n\nEl siguiente paso es implementar la función de error que guía el proceso de optimización, en el caso de regresión logística de dos clases la función de error corresponde a la media de la Entropía Cruzada (@eq-lineal-entropia-cruzada). Esta función se implementa en dos pasos, primero se calcula la entropía cruzada usando el siguiente procedimiento. Donde la primera línea verifica selecciona si se trata de la clase positiva ($1$) o de la clase negativa $0$ y calcula el $\\log \\hat y$ o $\\log (1 - \\hat y)$ respectivamente.\n\n::: {#789f4faf .cell execution_count=18}\n``` {.python .cell-code}\ndef entropia_cruzada(y, hy):\n    _ = lax.cond(y == 1, lambda w: jnp.log(w), lambda w: jnp.log(1 - w), hy)\n    return lax.cond(_ == -jnp.inf, lambda w: jnp.log(1e-6), lambda w: w, _)\n```\n:::\n\n\nEl segundo paso es calcular la media de la entropía cruzada de cada elemento de $\\mathcal D,$ esto se puede realizar con la siguiente función. \n\n::: {#60862b5f .cell execution_count=19}\n``` {.python .cell-code}\ndef suma_entropia_cruzada(params, x, y):\n    hy = modelo(params, x)\n    return - lax.fori_loop(0, y.shape[0],\n                           lambda i, x: x + entropia_cruzada(y[i], hy[i]),\n                           1) / y.shape[0]\n```\n:::\n\n\nEl paso final es actualizar los parámetros, en esta ocasión se utilizará la función `value_and_grad` que regresa la evaluación de la función así como la derivada, esto para poder desplegar como el error disminuye con el paso de las iteraciones. El código es similar al mostrado anteriormente. La diferencias son la forma en generar los parámetros iniciales, primeras tres líneas, el parámetro $\\eta$, que se itera por $n=5000$ iteraciones y la función `value_and_grad`.\n\n::: {#739df68b .cell execution_count=20}\n``` {.python .cell-code}\nkey = random.PRNGKey(0)\nkey, subkey = random.split(key)\nparams = random.normal(subkey, (3,)) * jnp.sqrt(2 / 2)\neta = 0.1\nerror = []\nerror_grad  = value_and_grad(suma_entropia_cruzada)\nfor i in range(5000):\n    _, g = error_grad(params, T, y_t)\n    error.append(_)\n    params -= eta * g\n```\n:::\n\n\nLa @fig-optimizacion-descenso-ent-x muestra como se reduce la media de la entropía cruzada con respecto al número de iteraciones, este error sigue bajando aunque la velocidad disminuye drasticamente. \n\n::: {#cell-fig-optimizacion-descenso-ent-x .cell execution_count=21}\n``` {.python .cell-code code-fold=\"true\"}\ndf = pd.DataFrame({'Iteración': np.arange(5000), 'error': np.array(error)})\nsns.relplot(data=df, x='Iteración', y='error', kind='line')\n_ = plt.tight_layout()\n```\n\n::: {.cell-output .cell-output-display}\n![Descenso de Entropía Cruzada](10Optimizacion_files/figure-html/fig-optimizacion-descenso-ent-x-output-1.png){#fig-optimizacion-descenso-ent-x width=470 height=470}\n:::\n:::\n\n\n</details>\n\n\n\nOptimizando los parámetros de la regresión logística utilizando el procedimiento de diferenciación automática se obtiene [3.8149, 4.4874, -0.5492]. \n\nEn Discriminantes Lineales (@sec-discriminantes-lineales) se presento el procedimiento para entrenar un clasificador logístico usando la siguiente instrucción. \n\n::: {#977fe051 .cell execution_count=23}\n``` {.python .cell-code}\nlr = LogisticRegression().fit(T, y_t)\n```\n:::\n\n\n\n\nLos parámetros son [3.2214, 4.5592, -0.4105]. El error en el conjunto se entrenamiento es  se puede calcular de la siguiente manera. En comparación el error obtenido por diferenciación automática es $0.0032$, por supuesto este puede variar cambiando el número de iteraciones. \n\n::: {#a46b3eee .cell execution_count=25}\n``` {.python .cell-code}\n_ = jnp.array([lr.coef_[0, 0], lr.coef_[0, 1], lr.intercept_[0]])\nsuma_entropia_cruzada(_, T, y_t)\n```\n\n::: {.cell-output .cell-output-display execution_count=25}\n```\nArray(0.0037642, dtype=float32)\n```\n:::\n:::\n\n\n## Actualización de Parámetros {#sec-actualizacion-parametros}\n\nPor supuesto la regla $\\mathbf w^{t+1} = \\mathbf w^{t-1} - \\eta \\nabla_{\\mathbf w} E$ para actualizar los parámetros $\\mathbf w$ no es única y existe una gama de métodos que se pueden seleccionar dependiendo de las características del problema. En particular regresión logística es una problema de optimización convexo, en este tipo de problemas un algoritmos para encontrar los parámetros es el `BFGS`. Por ejemplo el siguiente código utiliza este algoritmo para encontrar los parámetros de la regresión logística. \n\n::: {#b0b6c667 .cell execution_count=26}\n``` {.python .cell-code}\np = random.normal(subkey, (3,)) * jnp.sqrt(2 / 2)\nres = minimize(suma_entropia_cruzada, p, args=(T, y_t), method='BFGS')\n```\n:::\n\n\n\n\nSe observa como se usa la misma función de error (`suma_entropia_cruzada`) y los mismos parámetros iniciales. Los parámetros que se encuentran con este método son [21.5859, 46.1358, -4.5738] y tiene un error $-0.0004$.\n\nLa @fig-optimizacion-comparacion muestra la función discriminantes obtenida por cada uno de los métodos, el método de diferenciación automática (JAX), el método de la librearía `sklearn` y el algoritmo `BFGS`. Se puede observar que el plano de `sklearn` y `BFGS` son más similares, esto no es sorpresa porque los dos métodos implementan el mismo método de optimización, es decir, `BFGS`. Por supuesto la implementaciones tiene algunas diferencias, que pueden ir desde el número de iteraciones y la forma de generar los parámetros iniciales. Si se escalan los parámetros $\\mathbf w$ para que tengan una longitud de $1$ se observaría que `sklearn` y `BFGS` se tocan.\n\n::: {#cell-fig-optimizacion-comparacion .cell execution_count=28}\n``` {.python .cell-code code-fold=\"true\"}\nW = [jnp.array([lr.coef_[0, 0], lr.coef_[0, 1], lr.intercept_[0]]), params, res.x]\ng = []\nfor w, tipo in zip(W, ['sklearn', 'JAX', 'BFGS']):\n     w_1, w_2, w_0 = w\n     g += [dict(x1=float(x), x2=float(y), tipo=tipo)\n           for x, y in zip(T[:, 0], (-w_0 - w_1 * T[:, 0]) / w_2)]\n     # w_1, w_2, w_0 = w / np.linalg.norm(w) * 3\n     g.append(dict(x1=float(w_1), x2=float(w_2), clase=tipo))\n\ndf = pd.DataFrame(g + \\\n                  [dict(x1=x, x2=y, clase='P') for x, y in normalize.transform(X_1)] + \\\n                  [dict(x1=x, x2=y, clase='N') for x, y in normalize.transform(X_2)]\n                 )\n\nax = sns.scatterplot(data=df, x='x1', y='x2', hue='clase', legend=True)\nsns.lineplot(data=df, x='x1', y='x2', \n             ax=ax, \n             hue='tipo', \n             palette=sns.color_palette()[:3],\n             legend=True)\n\n_ = ax.axis('equal')\n```\n\n::: {.cell-output .cell-output-display}\n![Comparación de modelos lineales con diferentes optimizadores](10Optimizacion_files/figure-html/fig-optimizacion-comparacion-output-1.png){#fig-optimizacion-comparacion width=582 height=427}\n:::\n:::\n\n\n",
    "supporting": [
      "10Optimizacion_files/figure-html"
    ],
    "filters": [],
    "includes": {}
  }
}