{
  "hash": "b9f4a18dae67e9c214fa4d879ace3500",
  "result": {
    "engine": "jupyter",
    "markdown": "# Comparación de Algoritmos\n\nEl **objetivo** de la unidad es conocer y aplicar diferentes procedimientos estadísticos para comparar y analizar el rendimiento de algoritmos. \n\n## Paquetes usados\n\n::: {#7fe534e7 .cell execution_count=1}\n``` {.python .cell-code}\nfrom CompStats import StatisticSamples, CI\nfrom CompStats import performance, plot_performance\nfrom CompStats import difference, plot_difference\nfrom scipy.stats import norm, wilcoxon\nfrom sklearn.datasets import load_iris, load_breast_cancer\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split, StratifiedKFold\nfrom sklearn.metrics import recall_score\nimport numpy as np\nimport seaborn as sns\nimport pandas as pd\n```\n:::\n\n\n\n\n::: {.content-visible when-format=\"html\"}\n\n---\n\n\n{{< video https://www.youtube.com/embed/a6zzTtscYtU width=\"560\" height=\"315\" >}}\n\n\n\n---\n\n:::\n\n## Introducción\n\nHasta el momento se han descrito diferentes algoritmos de clasificación y regresión; se han presentado diferentes medidas para conocer su rendimiento, pero se ha dejado de lado el conocer la distribución de estas medidas para poder tener mayor información sobre el rendimiento del algoritmo y también poder comparar y seleccionar el algoritmo que tenga las mejores prestaciones ya sea en rendimiento o en complejidad. \n\n## Intervalos de confianza {#sec-intervalos}\n\nEl análisis del rendimiento se inicia partiendo de que el rendimiento se puede estimar a partir del conjunto de prueba, $\\mathcal G$; el valor obtenido estima el rendimiento real, $\\theta$, el cual se considera una constante. Una manera de conocer el rango de valores donde se puede encontrar $\\theta$ es generando su intervalo de confianza. El intervalo de confianza de $\\theta$ está dado por $C = (a(\\mathcal G), b(\\mathcal G)),$ de tal manera que $P_{\\theta}(\\theta \\in C) \\geq 1 - \\alpha$. Es importante mencionar que el intervalo no mide la probabilidad de $\\theta$ dado que $\\theta$ es una constante, en su lugar mide de que el valor estimado esté dentro de esos límites con esa probabilidad. Por otro lado se utiliza la notación $a(\\mathcal G)$ y $b(\\mathcal G)$ para hacer explicito que en este caso los límites del intervalo son obtenidos utilizando el conjunto de prueba. Una manera de entender el intervalo de confianza de cualquier parámetro es suponer que si el parámetro se estima $100$ veces con el mismo procedimiento, en diferentes muestras, un intervalo del 95% de confianza dice que 95 de las veces la estimación del parámetro estará en el intervalo calculado.\n\n### Método: Distribución Normal\n\nExisten diferentes procedimientos para generar intervalos de confianza, uno de ellos es asumir que la estimación de $\\theta$, i.e., $\\hat \\theta$ se distribuye como una normal, i.e., $\\hat \\theta \\sim \\mathcal N(\\mu, \\sigma^2),$ donde $\\sigma=\\textsf{se}=\\sqrt{\\mathbb V(\\hat \\theta)}$ corresponde al error estándar (@sec-error-estandar) de la estimación $\\hat \\theta.$ En estas condiciones el intervalo está dado por:\n\n$$\nC = (\\hat \\theta - z_{\\frac{\\alpha}{2}}\\textsf{se}, \\hat \\theta + z_{\\frac{\\alpha}{2}}\\textsf{se}),\n$$\n\ndonde $z_{\\frac{\\alpha}{2}} = \\Phi^{-1}(1 - \\frac{\\alpha}{2})$ y $\\Phi$ es la función de distribución acumulada de una normal. \n\n### Ejemplo: Exactitud\n\nRecordado que dado una entrada el clasificador puede acertar la clase a la que pertenece esa entrada, entonces el resultado se puede representar como $1$ si la respuesta es correcta y $0$ de lo contrario. En este caso la respuesta es una variable aleatoria con una distribución de Bernoulli. Recordando que la distribución Bernoulli está definida por un parámetro $p$, estimado como $\\hat p = \\frac{1}{N} \\sum_{i=1}^N \\mathcal X_i$ donde $\\mathcal X_i$ corresponde al resultado del algoritmo en el $i$-ésimo ejemplo. La varianza de una distribución Bernoulli es $p(1-p)$ por lo que el error estándar es: $se=\\sqrt{\\frac{p(1-p)}{N}}$ dando como resultado el siguiente intervalo:\n\n$$\nC = (\\hat p_N - z_{\\frac{\\alpha}{2}}\\sqrt{\\frac{p(1-p)}{N}}, \\hat p_N + z_{\\frac{\\alpha}{2}}\\sqrt{\\frac{p(1-p)}{N}}).\n$$\n\nSuponiendo $N=100$ y $p=0.85$ el siguiente código calcula el intervalo usando $\\alpha=0.05$\n\n::: {#0000f347 .cell execution_count=3}\n``` {.python .cell-code}\nalpha = 0.05\nz = norm().ppf(1 - alpha / 2)\np = 0.85\nN = 100\nCn = (p - z * np.sqrt(p * (1 - p) / N),\n      p + z * np.sqrt(p * (1 - p) / N))\n```\n:::\n\n\ndando como resultado el siguiente intervalo, $C = (0.78, 0.92)$.\n\nEn el caso anterior se supuso que se contaba con los resultados de un algoritmo de clasificación, con el objetivo de completar este ejemplo a continuación se presenta el análisis con un Naive Bayes en el problema del Iris. \n\nLo primero que se realiza es cargar los datos y dividir en el conjunto de entrenamiento ($\\mathcal T$) y prueba ($\\mathcal G$) como se muestra a continuación. \n\n::: {#17b8ab01 .cell execution_count=4}\n``` {.python .cell-code}\nX, y = load_iris(return_X_y=True)\nT, G, y_t, y_g = train_test_split(X, y,\n                                  random_state=1,  \n                                  test_size=0.3)\n```\n:::\n\n\nEl siguiente paso es entrenar el algoritmo y realizar las predicciones en el conjunto de prueba ($\\mathcal G$) tal y como se muestra en las siguientes instrucciones. \n\n::: {#812f1bef .cell execution_count=5}\n``` {.python .cell-code}\nmodel = GaussianNB().fit(T, y_t)\nhy = model.predict(G)\n```\n:::\n\n\nCon las predicciones se estima la exactitud y se siguen los pasos para calcular el intervalo de confianza como se ilustra en el siguiente código.\n\n::: {#851fb161 .cell execution_count=6}\n``` {.python .cell-code}\n_ = np.where(y_g == hy, 1, 0)\np = _.mean()\nN = _.shape[0]\nC = (p - z * np.sqrt(p * (1 - p) / N), p + z * np.sqrt(p * (1 - p) / N))\n```\n:::\n\n\nEl intervalo de confianza obtenido es $C = (0.86, 1.01)$. se puede observar que el límite superior es mayor que $1$ lo cual no es posible dado que el máximo valor del accuracy es $1,$ esto es resultado de generar el intervalo de confianza asumiendo una distribución normal. \n\nCuando se cuenta con conjuntos de datos pequeños y además no se ha definido un conjunto de prueba, se puede obtener las predicciones del algoritmo de clasificación mediante el uso de validación cruzada usando K-fold. En el siguiente código se muestra su uso, el cambio solamente es en el procedimiento para obtener las predicciones.\n\n::: {#f7d8b923 .cell execution_count=7}\n``` {.python .cell-code}\nkf = StratifiedKFold(n_splits=10,\n                     random_state=0,\n                     shuffle=True)\nhy = np.empty_like(y)\nfor tr, ts in kf.split(X, y):\n    model = GaussianNB().fit(X[tr], y[tr])\n    hy[ts] = model.predict(X[ts])\n```\n:::\n\n\n\n\nEl resto del código es equivalente al usado previamente obteniendo el siguiente intervalo de confianza $C = (0.92, 0.99)$.\n\n### Método: Bootstrap del error estándar {#sec-bootstrap-error-estandar}\n\nExisten ocasiones donde no es sencillo identificar el error estándar ($\\textsf{se}$) y por lo mismo no se puede calcular el intervalo de confianza. En estos casos se emplea la técnica de Bootstrap (@sec-bootstrap) para estimar $\\mathbb V(\\hat \\theta).$ Un ejemplo donde no es sencillo encontrar analíticamente el error estándar es en el $recall$ (@sec-recall).\n\nEs más sencillo entender este método mediante un ejemplo. Usando el ejercicio de $N=100$ y $p=0.85$ y $\\alpha=0.05$ descrito previamente, el siguiente código primero construye las variables aleatorias de tal manera que den $p=0.85$\n\n::: {#95e75ce5 .cell execution_count=9}\n``` {.python .cell-code}\nalpha = 0.05\nN = 100\nz = norm().ppf(1 - alpha / 2)\nX = np.zeros(N)\nX[:85] = 1\n```\n:::\n\n\n`X` es una arreglo que podrían provenir de la evaluación de un clasificador usando alguna medida de similitud entre predicción y valor medido. El siguiente paso es generar seleccionar con remplazo y obtener $\\hat \\theta$ para cada muestra, en este caso $\\hat \\theta$ corresponde a la media. El resultado se guarda en una lista $B$ y se repite el experimento $500$ veces.\n\n::: {#5e35eec8 .cell execution_count=10}\n``` {.python .cell-code}\nS = np.random.randint(X.shape[0],\n                      size=(500, X.shape[0]))\nB = [X[s].mean() for s in S]\n```\n:::\n\n\nEl error estándar es y el intervalo de confianza se calcula con las siguientes instrucciones \n\n::: {#77568081 .cell execution_count=11}\n``` {.python .cell-code}\nse = np.sqrt(np.var(B))\nC = (p - z * se, p + z * se)\n```\n:::\n\n\nel intervalo de confianza corresponde a $C = (0.88, 1.02)$. \n\nContinuando con el mismo ejemplo pero ahora analizando Naive Bayes en el problema del Iris. El primer paso es obtener evaluar las predicciones que se puede observar en el siguiente código (previamente descrito.)\n\n::: {#10fb5542 .cell execution_count=12}\n``` {.python .cell-code}\nX, y = load_iris(return_X_y=True)\nkf = StratifiedKFold(n_splits=10,\n                     random_state=0,\n                     shuffle=True)\n\nhy = np.empty_like(y)\nfor tr, ts in kf.split(X, y):\n    model = GaussianNB().fit(X[tr], y[tr])\n    hy[ts] = model.predict(X[ts])\nX = np.where(y == hy, 1, 0)\n```\n:::\n\n\nRealizando la selección con remplazo se obtiene el intervalo con las siguientes instrucciones \n\n::: {#874aae63 .cell execution_count=13}\n``` {.python .cell-code}\nB = [X[s].mean() for s in S]\nse = np.sqrt(np.var(B))\nC = (p - z * se, p + z * se)\n```\n:::\n\n\nteniendo un valor de $C = (0.92, 0.99)$.  \n\n\n### Método: Percentil\n\nExiste otra manera de calcular los intervalos de confianza y es mediante el uso del percentil, utilizando directamente las estimaciones realizadas a $\\hat \\theta$ en la selección. El siguiente código muestra este método usando el ejemplo anterior, \n\n```python\nalpha = 0.05 / 2\nC = (np.percentile(B, alpha * 100),\n     np.percentile(B, (1 - alpha) * 100))\n```\n\nobteniendo un intervalo de $C = (0.92, 0.99)$.\n\n### Ejemplo: macro-recall\n\nHasta el momento se ha usado una medida de rendimiento para la cual se puede conocer su varianza de manera analítica. Existen problemas donde esta medida no es recomendada, en el siguiente ejemplo utilizaremos macro-recall para medir el rendimiento de Naive Bayes en el problema del Iris. El primer paso es realizar las predicciones del algoritmo usando validación cruzada y hacer la muestra con reemplazo $B$. \n\n::: {#03cfec8a .cell execution_count=14}\n``` {.python .cell-code}\nalpha = 0.05\nz = norm().ppf(1 - alpha / 2)\n\nX, y = load_iris(return_X_y=True)\nkf = StratifiedKFold(n_splits=10,\n                     random_state=0,\n                     shuffle=True)\n\nhy = np.empty_like(y)\nfor tr, ts in kf.split(X, y):\n    model = GaussianNB().fit(X[tr], y[tr])\n    hy[ts] = model.predict(X[ts])\n\nS = np.random.randint(hy.shape[0],\n                      size=(500, hy.shape[0]))\nB = [recall_score(y[s], hy[s], average=\"macro\")\n     for s in S]\n```\n:::\n\n\nEl siguiente paso es calcular el intervalo asumiendo que este se comporta como una normal tal y como se muestra en las siguientes instrucciones;\n\n::: {#b8c98b7a .cell execution_count=15}\n``` {.python .cell-code}\np = np.mean(B)\nse = np.sqrt(np.var(B))\nC = (p - z * se, p + z * se)\n```\n:::\n\n\n\n\nobteniendo un intervalo de $C = (0.92, 0.99)$ Completando el ejercicio, el intervalo se puede calcular directamente usando el percentil, estimando un intervalo de $C = (0.92, 0.98)$\n\n\n::: {.callout-note}\nEl método de bootstrap para calcular el error estándar y el método de percentil para calcular el intervalo de confianza se pueden estimar utilizando la clase `StatisticSamples`. Las siguientes instrucciones se pueden utilizar para calcular el error estándar. \n\n::: {#715d24f3 .cell execution_count=17}\n``` {.python .cell-code}\nrecall = lambda y, hy: recall_score(y, hy,\n                                    average=\"macro\")\n    \nstatistic = StatisticSamples(statistic=recall)\nsamples = statistic(y, hy)\nnp.std(samples)                               \n```\n\n::: {.cell-output .cell-output-display execution_count=17}\n```\n0.017322381817494535\n```\n:::\n:::\n\n\nComplementando el intervalo de confianza con el método de percentil se implementa en el siguiente código.\n\n::: {#4b64e651 .cell execution_count=18}\n``` {.python .cell-code}\nCI(samples)\n```\n\n::: {.cell-output .cell-output-display execution_count=18}\n```\n(0.9182958289698858, 0.9813722126929675)\n```\n:::\n:::\n\n\n:::\n\n## Comparación de Algoritmos\n\nSe han descrito varios procedimientos para conocer los intervalos de confianza de un algoritmos de aprendizaje. Es momento para describir la metodología para conocer si dos algoritmos se comportan similar en un problema dado. \n\n### Método: Distribución $t$ de Student\n\nSuponiendo que se tienen las medidas de rendimiento de dos algoritmos mediante validación cruzada de K-fold, es decir, se tiene el rendimiento del primer algoritmo como $p_i^1$ y del segundo como $p_i^2$ en la $i$-ésima instancia. Suponiendo que el rendimiento es una normal, entonces la resta, i.e., $p_i = p_i^1 - p_i^2$ también sería normal. Dado que se está comparando los algoritmos en los mismos datos, se puede utilizar la prueba $t$ de Student de muestras dependientes. La estadística de la prueba está dada por $\\frac{\\sqrt{K} m}{S} \\sim t_{K-1}$, donde $m$ \ny $S^2$ es la media varianza estimada.\n\nEn el siguiente ejemplo se compara el rendimiento de Árboles Aleatorios y Naive Bayes en el problema de Breast Cancer. El primer paso es cargar las librerías así como obtener las predicciones de los algoritmos. \n\n::: {#547736bb .cell execution_count=19}\n``` {.python .cell-code}\nK = 30\nkf = StratifiedKFold(n_splits=K,\n                     random_state=0,\n                     shuffle=True)\nX, y = load_breast_cancer(return_X_y=True)\n\nP = []\nfor tr, ts in kf.split(X, y):\n    forest = RandomForestClassifier().fit(X[tr], y[tr]).predict(X[ts])\n    naive = GaussianNB().fit(X[tr], y[tr]).predict(X[ts])\n    P.append([recall_score(y[ts], hy, average=\"macro\") for hy in [forest, naive]])\nP = np.array(P)\n```\n:::\n\n\nComo se puede observar la medida de rendimiento es macro-recall. Continuando con el procedimiento para obtener la estadística $t_{K-1}$\n\n::: {#e5d76359 .cell execution_count=20}\n``` {.python .cell-code}\np = P[:, 0] - P[:, 1]\nt = np.sqrt(K) * np.mean(p) / np.std(p)\n```\n:::\n\n\ndonde el valor de la estadística es $2.8975$, si el valor está fuera del siguiente intervalo $(-2.045, 2.045)$ se rechaza la hipótesis nula de que los dos algoritmos se comportan similar. \n\nEn caso de que la medida de rendimiento no esté normalmente distribuido, la prueba no-parametrica equivalente corresponde a Wilcoxon. La instrucción `wilcoxon(P[:, 0], P[:, 1])` se puede utilizar para calcularla, dando un $p_{\\text{value}}$ de $0.0170$. En ambos casos podemos concluir que los algoritmos Árboles Aleatorios y Naive Bayes son estadisticamente diferentes con una confianza del 95% en el problema de Breast Cancer.\n\n### Método: Bootstrap en diferencias\n\nUn método para comparar el rendimiento de dos algoritmo que no asume ningún tipo de distribución se puede realizar mediante la técnica de Bootstrap. @Nava2024 utilizan esta idea para comparar diferentes algoritmos en el esquema de una competencia de aprendizaje supervisado. La idea es calcular las predicciones de los algoritmos y realizar la muestra calculando en cada una la diferencia del rendimiento. Este se procedimiento se explicará mediante un ejemplo. \n\nEl primer paso es calcular las predicciones de los algoritmos, en este caso se realizar una validación cruzada, tal y como se muestra a continuación. \n\n::: {#66844466 .cell execution_count=21}\n``` {.python .cell-code}\nforest = np.empty_like(y)\nnaive = np.empty_like(y)\nfor tr, ts in kf.split(X, y):\n    forest[ts] = RandomForestClassifier().fit(X[tr], y[tr]).predict(X[ts])\n    naive[ts] = GaussianNB().fit(X[tr], y[tr]).predict(X[ts])\n```\n:::\n\n\nEl macro-recall para los Bosques Aleatorios es $0.96$ y para el Naive Bayes es $0.93$. Lo que se observa es que los bosques tienen un mejor rendimiento, entonces la distribución de la diferencia del rendimiento entre bosques y Naive Bayes no debería de incluir al cero, si lo incluye la masa que está al lado izquierdo del cero debe de ser menor, esa mas corresponde al valor $p.$\n\nLas muestras de la diferencia de rendimiento se pueden calcular de las siguientes instrucciones. \n\n::: {#cb9b7292 .cell execution_count=22}\n``` {.python .cell-code}\nS = np.random.randint(y.shape[0],\n                      size=(500, y.shape[0]))\nr = recall_score                      \ndiff = lambda y, hy1, hy2: r(y, hy1, average=\"macro\") -\\\n                           r(y, hy2, average=\"macro\")\nB = [diff(y[s], forest[s], naive[s])\n     for s in S]\n```\n:::\n\n\nFinalmente, el $p_{\\text{value}}$ corresponde a la proporción de elementos que son menores que cero, i.e., `(np.array(B) < 0).mean()`, es decir, aquellas muestras donde Naive Bayes tiene un mejor desempeño que los bosques. En este caso el $p_{\\text{value}}$ tiene un valor de $0.0020$. Dado que el valor es menor que $0.05$ se puede rechazar la hipótesis nula con una confianza superior al 95% y concluir que existe una diferencia estadísticamente significativa en el rendimiento entre los dos algoritmos. La @fig-comparacion-diff muestra la distribución de la diferencia de rendimiento, en esta se puede observar como la mayor parte de la masa se encuentra del lado positivo y que muy poca masa es menor que cero. \n\n::: {#cell-fig-comparacion-diff .cell execution_count=23}\n``` {.python .cell-code code-fold=\"true\"}\nfig = sns.displot(B, kde=True)\n```\n\n::: {.cell-output .cell-output-display}\n![Distribución de la diferencia de rendimiento](13Comparacion_files/figure-html/fig-comparacion-diff-output-1.png){#fig-comparacion-diff width=471 height=471}\n:::\n:::\n\n\n## CompStats \n\nLa librería [CompStats](https://compstats.readthedocs.io) permite realizar de manera sencilla la comparación entre el rendimiento de varios algoritmos, además de presentar gráficas sobre su comportamiento. Esta librería usa la técnica de Bootstrap tal y como se ha mostrado en este capítulo. \n\n\nLo primero que se realiza es calcular las muestras del rendimiento entre los dos algoritmos analizados, los cuales tienen sus predicciones en las variables `naive` y `forest`. En la primera línea se inicializa la variable `macro_recall` que calcula el promedio de la cobertura. La segunda línea crea un cuadro de datos (`DataFrame`) que contiene tanto la variable medida como las predicciones. La tercera línea calcula el rendimiento de los dos algoritmos. \n\n::: {#ccf13265 .cell execution_count=25}\n``` {.python .cell-code}\nmacro_recall = lambda y, hy: recall_score(y, hy,\n                                          average='macro')\ndf = pd.DataFrame(dict(y=y,\n                       naive=naive,\n                       forest=forest))\nperf = performance(df, score=macro_recall)\n```\n:::\n\n\nLa variable `perf` contiene las muestras del rendimiento de cada algoritmo. Por ejemplo, la siguiente línea calcula el intervalo de confianza de cada algoritmo. \n\n::: {#4d1c7c45 .cell execution_count=26}\n``` {.python .cell-code}\nCI(perf)\n```\n\n::: {.cell-output .cell-output-display execution_count=26}\n```\n{'naive': (0.9031633862672502, 0.9509194577792208),\n 'forest': (0.9379929657295595, 0.9735565840568744)}\n```\n:::\n:::\n\n\nLos intervalos de confianza se pueden visualizar en la @fig-intervalos-compstats. \n\n::: {#cell-fig-intervalos-compstats .cell execution_count=27}\n``` {.python .cell-code code-fold=\"true\"}\nplot_performance(perf)\n```\n\n::: {.cell-output .cell-output-display}\n![Intervalos de confianzas](13Comparacion_files/figure-html/fig-intervalos-compstats-output-1.png){#fig-intervalos-compstats width=487 height=470}\n:::\n:::\n\n\nEl segundo paso es conocer si la diferencia en rendimiento es significativa, en la @fig-comparacion-compstats se observa la comparación entre el mejor algoritmo (i.e., `forest`) y los otros algoritmos (`naive`) incluidos en la comparación. Se observa, en la figura, una linea puntuada que se encuentra en cero para facilitar la comparación, si el intervalo de confianza intersecta con la linea se puede concluir que los algoritmos comparados no son estadísticamente diferentes. En el ejemplo mostrado se ve que el intervalo no intersecta entonces se puede concluir que la hipótesis nula se puede descartar con una cierta confianza.  \n\n::: {#cell-fig-comparacion-compstats .cell execution_count=28}\n``` {.python .cell-code code-fold=\"true\"}\ndiff = difference(perf)\nplot_difference(diff)\n```\n\n::: {.cell-output .cell-output-display}\n![Comparación contra el mejor](13Comparacion_files/figure-html/fig-comparacion-compstats-output-1.png){#fig-comparacion-compstats width=553 height=490}\n:::\n:::\n\n\n",
    "supporting": [
      "13Comparacion_files"
    ],
    "filters": [],
    "includes": {}
  }
}